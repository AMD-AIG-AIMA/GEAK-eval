2025-08-08_09-44-59 => File: moe_gemm.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_correctness_int8_w8a8_True_True_1_14336_128_2_4', 'test_correctness_int8_w8a16_True_True_1_14336_128_2_4', 'test_correctness_fp8_fp8_type1_True_False_64_3584_128_2_8', 'test_correctness_True_16_14336_128_1_1', 'test_correctness_fp8_fp8_type0_True_False_16_14336_128_1_1', 'test_correctness_fp8_fp8_type1_True_True_1_14336_128_2_4', 'test_correctness_fp8_fp8_type0_True_False_16_14336_128_1_4', 'test_correctness_int8_w8a8_True_True_64_14336_4096_2_8', 'test_correctness_int8_w8a8_True_False_16_14336_1_2_4', 'test_correctness_fp8_fp8_type0_True_False_64_7186_128_2_8', 'test_correctness_fp8_fp8_type0_True_True_64_3584_128_2_8', 'test_correctness_fp8_fp8_type0_True_True_16_14336_128_1_4', 'test_correctness_False_16_14336_128_1_4', 'test_correctness_int8_w8a16_True_True_16_14336_1_2_4', 'test_correctness_int8_w8a8_True_True_16_14336_1_2_4', 'test_correctness_int8_w8a16_True_True_16_14336_128_1_4', 'test_correctness_fp8_fp8_type1_True_False_64_1792_128_2_8', 'test_correctness_int8_w8a8_True_False_16_14336_128_1_4', 'test_correctness_False_64_1792_128_2_8', 'test_correctness_False_64_3584_128_2_8', 'test_correctness_False_16_14336_1_2_4', 'test_correctness_fp8_fp8_type1_True_False_64_14336_4096_2_8', 'test_correctness_False_64_14336_4096_2_8', 'test_correctness_int8_w8a16_True_False_16_14336_1_2_4', 'test_correctness_int8_w8a8_True_True_16_14336_128_1_1', 'test_correctness_int8_w8a16_True_True_16_14336_128_1_1', 'test_correctness_fp8_fp8_type1_True_False_16_14336_128_1_4', 'test_correctness_fp8_fp8_type1_True_False_16_14336_128_1_1', 'test_correctness_int8_w8a16_True_False_64_1792_128_2_8', 'test_correctness_int8_w8a8_True_False_64_7186_128_2_8', 'test_correctness_fp8_fp8_type1_True_True_64_7186_128_2_8', 'test_correctness_fp8_fp8_type0_True_False_64_64_128_2_8', 'test_correctness_fp8_fp8_type0_True_True_64_7186_128_2_8', 'test_correctness_fp8_fp8_type1_True_True_64_64_128_2_8', 'test_correctness_fp8_fp8_type0_True_True_16_14336_1_2_4', 'test_correctness_int8_w8a8_True_True_64_3584_128_2_8', 'test_correctness_int8_w8a8_True_False_64_14336_4096_2_8', 'test_correctness_fp8_fp8_type0_True_True_16_14336_128_1_1', 'test_correctness_fp8_fp8_type0_True_True_64_1792_128_2_8', 'test_correctness_fp8_fp8_type1_True_False_64_7186_128_2_8', 'test_correctness_fp8_fp8_type1_True_True_64_3584_128_2_8', 'test_correctness_fp8_fp8_type1_True_False_1_14336_128_2_4', 'test_correctness_True_16_14336_128_1_4', 'test_correctness_fp8_fp8_type1_True_True_64_14336_4096_2_8', 'test_correctness_fp8_fp8_type1_True_False_64_64_128_2_8', 'test_correctness_int8_w8a16_True_False_1_14336_128_2_4', 'test_correctness_False_64_7186_128_2_8', 'test_correctness_int8_w8a8_True_True_64_1792_128_2_8', 'test_correctness_int8_w8a8_True_True_64_64_128_2_8', 'test_correctness_int8_w8a8_True_False_64_1792_128_2_8', 'test_correctness_False_3_14336_128_2_4', 'test_correctness_fp8_fp8_type1_True_True_16_14336_128_1_4', 'test_correctness_int8_w8a8_True_False_1_14336_128_2_4', 'test_correctness_fp8_fp8_type0_True_True_64_64_128_2_8', 'test_correctness_int8_w8a16_True_True_64_64_128_2_8', 'test_correctness_int8_w8a16_True_False_16_14336_128_1_4', 'test_correctness_fp8_fp8_type1_True_False_16_14336_1_2_4', 'test_correctness_int8_w8a16_True_True_64_1792_128_2_8', 'test_correctness_int8_w8a8_True_True_16_14336_128_1_4', 'test_correctness_False_1_14336_128_2_4', 'test_correctness_fp8_fp8_type0_True_False_64_3584_128_2_8', 'test_correctness_True_64_3584_128_2_8', 'test_correctness_True_3_14336_128_2_4', 'test_correctness_int8_w8a8_True_False_64_64_128_2_8', 'test_correctness_False_64_64_128_2_8', 'test_correctness_fp8_fp8_type0_True_False_1_14336_128_2_4', 'test_correctness_int8_w8a16_True_False_16_14336_128_1_1', 'test_correctness_int8_w8a8_True_True_64_7186_128_2_8', 'test_correctness_int8_w8a16_True_True_64_3584_128_2_8', 'test_correctness_True_64_14336_4096_2_8', 'test_correctness_fp8_fp8_type0_True_True_64_14336_4096_2_8', 'test_correctness_True_64_7186_128_2_8', 'test_correctness_int8_w8a16_True_False_64_14336_4096_2_8', 'test_correctness_fp8_fp8_type1_True_True_16_14336_128_1_1', 'test_correctness_fp8_fp8_type1_True_True_16_14336_1_2_4', 'test_correctness_False_16_14336_128_1_1', 'test_correctness_True_64_1792_128_2_8', 'test_correctness_int8_w8a8_True_False_16_14336_128_1_1', 'test_correctness_int8_w8a16_True_False_64_3584_128_2_8', 'test_correctness_True_16_14336_1_2_4', 'test_correctness_int8_w8a16_True_False_64_7186_128_2_8', 'test_correctness_int8_w8a16_True_True_64_7186_128_2_8', 'test_correctness_int8_w8a16_True_False_64_64_128_2_8', 'test_correctness_True_1_14336_128_2_4', 'test_correctness_True_64_64_128_2_8', 'test_correctness_fp8_fp8_type1_True_True_64_1792_128_2_8', 'test_correctness_fp8_fp8_type0_True_True_1_14336_128_2_4', 'test_correctness_int8_w8a16_True_True_64_14336_4096_2_8', 'test_correctness_int8_w8a8_True_False_64_3584_128_2_8', 'test_correctness_fp8_fp8_type0_True_False_64_1792_128_2_8', 'test_correctness_fp8_fp8_type0_True_False_16_14336_1_2_4', 'test_correctness_fp8_fp8_type0_True_False_64_14336_4096_2_8'}___________________ test_correctness[True-64-14336-4096-2-8] ___________________

M = 64, N = 14336, K = 4096, top_k = 2, E = 8, routed_weight = True
request = <FixtureRequest for <Function test_correctness[True-64-14336-4096-2-8]>>
dtype = torch.float16

    @pytest.mark.parametrize("M, N, K, top_k, E", [
        (64, 14336, 4096, 2, 8),
        (16, 14336, 1, 2, 4),
        (1, 14336, 128, 2, 4),
        (3, 14336, 128, 2, 4),
        (16, 14336, 128, 1, 4),
        (16, 14336, 128, 1, 1),
        (64, 7186, 128, 2, 8),
        (64, 3584, 128, 2, 8),
        (64, 1792, 128, 2, 8),
        (64, 64, 128, 2, 8),
    ])
    @pytest.mark.parametrize('routed_weight', [True, False])
    def test_correctness(M: int, N: int, K: int, top_k: int, E: int, routed_weight: bool, request, dtype=torch.float16):
    
        a, b, c, metadata = input_helper(M, N, K, top_k, E, routed_weight=routed_weight, use_fp8_w8a8=False,
                                         use_int8_w8a16=False, use_int8_w8a8=False, fp8_type=None, dtype=dtype)
    
>       tri_out = moe_gemm(a, b, c, metadata)

sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/moe_gemm_py_gen_triton_code_804264.py:589: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/moe_gemm_py_gen_triton_code_804264.py:486: in moe_gemm
    moe_gemm_kernel[grid](a, b, c, a_descale,
/var/lib/jenkins/triton/python/triton/runtime/jit.py:330: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
/var/lib/jenkins/triton/python/triton/runtime/jit.py:623: in run
    kernel = self.compile(
/var/lib/jenkins/triton/python/triton/compiler/compiler.py:273: in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <triton.compiler.compiler.ASTSource object at 0x7580fdc4efc0>
options = HIPOptions(num_warps=4, waves_per_eu=1, num_stages=2, num_ctas=1, num_buffers_warp_spec=0, num_consumer_groups=0, reg_...=1, allow_flush_denorm=False, max_num_imprecise_acc_default=0, backend_name='hip', instruction_sched_variant='default')
codegen_fns = {'min_dot_size': <function min_dot_size.<locals>.<lambda> at 0x7580fd6bfce0>}
module_map = {'triton.language.extra.libdevice': <module 'triton.language.extra.hip.libdevice' from '/var/lib/jenkins/triton/python/triton/language/extra/hip/libdevice.py'>}
context = <triton._C.libtriton.ir.context object at 0x758903575170>

    def make_ir(self, options, codegen_fns, module_map, context):
>       return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
                           module_map=module_map)
E       triton.compiler.errors.CompilationError: at 31:16:
E               block. It determines which expert matrix from B should be used for  
E               each block in A.  
E           This kernel performs the multiplication of a token by its corresponding  
E           expert matrix as determined by `expert_ids`. The sorting of  
E           `sorted_token_ids` by expert index and padding ensures divisibility by  
E           BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix  
E           multiplication across different blocks processed by the same expert.  
E           """
E           pid = tl.program_id(axis=0)
E           pid = expert_ids_ptr + pid
E           epid = tl.load(pid)
E           E = tl.load(EPIds + 1)
E                       ^
E       NameError('EPIds is not defined')

/var/lib/jenkins/triton/python/triton/compiler/compiler.py:100: CompilationError
2025-08-08_09-46-32 => File: multreduce_matmul_dot_kernel.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/multreduce_matmul_dot_kernel_py_gen_triton_code_700949_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/multreduce_matmul_dot_kernel_py_gen_triton_code_700949.py _
ImportError while importing test module '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/multreduce_matmul_dot_kernel_py_gen_triton_code_700949.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/multreduce_matmul_dot_kernel_py_gen_triton_code_700949.py:4: in <module>
    from triton.compiler import AttrsDescriptor
E   ImportError: cannot import name 'AttrsDescriptor' from 'triton.compiler' (/var/lib/jenkins/triton/python/triton/compiler/__init__.py)
2025-08-08_10-26-32 => File: triton_multreduce_matmul_kernel.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Command '['python3 /root/sapmajum/dev/TB-eval/geak_eval/evaluators/ROCm_correctness.py --gen_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/triton_multreduce_matmul_kernel_py_gen_triton_code_915635.py --ref_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/triton_multreduce_matmul_kernel_py_ref_triton_code_596389.py --atol 0.01 --rtol 0.01 --global_timeout 2400 --verbose']' timed out after 2400 seconds
2025-08-08_10-27-29 => File: gemm.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/gemm_py_gen_triton_code_312761_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/gemm_py_gen_triton_code_312761.py _
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/python.py:617: in _importtestmodule
    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/pathlib.py:564: in import_path
    importlib.import_module(module_name)
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:169: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:351: in _rewrite_test
    tree = ast.parse(source, filename=strfn)
/opt/conda/envs/py_3.12/lib/python3.12/ast.py:52: in parse
    return compile(source, filename, mode, flags,
E     File "/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/gemm_py_gen_triton_code_312761.py", line 358
E       "intermediate_block_cache_lse": [mo.detach().cpu().numpy
E                                       ^
E   SyntaxError: '[' was never closed
2025-08-08_10-29-02 => File: layernorm.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/layernorm_py_gen_triton_code_201380_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/layernorm_py_gen_triton_code_201380.py _
ImportError while importing test module '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/layernorm_py_gen_triton_code_201380.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/layernorm_py_gen_triton_code_201380.py:6: in <module>
    from model_center.engine.triton_kernel.llama_impl import get_cuda_autotune_config, layernorm_kernel as c_layernorm_kernel
E   ModuleNotFoundError: No module named 'model_center'
2025-08-08_10-30-33 => File: softmax.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/softmax_py_gen_triton_code_911960_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/softmax_py_gen_triton_code_911960.py _
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/softmax_py_gen_triton_code_911960.py:199: in <module>
    @pytest.mark.parametrize('M, N', [(1823, 781), (1, 1), (128, 1), (1, 128), (8192, 8192), (4096, 8192), (359, 1),
E   NameError: name 'pytest' is not defined
2025-08-08_10-31-06 => File: test_chained_dot_fp8.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_chained_dot_fp8_py_gen_triton_code_931659_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_chained_dot_fp8_py_gen_triton_code_931659.py _
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_chained_dot_fp8_py_gen_triton_code_931659.py:113: in <module>
    @pytest.mark.parametrize('M, N, D, dtype, msize', [(*shape, dtype, msize)
E   NameError: name 'pytest' is not defined
2025-08-08_10-31-31 => File: test_cast_matmul.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_cast_matmul_py_gen_triton_code_841575_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_cast_matmul_py_gen_triton_code_841575.py _
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_cast_matmul_py_gen_triton_code_841575.py:1: in <module>
    @triton.jit
E   NameError: name 'triton' is not defined
2025-08-08_10-32-12 => File: test_gemm_fusion.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_gemm_fusion_py_gen_triton_code_813115_py.pt'
2025-08-08_10-32-51 => File: test_chained_matmul.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_chained_matmul'}_____________________________ test_chained_matmul ______________________________

request = <FixtureRequest for <Function test_chained_matmul>>, device = 'cuda'

    def test_chained_matmul(request, device='cuda'):
        # Regression test for issue #1601
        set_seed()
    
    
        m, n, k = 32, 64, 128
        block_m, block_n, block_k = 16, 32, k
    
        grid = (triton.cdiv(m, block_m), )
        a = torch.randint(low=0, high=2, size=(m, k), dtype=torch.float16, device=device)
        b = torch.randint(low=0, high=2, size=(n, k), dtype=torch.float16, device=device)
        c = torch.randint_like(b, low=0, high=2)
        triton_result = torch.zeros_like(a)
    
        torch_result = chained_matmul_reference(a, b, c)
>       chained_matmul_kernel[grid](
            a, b, c, triton_result, m, n, k,  #
            block_m=block_m, block_n=block_n, block_k=block_k)

sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_chained_matmul_py_gen_triton_code_780419.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/var/lib/jenkins/triton/python/triton/runtime/jit.py:330: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
/var/lib/jenkins/triton/python/triton/runtime/jit.py:623: in run
    kernel = self.compile(
/var/lib/jenkins/triton/python/triton/compiler/compiler.py:273: in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <triton.compiler.compiler.ASTSource object at 0x70bb54d8d6d0>
options = HIPOptions(num_warps=4, waves_per_eu=1, num_stages=2, num_ctas=1, num_buffers_warp_spec=0, num_consumer_groups=0, reg_...=1, allow_flush_denorm=False, max_num_imprecise_acc_default=0, backend_name='hip', instruction_sched_variant='default')
codegen_fns = {'min_dot_size': <function min_dot_size.<locals>.<lambda> at 0x70bb54d9d260>}
module_map = {'triton.language.extra.libdevice': <module 'triton.language.extra.hip.libdevice' from '/var/lib/jenkins/triton/python/triton/language/extra/hip/libdevice.py'>}
context = <triton._C.libtriton.ir.context object at 0x70bb5574c570>

    def make_ir(self, options, codegen_fns, module_map, context):
>       return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
                           module_map=module_map)
E       triton.compiler.errors.CompilationError: def chained_matmul_kernel(A, B, C, out, m, n, k: tl.constexpr, block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr):
E           """
E           Brief description of the kernel:
E           This Triton JIT-compiled kernel, `chained_matmul_kernel`, is designed to efficiently compute
E           a chained matrix multiplication of the form `(A @ B.T) @ C` on a GPU.
E           It takes three input matrices: `A` of shape `(m, k)`, `B` of shape `(n, k)`, and `C` of shape `(n, k)`.
E           The transpose of `B` is used in the first multiplication, resulting in an intermediate
E           matrix of shape `(m, n)`. This intermediate result is then multiplied by `C` (after `C` is
E           effectively processed column-wise due to the dot product with the intermediate, or rather,
E           the accumulation logic results in an `(m,k)` output from `(m,n) @ (n,k)` where the second `(n,k)`
E           is `C`). The final output matrix `out` has the shape `(m, k)`.
E           The kernel utilizes a tiled approach for parallel processing. 
E       
E       
E           The user should implement the logic for (A @ B.T) @ C using Triton programming constructs.
E           This typically involves:
E           - Calculating program IDs and offsets for the current block.
E           - Loading tiles of A, B, and C.
E           - Performing the dot products and accumulations in a loop over the 'n' dimension.
E           - Handling boundary conditions carefully with masks.
E           - Storing the resulting tile to the output tensor `out`
E       
E           """
E           pid = tl.program_id(0)
E           offs_m = pid * block_m + tl.arange(0, block_m)
E           offs_n = tl.arange(0, block_n)
E           A_ptrs = A + offs_m[:, None] * k + tl.arange(0, block_k)[None, :]
E           B_ptrs = B + tl.arange(0, block_k)[:, None] * n + offs_n[None, :]
E           C_ptrs = C + tl.arange(0, block_k)[:, None] * n + offs_n[None, :] * k
E           accumulator = tl.zeros([block_m, block_k], dtype=tl.float32)
E           for _ in range(n // block_n):
E               a = tl.load(A_ptrs)
E               b = tl.load(B_ptrs)
E               c = tl.load(C_ptrs)
E               accumulator += tl.dot(a, b)
E               accumulator += tl.dot(a, c)
E               A_ptrs += block_k
E               B_ptrs += block_k * n
E               C_ptrs += block_k * n
E           offs_m = pid * block_m + tl.arange(0, block_m)
E           offs_k = tl.arange(0, block_k)
E           out_ptrs = out + offs_m[:, None] * k + offs_k[None, :]
E           tl.store(out_ptrs, accumulator)
E       
E       ValueError('Cannot make_shape_compatible: incompatible dimensions at index 1: 128 and 32')

/var/lib/jenkins/triton/python/triton/compiler/compiler.py:100: CompilationError
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_chained_matmul_py_gen_triton_code_780419.py::test_chained_matmul - triton.compiler.errors.CompilationError: def chained_matmul_kernel(A, B, C, out, m, n, k: tl.constexpr, block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr):
    """
    Brief description of the kernel:
    This Triton JIT-compiled kernel, `chained_matmul_kernel`, is designed to efficiently compute
    a chained matrix multiplication of the form `(A @ B.T) @ C` on a GPU.
    It takes three input matrices: `A` of shape `(m, k)`, `B` of shape `(n, k)`, and `C` of shape `(n, k)`.
    The transpose of `B` is used in the first multiplication, resulting in an intermediate
    matrix of shape `(m, n)`. This intermediate result is then multiplied by `C` (after `C` is
    effectively processed column-wise due to the dot product with the intermediate, or rather,
    the accumulation logic results in an `(m,k)` output from `(m,n) @ (n,k)` where the second `(n,k)`
    is `C`). The final output matrix `out` has the shape `(m, k)`.
    The kernel utilizes a tiled approach for parallel processing. 


    The user should implement the logic for (A @ B.T) @ C using Triton programming constructs.
    This typically involves:
    - Calculating program IDs and offsets for the current block.
    - Loading tiles of A, B, and C.
    - Performing the dot products and accumulations in a loop over the 'n' dimension.
    - Handling boundary conditions carefully with masks.
    - Storing the resulting tile to the output tensor `out`

    """
    pid = tl.program_id(0)
    offs_m = pid * block_m + tl.arange(0, block_m)
    offs_n = tl.arange(0, block_n)
    A_ptrs = A + offs_m[:, None] * k + tl.arange(0, block_k)[None, :]
    B_ptrs = B + tl.arange(0, block_k)[:, None] * n + offs_n[None, :]
    C_ptrs = C + tl.arange(0, block_k)[:, None] * n + offs_n[None, :] * k
    accumulator = tl.zeros([block_m, block_k], dtype=tl.float32)
    for _ in range(n // block_n):
        a = tl.load(A_ptrs)
        b = tl.load(B_ptrs)
        c = tl.load(C_ptrs)
        accumulator += tl.dot(a, b)
        accumulator += tl.dot(a, c)
        A_ptrs += block_k
        B_ptrs += block_k * n
        C_ptrs += block_k * n
    offs_m = pid * block_m + tl.arange(0, block_m)
    offs_k = tl.arange(0, block_k)
    out_ptrs = out + offs_m[:, None] * k + offs_k[None, :]
    tl.store(out_ptrs, accumulator)

ValueError('Cannot make_shape_compatible: incompatible dimensions at index 1: 128 and 32')
================= 1 failed, 1 passed, 151 deselected in 11.95s =================

Generated call accuracy: False
Execution accuracy: False
Match percentage: 0.00%
Error: Key mismatch at : Missing keys: {'test_chained_matmul'} 
2025-08-08_10-33-20 => File: test_batched_vecmat.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_batched_vecmat_py_gen_triton_code_336953_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_batched_vecmat_py_gen_triton_code_336953.py _
ImportError while importing test module '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_batched_vecmat_py_gen_triton_code_336953.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_batched_vecmat_py_gen_triton_code_336953.py:4: in <module>
    from deepspeed.accelerator import get_accelerator
E   ModuleNotFoundError: No module named 'deepspeed'
2025-08-08_10-33-50 => File: test_iv_dependent_matmul.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_iv_dependent_matmul_py_gen_triton_code_679326_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_iv_dependent_matmul_py_gen_triton_code_679326.py _
ImportError while importing test module '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_iv_dependent_matmul_py_gen_triton_code_679326.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_iv_dependent_matmul_py_gen_triton_code_679326.py:7: in <module>
    from flash import flash_decoding_attention, flash_decoding_attention_triton_wrapper, softmax, new_kcache_flash_decoding_attention, new_kcache_flash_decoding_attention_triton_wrapper
E   ModuleNotFoundError: No module named 'flash'
2025-08-08_10-34-22 => File: test_reverse_range.py, Call Status: True, Exec Status: False, difficulty: -1, stderr: Value mismatch at test_reverse_range: Tensor-likes are not close!

Mismatched elements: 507 / 512 (99.0%)
Greatest absolute difference: 5.198028564453125 at index (199,) (up to 0.01 allowed)
Greatest relative difference: 405.75860595703125 at index (161,) (up to 0.01 allowed)______________________________ test_reverse_range ______________________________

request = <FixtureRequest for <Function test_reverse_range>>, device = 'cuda'

    def test_reverse_range(request, device='cuda'):
        set_seed()
    
    
        data = torch.randn((516, ), dtype=torch.float32, device=device)
        res = torch.empty((512, ), dtype=torch.float32, device=device)
        reverse_range[(1, )](data, res)
        ref = torch.flip(data[1:513], [0])
    
        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])
    
        ################### save tri_out in result_gold ###################
        test_case_name = request.node.name
        sanitized_key_name = test_case_name.replace("::", "_").replace("[", "_").replace("]", "").replace("-", "_")
        result_gold[sanitized_key_name] = res.clone().detach().cpu()
        ###################################################################
    
    
>       assert (res == ref).all()
E       AssertionError: assert tensor(False, device='cuda:0')
E        +  where tensor(False, device='cuda:0') = <built-in method all of Tensor object at 0x77c09830c460>()
E        +    where <built-in method all of Tensor object at 0x77c09830c460> = tensor([ 1.94...vice='cuda:0') == tensor([ 1.07...vice='cuda:0')
E             Full diff:
E             - tensor([ 1.0769e+00, -8.5997e-01,  3.6960e-01,  1.9429e-01,  5.2361e-01,
E             -         -2.0448e-01, -1.0163e+00, -5.3262e-01, -5.6356e-01,  4.4331e-01,
E             -         -7.6480e-01, -1.5283e+00,  2.9330e-01,  2.0357e-01, -5.0518e-02,
E             -         -1.4092e-01, -1.6877e+00, -9.1780e-01, -4.8736e-01,  2.1481e-02,
E             -         -1.1530e+00,  1.7362e+00, -9.6700e-03, -1.0992e+00, -9.5087e-01,
E             -          3.0236e-01,  9.4407e-01,  3.0331e+00, -2.0096e-02, -5.5395e-01,
E             -          5.1846e-01, -4.1567e-01, -3.2510e-01,  8.7472e-01,  5.4741e-01,
E             -          9.0173e-01,  2.1771e-01, -3.9101e-01,  1.0265e-01, -7.7151e-01,
E             -          3.5965e-01,  4.3939e-01, -9.9589e-01,  1.8399e-01,  1.5378e+00,
E             -          8.2784e-02, -8.3349e-02,  1.5396e-01, -3.0450e-01, -1.0052e+00,
E             + tensor([ 1.9402e-01,  2.1614e+00, -1.7205e-01,  8.4906e-01, -1.9244e+00,
E             +          6.5299e-01, -6.4944e-01, -8.1753e-01,  5.2796e-01, -1.2753e+00,
E             +         -1.6621e+00, -3.0331e-01, -9.2570e-02,  1.9924e-01, -1.1204e+00,
E             +          1.8577e+00, -7.1452e-01,  6.8811e-01,  7.9683e-01, -3.3403e-02,
E             +          1.4917e+00, -5.1651e-01, -2.5410e-01,  1.4746e+00, -3.2604e-01,
E             +         -1.1600e+00,  2.3551e+00, -6.9245e-01,  1.8374e-01, -1.1835e+00,
E             +         -1.8029e+00, -1.5808e+00,  8.3867e-01,  1.4192e+00,  6.4694e-01,
E             +          4.2527e-01, -1.5892e+00,  6.2235e-01,  1.6898e+00, -6.6480e-01,
E             +          9.4254e-01,  7.8325e-02,  8.4656e-02, -1.4083e-01,  3.3156e-01,
E             +         -5.8898e-01, -1.0723e+00,  9.5396e-02, -3.3469e-01, -5.2580e-01,
E             +         -8.7763e-01,  3.9383e-01,  1.6396e-01, -1.9768e-01,  1.0104e+00,
E             +         -1.3482e+00, -3.4977e-01, -6.4427e-01,  4.4678e-01, -5.3711e-01,
E             +          1.2423e+00, -8.1460e-01,  2.5015e-01, -4.2726e-01,  1.1044e+00,
E             +         -1.1028e+00,  5.5433e-01, -1.2847e+00, -3.8158e-01,  5.1395e-01,
E             +          1.0019e-01,  2.5863e-01,  3.6168e-01,  2.2787e+00,  2.3345e-02,
E             +          1.5828e+00, -1.1592e+00,  9.4839e-01, -4.5735e-01,  7.6055e-01,
E             +         -5.7868e-01, -7.0502e-01, -7.2339e-01, -5.0706e-01, -4.3985e-01,
E             +         -4.1817e-01,  1.7414e-01,  4.4268e-01,  5.0690e-01, -1.2168e+00,
E             +         -2.7187e-01,  2.7655e-01, -1.4398e+00, -6.4632e-01,  7.4870e-02,
E             +          1.9388e-01,  5.9601e-01,  2.3220e-01,  1.1415e+00, -6.8171e-01,
E             +         -1.6531e+00,  6.0357e-03,  1.3815e+00,  1.2704e+00,  2.3238e-02,
E             +         -1.3001e+00, -7.5094e-01,  3.7562e-01, -5.4744e-01, -3.9641e-02,
E             +         -7.7786e-01, -2.5019e+00,  7.0002e-01, -9.3775e-02, -2.1626e-01,
E             +          4.4839e-01, -3.1520e-01,  2.1638e-02,  6.2535e-01,  2.4658e-01,
E             +          7.4856e-01, -1.1692e-01, -1.0216e-01, -5.0108e-01, -5.0489e-01,
E             +         -1.2072e+00, -2.4376e-01, -6.7843e-01,  1.9729e-01,  9.7822e-01,
E             +         -2.8668e-02,  1.6826e+00,  1.0909e+00, -9.9209e-01, -6.7126e-01,
E             +          1.7196e+00,  2.4606e+00, -6.1984e-01,  1.2714e+00, -2.7987e-01,
E             +          4.3597e-01,  4.2602e-01,  1.0646e+00, -2.0280e+00, -6.3258e-01,
E             +          2.1106e+00, -9.4746e-02,  2.3588e-01, -7.3007e-01, -1.6857e+00,
E             +          9.1142e-01,  7.8855e-01, -6.2873e-01,  2.1596e+00,  1.1642e+00,
E             +         -4.2566e-01,  2.3932e-01, -1.2777e+00, -1.2065e-01, -6.0658e-01,
E             +         -8.0770e-01,  2.1475e-03, -1.9441e-01,  7.4935e-01,  8.1180e-01,
E             +          8.2455e-01, -6.0982e-01,  1.2988e+00, -1.0732e+00, -2.3305e-02,
E             +         -3.6570e-01, -4.8658e-01,  1.4972e-01, -1.5095e+00,  3.1030e-01,
E             +         -4.4865e-01, -1.0967e+00, -1.2699e+00, -1.5775e+00, -1.5885e+00,
E             +         -6.5611e-01,  7.6001e-01, -6.0730e-01,  3.0230e-01, -1.3438e+00,
E             +         -3.1851e-01,  5.9325e-01,  5.7222e-01, -7.5577e-01, -1.2661e-01,
E             +         -4.3212e-01, -1.3943e+00,  3.8755e-01, -2.0533e-01,  5.9966e-01,
E             +         -1.0031e+00, -3.7096e-03, -8.0195e-01,  3.9394e-01,  2.5350e+00,
E             +          2.4914e-01,  8.9708e-01,  7.8051e-01,  9.1504e-01, -3.6730e-01,
E             +          8.5807e-02, -8.4349e-01,  4.2529e-01,  1.1195e+00,  3.9966e-01,
E             +         -8.6461e-02,  8.4455e-01, -3.9797e-01, -8.6015e-02,  1.9523e+00,
E             +         -4.2219e-01,  5.4871e-01,  1.4957e-01, -1.9746e+00, -3.4508e-02,
E             +         -1.2641e+00, -2.5442e-01,  2.3327e-02,  2.0083e+00,  9.7957e-01,
E             +          1.3883e+00, -9.8531e-01, -4.8300e-01, -1.9338e-02, -1.7386e+00,
E             +          1.6839e-01,  2.6133e-01, -2.2314e-01, -8.5228e-01,  2.7336e-01,
E             +          9.9141e-01,  1.4833e+00,  3.4798e-01,  1.2578e-01, -6.5960e-01,
E             +         -1.5767e+00, -6.6381e-01,  1.4691e+00,  2.9919e-01, -5.5614e-01,
E             +          4.6092e-02, -1.0763e+00, -1.4474e-01, -1.0826e-01, -7.4525e-01,
E             +         -3.4790e-01,  9.3937e-02,  6.4621e-02, -9.2259e-01,  7.8928e-01,
E             +         -1.8010e+00, -1.9351e+00,  1.1007e+00,  2.5060e-02,  5.8018e-01,
E             +          5.3844e-01,  5.1255e-01, -1.4316e+00,  2.3788e+00, -1.5440e+00,
E             +         -1.7424e-01, -2.2441e-02,  2.1756e-01,  6.5184e-01,  1.5593e+00,
E             +          2.6897e-01,  5.0070e-01, -2.4382e-01, -8.6397e-01,  4.2776e-01,
E             +         -2.1070e-01, -1.5396e+00, -1.0755e-01, -2.5417e-02, -1.6139e-01,
E             +          5.5062e-01,  2.5429e+00, -1.6089e-01, -8.6827e-01,  1.4119e+00,
E             +          6.8959e-01,  8.4786e-01,  2.8471e-01,  1.7041e-02,  7.2862e-01,
E             +          1.1998e+00, -1.1577e-01, -1.4126e+00, -6.4224e-01,  8.8413e-01,
E             +          7.4981e-01,  6.7947e-01, -5.3078e-01, -1.7914e+00, -2.8631e-01,
E             +          8.0537e-02, -1.3001e+00,  1.3404e+00, -1.3251e+00,  2.6113e-01,
E             +          6.0385e-01, -1.4919e+00, -1.7976e+00,  1.1268e+00,  1.4026e+00,
E             +         -4.3711e-01, -3.7269e-01, -1.1810e+00, -2.6631e+00, -3.2334e-01,
E             +          1.0203e-01,  1.2566e+00, -1.3704e+00,  3.5738e-01,  1.7166e+00,
E             +         -2.3089e+00,  1.3171e+00, -1.1485e+00, -1.2994e+00, -6.5805e-02,
E             +          1.0340e+00, -2.3067e+00, -1.2807e-01,  8.5362e-01, -6.0600e-02,
E             +         -9.5431e-01, -1.7906e+00, -2.6329e-02,  9.1661e-02, -1.1820e+00,
E             +         -1.0992e+00,  2.4421e+00,  3.0939e-01, -1.5182e+00, -7.7751e-01,
E             +          7.2537e-01,  8.8934e-01,  1.0255e+00,  8.1530e-01,  2.3177e+00,
E             +         -2.2716e-01,  3.0356e+00,  5.4943e-01,  2.2876e-02, -4.0845e-01,
E             +         -2.6245e-01,  8.7350e-01, -1.2119e+00, -1.2140e+00, -3.0020e-01,
E             +         -3.7910e-01,  3.1629e-01, -1.0083e+00,  1.9894e+00,  2.2134e-01,
E             +         -9.2283e-02,  1.4385e+00,  6.1304e-01,  6.6630e-01,  7.6207e-01,
E             +          4.3683e-01, -9.6812e-02, -4.5631e-01, -1.1774e+00,  2.3460e-01,
E             +          1.6107e+00,  1.0387e+00,  1.1985e+00, -1.1631e+00,  2.3499e-01,
E             +          1.2520e+00, -1.4929e-01, -6.2696e-01,  1.1562e+00, -2.4196e+00,
E             +          6.8326e-01, -6.8725e-01, -9.1886e-02, -7.1323e-01, -1.2954e+00,
E             +          1.2162e+00, -4.7911e-02, -7.9707e-01,  4.2452e-01, -1.9377e-01,
E             +         -1.5655e-01, -1.4242e-01, -2.8475e-01, -1.4490e+00, -5.3126e-01,
E             +         -8.2427e-02, -1.6477e+00, -2.7191e-01,  1.8944e+00, -5.5650e-01,
E             +         -4.6061e-01, -3.1016e-01,  5.9091e-02,  5.1858e-01,  7.3636e-02,
E             +          1.4062e+00, -4.8689e-01,  2.9422e-01,  1.1974e+00, -2.2358e-01,
E             +          8.6294e-01,  3.6812e-01,  1.7055e+00, -9.1968e-01,  5.3774e-01,
E             +          1.1838e+00,  1.6550e+00, -2.3531e-01, -1.6573e+00,  1.9710e+00,
E             +          9.9860e-01,  7.8983e-01, -9.1255e-01,  8.2469e-01,  1.5171e+00,
E             +         -1.6480e-01,  1.0455e+00, -3.8206e-01,  6.2568e-01,  9.8945e-02,
E             +         -7.0163e-01,  1.0346e-01, -1.4413e+00,  2.4425e+00,  1.1643e+00,
E             +          2.5416e-01, -3.6792e-01,  8.4537e-01,  4.1036e-01,  4.8213e-01,
E             +         -1.8672e+00,  9.6832e-01, -9.4232e-01,  1.1530e+00,  8.0481e-01,
E             +          1.0607e+00,  5.2392e-01, -1.4033e-01,  1.3890e+00,  3.0828e-01,
E             +          4.3953e-01, -3.0868e-02, -1.0648e+00,  6.5336e-02,  2.8089e-01,
E             +          2.1108e+00, -1.3801e+00, -4.2719e-01,  9.2045e-01, -1.4202e+00,
E             -          2.9140e-01, -9.4778e-01,  5.5006e-01, -1.4202e+00,  9.2045e-01,
E             ?          ^ ^^^                     ^ ^ --         --        ^^ -
E             +          5.5006e-01, -9.4778e-01,  2.9140e-01, -1.0052e+00, -3.0450e-01,
E             ?          ^ ^ ++                    ^ ^^^           ++       ^^    +
E             +          1.5396e-01, -8.3349e-02,  8.2784e-02,  1.5378e+00,  1.8399e-01,
E             +         -9.9589e-01,  4.3939e-01,  3.5965e-01, -7.7151e-01,  1.0265e-01,
E             +         -3.9101e-01,  2.1771e-01,  9.0173e-01,  5.4741e-01,  8.7472e-01,
E             +         -3.2510e-01, -4.1567e-01,  5.1846e-01, -5.5395e-01, -2.0096e-02,
E             +          3.0331e+00,  9.4407e-01,  3.0236e-01, -9.5087e-01, -1.0992e+00,
E             +         -9.6700e-03,  1.7362e+00, -1.1530e+00,  2.1481e-02, -4.8736e-01,
E             +         -9.1780e-01, -1.6877e+00, -1.4092e-01, -5.0518e-02,  2.0357e-01,
E             +          2.9330e-01, -1.5283e+00, -7.6480e-01,  4.4331e-01, -5.6356e-01,
E             +         -5.3262e-01, -1.0163e+00, -2.0448e-01,  5.2361e-01,  1.9429e-01,
E             +          3.6960e-01, -8.5997e-01], device='cuda:0',
E             -         -4.2719e-01, -1.3801e+00,  2.1108e+00,  2.8089e-01,  6.5336e-02,
E             -         -1.0648e+00, -3.0868e-02,  4.3953e-01,  3.0828e-01,  1.3890e+00,
E             -         -1.4033e-01,  5.2392e-01,  1.0607e+00,  8.0481e-01,  1.1530e+00,
E             -         -9.4232e-01,  9.6832e-01, -1.8672e+00,  4.8213e-01,  4.1036e-01,
E             -          8.4537e-01, -3.6792e-01,  2.5416e-01,  1.1643e+00,  2.4425e+00,
E             -         -1.4413e+00,  1.0346e-01, -7.0163e-01,  9.8945e-02,  6.2568e-01,
E             -         -3.8206e-01,  1.0455e+00, -1.6480e-01,  1.5171e+00,  8.2469e-01,
E             -         -9.1255e-01,  7.8983e-01,  9.9860e-01,  1.9710e+00, -1.6573e+00,
E             -         -2.3531e-01,  1.6550e+00,  1.1838e+00,  5.3774e-01, -9.1968e-01,
E             -          1.7055e+00,  3.6812e-01,  8.6294e-01, -2.2358e-01,  1.1974e+00,
E             -          2.9422e-01, -4.8689e-01,  1.4062e+00,  7.3636e-02,  5.1858e-01,
E             -          5.9091e-02, -3.1016e-01, -4.6061e-01, -5.5650e-01,  1.8944e+00,
E             -         -2.7191e-01, -1.6477e+00, -8.2427e-02, -5.3126e-01, -1.4490e+00,
E             -         -2.8475e-01, -1.4242e-01, -1.5655e-01, -1.9377e-01,  4.2452e-01,
E             -         -7.9707e-01, -4.7911e-02,  1.2162e+00, -1.2954e+00, -7.1323e-01,
E             -         -9.1886e-02, -6.8725e-01,  6.8326e-01, -2.4196e+00,  1.1562e+00,
E             -         -6.2696e-01, -1.4929e-01,  1.2520e+00,  2.3499e-01, -1.1631e+00,
E             -          1.1985e+00,  1.0387e+00,  1.6107e+00,  2.3460e-01, -1.1774e+00,
E             -         -4.5631e-01, -9.6812e-02,  4.3683e-01,  7.6207e-01,  6.6630e-01,
E             -          6.1304e-01,  1.4385e+00, -9.2283e-02,  2.2134e-01,  1.9894e+00,
E             -         -1.0083e+00,  3.1629e-01, -3.7910e-01, -3.0020e-01, -1.2140e+00,
E             -         -1.2119e+00,  8.7350e-01, -2.6245e-01, -4.0845e-01,  2.2876e-02,
E             -          5.4943e-01,  3.0356e+00, -2.2716e-01,  2.3177e+00,  8.1530e-01,
E             -          1.0255e+00,  8.8934e-01,  7.2537e-01, -7.7751e-01, -1.5182e+00,
E             -          3.0939e-01,  2.4421e+00, -1.0992e+00, -1.1820e+00,  9.1661e-02,
E             -         -2.6329e-02, -1.7906e+00, -9.5431e-01, -6.0600e-02,  8.5362e-01,
E             -         -1.2807e-01, -2.3067e+00,  1.0340e+00, -6.5805e-02, -1.2994e+00,
E             -         -1.1485e+00,  1.3171e+00, -2.3089e+00,  1.7166e+00,  3.5738e-01,
E             -         -1.3704e+00,  1.2566e+00,  1.0203e-01, -3.2334e-01, -2.6631e+00,
E             -         -1.1810e+00, -3.7269e-01, -4.3711e-01,  1.4026e+00,  1.1268e+00,
E             -         -1.7976e+00, -1.4919e+00,  6.0385e-01,  2.6113e-01, -1.3251e+00,
E             -          1.3404e+00, -1.3001e+00,  8.0537e-02, -2.8631e-01, -1.7914e+00,
E             -         -5.3078e-01,  6.7947e-01,  7.4981e-01,  8.8413e-01, -6.4224e-01,
E             -         -1.4126e+00, -1.1577e-01,  1.1998e+00,  7.2862e-01,  1.7041e-02,
E             -          2.8471e-01,  8.4786e-01,  6.8959e-01,  1.4119e+00, -8.6827e-01,
E             -         -1.6089e-01,  2.5429e+00,  5.5062e-01, -1.6139e-01, -2.5417e-02,
E             -         -1.0755e-01, -1.5396e+00, -2.1070e-01,  4.2776e-01, -8.6397e-01,
E             -         -2.4382e-01,  5.0070e-01,  2.6897e-01,  1.5593e+00,  6.5184e-01,
E             -          2.1756e-01, -2.2441e-02, -1.7424e-01, -1.5440e+00,  2.3788e+00,
E             -         -1.4316e+00,  5.1255e-01,  5.3844e-01,  5.8018e-01,  2.5060e-02,
E             -          1.1007e+00, -1.9351e+00, -1.8010e+00,  7.8928e-01, -9.2259e-01,
E             -          6.4621e-02,  9.3937e-02, -3.4790e-01, -7.4525e-01, -1.0826e-01,
E             -         -1.4474e-01, -1.0763e+00,  4.6092e-02, -5.5614e-01,  2.9919e-01,
E             -          1.4691e+00, -6.6381e-01, -1.5767e+00, -6.5960e-01,  1.2578e-01,
E             -          3.4798e-01,  1.4833e+00,  9.9141e-01,  2.7336e-01, -8.5228e-01,
E             -         -2.2314e-01,  2.6133e-01,  1.6839e-01, -1.7386e+00, -1.9338e-02,
E             -         -4.8300e-01, -9.8531e-01,  1.3883e+00,  9.7957e-01,  2.0083e+00,
E             -          2.3327e-02, -2.5442e-01, -1.2641e+00, -3.4508e-02, -1.9746e+00,
E             -          1.4957e-01,  5.4871e-01, -4.2219e-01,  1.9523e+00, -8.6015e-02,
E             -         -3.9797e-01,  8.4455e-01, -8.6461e-02,  3.9966e-01,  1.1195e+00,
E             -          4.2529e-01, -8.4349e-01,  8.5807e-02, -3.6730e-01,  9.1504e-01,
E             -          7.8051e-01,  8.9708e-01,  2.4914e-01,  2.5350e+00,  3.9394e-01,
E             -         -8.0195e-01, -3.7096e-03, -1.0031e+00,  5.9966e-01, -2.0533e-01,
E             -          3.8755e-01, -1.3943e+00, -4.3212e-01, -1.2661e-01, -7.5577e-01,
E             -          5.7222e-01,  5.9325e-01, -3.1851e-01, -1.3438e+00,  3.0230e-01,
E             -         -6.0730e-01,  7.6001e-01, -6.5611e-01, -1.5885e+00, -1.5775e+00,
E             -         -1.2699e+00, -1.0967e+00, -4.4865e-01,  3.1030e-01, -1.5095e+00,
E             -          1.4972e-01, -4.8658e-01, -3.6570e-01, -2.3305e-02, -1.0732e+00,
E             -          1.2988e+00, -6.0982e-01,  8.2455e-01,  8.1180e-01,  7.4935e-01,
E             -         -1.9441e-01,  2.1475e-03, -8.0770e-01, -6.0658e-01, -1.2065e-01,
E             -         -1.2777e+00,  2.3932e-01, -4.2566e-01,  1.1642e+00,  2.1596e+00,
E             -         -6.2873e-01,  7.8855e-01,  9.1142e-01, -1.6857e+00, -7.3007e-01,
E             -          2.3588e-01, -9.4746e-02,  2.1106e+00, -6.3258e-01, -2.0280e+00,
E             -          1.0646e+00,  4.2602e-01,  4.3597e-01, -2.7987e-01,  1.2714e+00,
E             -         -6.1984e-01,  2.4606e+00,  1.7196e+00, -6.7126e-01, -9.9209e-01,
E             -          1.0909e+00,  1.6826e+00, -2.8668e-02,  9.7822e-01,  1.9729e-01,
E             -         -6.7843e-01, -2.4376e-01, -1.2072e+00, -5.0489e-01, -5.0108e-01,
E             -         -1.0216e-01, -1.1692e-01,  7.4856e-01,  2.4658e-01,  6.2535e-01,
E             -          2.1638e-02, -3.1520e-01,  4.4839e-01, -2.1626e-01, -9.3775e-02,
E             -          7.0002e-01, -2.5019e+00, -7.7786e-01, -3.9641e-02, -5.4744e-01,
E             -          3.7562e-01, -7.5094e-01, -1.3001e+00,  2.3238e-02,  1.2704e+00,
E             -          1.3815e+00,  6.0357e-03, -1.6531e+00, -6.8171e-01,  1.1415e+00,
E             -          2.3220e-01,  5.9601e-01,  1.9388e-01,  7.4870e-02, -6.4632e-01,
E             -         -1.4398e+00,  2.7655e-01, -2.7187e-01, -1.2168e+00,  5.0690e-01,
E             -          4.4268e-01,  1.7414e-01, -4.1817e-01, -4.3985e-01, -5.0706e-01,
E             -         -7.2339e-01, -7.0502e-01, -5.7868e-01,  7.6055e-01, -4.5735e-01,
E             -          9.4839e-01, -1.1592e+00,  1.5828e+00,  2.3345e-02,  2.2787e+00,
E             -          3.6168e-01,  2.5863e-01,  1.0019e-01,  5.1395e-01, -3.8158e-01,
E             -         -1.2847e+00,  5.5433e-01, -1.1028e+00,  1.1044e+00, -4.2726e-01,
E             -          2.5015e-01, -8.1460e-01,  1.2423e+00, -5.3711e-01,  4.4678e-01,
E             -         -6.4427e-01, -3.4977e-01, -1.3482e+00,  1.0104e+00, -1.9768e-01,
E             -          1.6396e-01,  3.9383e-01, -8.7763e-01, -5.2580e-01, -3.3469e-01,
E             -          9.5396e-02, -1.0723e+00, -5.8898e-01,  3.3156e-01, -1.4083e-01,
E             -          8.4656e-02,  7.8325e-02,  9.4254e-01, -6.6480e-01,  1.6898e+00,
E             -          6.2235e-01, -1.5892e+00,  4.2527e-01,  6.4694e-01,  1.4192e+00,
E             -          8.3867e-01, -1.5808e+00, -1.8029e+00, -1.1835e+00,  1.8374e-01,
E             -         -6.9245e-01,  2.3551e+00, -1.1600e+00, -3.2604e-01,  1.4746e+00,
E             -         -2.5410e-01, -5.1651e-01,  1.4917e+00, -3.3403e-02,  7.9683e-01,
E             -          6.8811e-01, -7.1452e-01,  1.8577e+00, -1.1204e+00,  1.9924e-01,
E             -         -9.2570e-02, -3.0331e-01, -1.6621e+00, -1.2753e+00,  5.2796e-01,
E             -         -8.1753e-01, -6.4944e-01,  6.5299e-01, -1.9244e+00,  8.4906e-01,
E             -         -1.7205e-01,  2.1614e+00], device='cuda:0',
E               ).all

sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_reverse_range_py_gen_triton_code_719222.py:74: AssertionError
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_reverse_range_py_gen_triton_code_719222.py::test_reverse_range - AssertionError: assert tensor(False, device='cuda:0')
 +  where tensor(False, device='cuda:0') = <built-in method all of Tensor object at 0x77c09830c460>()
 +    where <built-in method all of Tensor object at 0x77c09830c460> = tensor([ 1.94...vice='cuda:0') == tensor([ 1.07...vice='cuda:0')
      Full diff:
      - tensor([ 1.0769e+00, -8.5997e-01,  3.6960e-01,  1.9429e-01,  5.2361e-01,
      -         -2.0448e-01, -1.0163e+00, -5.3262e-01, -5.6356e-01,  4.4331e-01,
      -         -7.6480e-01, -1.5283e+00,  2.9330e-01,  2.0357e-01, -5.0518e-02,
      -         -1.4092e-01, -1.6877e+00, -9.1780e-01, -4.8736e-01,  2.1481e-02,
      -         -1.1530e+00,  1.7362e+00, -9.6700e-03, -1.0992e+00, -9.5087e-01,
      -          3.0236e-01,  9.4407e-01,  3.0331e+00, -2.0096e-02, -5.5395e-01,
      -          5.1846e-01, -4.1567e-01, -3.2510e-01,  8.7472e-01,  5.4741e-01,
      -          9.0173e-01,  2.1771e-01, -3.9101e-01,  1.0265e-01, -7.7151e-01,
      -          3.5965e-01,  4.3939e-01, -9.9589e-01,  1.8399e-01,  1.5378e+00,
      -          8.2784e-02, -8.3349e-02,  1.5396e-01, -3.0450e-01, -1.0052e+00,
      + tensor([ 1.9402e-01,  2.1614e+00, -1.7205e-01,  8.4906e-01, -1.9244e+00,
      +          6.5299e-01, -6.4944e-01, -8.1753e-01,  5.2796e-01, -1.2753e+00,
      +         -1.6621e+00, -3.0331e-01, -9.2570e-02,  1.9924e-01, -1.1204e+00,
      +          1.8577e+00, -7.1452e-01,  6.8811e-01,  7.9683e-01, -3.3403e-02,
      +          1.4917e+00, -5.1651e-01, -2.5410e-01,  1.4746e+00, -3.2604e-01,
      +         -1.1600e+00,  2.3551e+00, -6.9245e-01,  1.8374e-01, -1.1835e+00,
      +         -1.8029e+00, -1.5808e+00,  8.3867e-01,  1.4192e+00,  6.4694e-01,
      +          4.2527e-01, -1.5892e+00,  6.2235e-01,  1.6898e+00, -6.6480e-01,
      +          9.4254e-01,  7.8325e-02,  8.4656e-02, -1.4083e-01,  3.3156e-01,
      +         -5.8898e-01, -1.0723e+00,  9.5396e-02, -3.3469e-01, -5.2580e-01,
      +         -8.7763e-01,  3.9383e-01,  1.6396e-01, -1.9768e-01,  1.0104e+00,
      +         -1.3482e+00, -3.4977e-01, -6.4427e-01,  4.4678e-01, -5.3711e-01,
      +          1.2423e+00, -8.1460e-01,  2.5015e-01, -4.2726e-01,  1.1044e+00,
      +         -1.1028e+00,  5.5433e-01, -1.2847e+00, -3.8158e-01,  5.1395e-01,
      +          1.0019e-01,  2.5863e-01,  3.6168e-01,  2.2787e+00,  2.3345e-02,
      +          1.5828e+00, -1.1592e+00,  9.4839e-01, -4.5735e-01,  7.6055e-01,
      +         -5.7868e-01, -7.0502e-01, -7.2339e-01, -5.0706e-01, -4.3985e-01,
      +         -4.1817e-01,  1.7414e-01,  4.4268e-01,  5.0690e-01, -1.2168e+00,
      +         -2.7187e-01,  2.7655e-01, -1.4398e+00, -6.4632e-01,  7.4870e-02,
      +          1.9388e-01,  5.9601e-01,  2.3220e-01,  1.1415e+00, -6.8171e-01,
      +         -1.6531e+00,  6.0357e-03,  1.3815e+00,  1.2704e+00,  2.3238e-02,
      +         -1.3001e+00, -7.5094e-01,  3.7562e-01, -5.4744e-01, -3.9641e-02,
      +         -7.7786e-01, -2.5019e+00,  7.0002e-01, -9.3775e-02, -2.1626e-01,
      +          4.4839e-01, -3.1520e-01,  2.1638e-02,  6.2535e-01,  2.4658e-01,
      +          7.4856e-01, -1.1692e-01, -1.0216e-01, -5.0108e-01, -5.0489e-01,
      +         -1.2072e+00, -2.4376e-01, -6.7843e-01,  1.9729e-01,  9.7822e-01,
      +         -2.8668e-02,  1.6826e+00,  1.0909e+00, -9.9209e-01, -6.7126e-01,
      +          1.7196e+00,  2.4606e+00, -6.1984e-01,  1.2714e+00, -2.7987e-01,
      +          4.3597e-01,  4.2602e-01,  1.0646e+00, -2.0280e+00, -6.3258e-01,
      +          2.1106e+00, -9.4746e-02,  2.3588e-01, -7.3007e-01, -1.6857e+00,
      +          9.1142e-01,  7.8855e-01, -6.2873e-01,  2.1596e+00,  1.1642e+00,
      +         -4.2566e-01,  2.3932e-01, -1.2777e+00, -1.2065e-01, -6.0658e-01,
      +         -8.0770e-01,  2.1475e-03, -1.9441e-01,  7.4935e-01,  8.1180e-01,
      +          8.2455e-01, -6.0982e-01,  1.2988e+00, -1.0732e+00, -2.3305e-02,
      +         -3.6570e-01, -4.8658e-01,  1.4972e-01, -1.5095e+00,  3.1030e-01,
      +         -4.4865e-01, -1.0967e+00, -1.2699e+00, -1.5775e+00, -1.5885e+00,
      +         -6.5611e-01,  7.6001e-01, -6.0730e-01,  3.0230e-01, -1.3438e+00,
      +         -3.1851e-01,  5.9325e-01,  5.7222e-01, -7.5577e-01, -1.2661e-01,
      +         -4.3212e-01, -1.3943e+00,  3.8755e-01, -2.0533e-01,  5.9966e-01,
      +         -1.0031e+00, -3.7096e-03, -8.0195e-01,  3.9394e-01,  2.5350e+00,
      +          2.4914e-01,  8.9708e-01,  7.8051e-01,  9.1504e-01, -3.6730e-01,
      +          8.5807e-02, -8.4349e-01,  4.2529e-01,  1.1195e+00,  3.9966e-01,
      +         -8.6461e-02,  8.4455e-01, -3.9797e-01, -8.6015e-02,  1.9523e+00,
      +         -4.2219e-01,  5.4871e-01,  1.4957e-01, -1.9746e+00, -3.4508e-02,
      +         -1.2641e+00, -2.5442e-01,  2.3327e-02,  2.0083e+00,  9.7957e-01,
      +          1.3883e+00, -9.8531e-01, -4.8300e-01, -1.9338e-02, -1.7386e+00,
      +          1.6839e-01,  2.6133e-01, -2.2314e-01, -8.5228e-01,  2.7336e-01,
      +          9.9141e-01,  1.4833e+00,  3.4798e-01,  1.2578e-01, -6.5960e-01,
      +         -1.5767e+00, -6.6381e-01,  1.4691e+00,  2.9919e-01, -5.5614e-01,
      +          4.6092e-02, -1.0763e+00, -1.4474e-01, -1.0826e-01, -7.4525e-01,
      +         -3.4790e-01,  9.3937e-02,  6.4621e-02, -9.2259e-01,  7.8928e-01,
      +         -1.8010e+00, -1.9351e+00,  1.1007e+00,  2.5060e-02,  5.8018e-01,
      +          5.3844e-01,  5.1255e-01, -1.4316e+00,  2.3788e+00, -1.5440e+00,
      +         -1.7424e-01, -2.2441e-02,  2.1756e-01,  6.5184e-01,  1.5593e+00,
      +          2.6897e-01,  5.0070e-01, -2.4382e-01, -8.6397e-01,  4.2776e-01,
      +         -2.1070e-01, -1.5396e+00, -1.0755e-01, -2.5417e-02, -1.6139e-01,
      +          5.5062e-01,  2.5429e+00, -1.6089e-01, -8.6827e-01,  1.4119e+00,
      +          6.8959e-01,  8.4786e-01,  2.8471e-01,  1.7041e-02,  7.2862e-01,
      +          1.1998e+00, -1.1577e-01, -1.4126e+00, -6.4224e-01,  8.8413e-01,
      +          7.4981e-01,  6.7947e-01, -5.3078e-01, -1.7914e+00, -2.8631e-01,
      +          8.0537e-02, -1.3001e+00,  1.3404e+00, -1.3251e+00,  2.6113e-01,
      +          6.0385e-01, -1.4919e+00, -1.7976e+00,  1.1268e+00,  1.4026e+00,
      +         -4.3711e-01, -3.7269e-01, -1.1810e+00, -2.6631e+00, -3.2334e-01,
      +          1.0203e-01,  1.2566e+00, -1.3704e+00,  3.5738e-01,  1.7166e+00,
      +         -2.3089e+00,  1.3171e+00, -1.1485e+00, -1.2994e+00, -6.5805e-02,
      +          1.0340e+00, -2.3067e+00, -1.2807e-01,  8.5362e-01, -6.0600e-02,
      +         -9.5431e-01, -1.7906e+00, -2.6329e-02,  9.1661e-02, -1.1820e+00,
      +         -1.0992e+00,  2.4421e+00,  3.0939e-01, -1.5182e+00, -7.7751e-01,
      +          7.2537e-01,  8.8934e-01,  1.0255e+00,  8.1530e-01,  2.3177e+00,
      +         -2.2716e-01,  3.0356e+00,  5.4943e-01,  2.2876e-02, -4.0845e-01,
      +         -2.6245e-01,  8.7350e-01, -1.2119e+00, -1.2140e+00, -3.0020e-01,
      +         -3.7910e-01,  3.1629e-01, -1.0083e+00,  1.9894e+00,  2.2134e-01,
      +         -9.2283e-02,  1.4385e+00,  6.1304e-01,  6.6630e-01,  7.6207e-01,
      +          4.3683e-01, -9.6812e-02, -4.5631e-01, -1.1774e+00,  2.3460e-01,
      +          1.6107e+00,  1.0387e+00,  1.1985e+00, -1.1631e+00,  2.3499e-01,
      +          1.2520e+00, -1.4929e-01, -6.2696e-01,  1.1562e+00, -2.4196e+00,
      +          6.8326e-01, -6.8725e-01, -9.1886e-02, -7.1323e-01, -1.2954e+00,
      +          1.2162e+00, -4.7911e-02, -7.9707e-01,  4.2452e-01, -1.9377e-01,
      +         -1.5655e-01, -1.4242e-01, -2.8475e-01, -1.4490e+00, -5.3126e-01,
      +         -8.2427e-02, -1.6477e+00, -2.7191e-01,  1.8944e+00, -5.5650e-01,
      +         -4.6061e-01, -3.1016e-01,  5.9091e-02,  5.1858e-01,  7.3636e-02,
      +          1.4062e+00, -4.8689e-01,  2.9422e-01,  1.1974e+00, -2.2358e-01,
      +          8.6294e-01,  3.6812e-01,  1.7055e+00, -9.1968e-01,  5.3774e-01,
      +          1.1838e+00,  1.6550e+00, -2.3531e-01, -1.6573e+00,  1.9710e+00,
      +          9.9860e-01,  7.8983e-01, -9.1255e-01,  8.2469e-01,  1.5171e+00,
      +         -1.6480e-01,  1.0455e+00, -3.8206e-01,  6.2568e-01,  9.8945e-02,
      +         -7.0163e-01,  1.0346e-01, -1.4413e+00,  2.4425e+00,  1.1643e+00,
      +          2.5416e-01, -3.6792e-01,  8.4537e-01,  4.1036e-01,  4.8213e-01,
      +         -1.8672e+00,  9.6832e-01, -9.4232e-01,  1.1530e+00,  8.0481e-01,
      +          1.0607e+00,  5.2392e-01, -1.4033e-01,  1.3890e+00,  3.0828e-01,
      +          4.3953e-01, -3.0868e-02, -1.0648e+00,  6.5336e-02,  2.8089e-01,
      +          2.1108e+00, -1.3801e+00, -4.2719e-01,  9.2045e-01, -1.4202e+00,
      -          2.9140e-01, -9.4778e-01,  5.5006e-01, -1.4202e+00,  9.2045e-01,
      ?          ^ ^^^                     ^ ^ --         --        ^^ -
      +          5.5006e-01, -9.4778e-01,  2.9140e-01, -1.0052e+00, -3.0450e-01,
      ?          ^ ^ ++                    ^ ^^^           ++       ^^    +
      +          1.5396e-01, -8.3349e-02,  8.2784e-02,  1.5378e+00,  1.8399e-01,
      +         -9.9589e-01,  4.3939e-01,  3.5965e-01, -7.7151e-01,  1.0265e-01,
      +         -3.9101e-01,  2.1771e-01,  9.0173e-01,  5.4741e-01,  8.7472e-01,
      +         -3.2510e-01, -4.1567e-01,  5.1846e-01, -5.5395e-01, -2.0096e-02,
      +          3.0331e+00,  9.4407e-01,  3.0236e-01, -9.5087e-01, -1.0992e+00,
      +         -9.6700e-03,  1.7362e+00, -1.1530e+00,  2.1481e-02, -4.8736e-01,
      +         -9.1780e-01, -1.6877e+00, -1.4092e-01, -5.0518e-02,  2.0357e-01,
      +          2.9330e-01, -1.5283e+00, -7.6480e-01,  4.4331e-01, -5.6356e-01,
      +         -5.3262e-01, -1.0163e+00, -2.0448e-01,  5.2361e-01,  1.9429e-01,
      +          3.6960e-01, -8.5997e-01], device='cuda:0',
      -         -4.2719e-01, -1.3801e+00,  2.1108e+00,  2.8089e-01,  6.5336e-02,
      -         -1.0648e+00, -3.0868e-02,  4.3953e-01,  3.0828e-01,  1.3890e+00,
      -         -1.4033e-01,  5.2392e-01,  1.0607e+00,  8.0481e-01,  1.1530e+00,
      -         -9.4232e-01,  9.6832e-01, -1.8672e+00,  4.8213e-01,  4.1036e-01,
      -          8.4537e-01, -3.6792e-01,  2.5416e-01,  1.1643e+00,  2.4425e+00,
      -         -1.4413e+00,  1.0346e-01, -7.0163e-01,  9.8945e-02,  6.2568e-01,
      -         -3.8206e-01,  1.0455e+00, -1.6480e-01,  1.5171e+00,  8.2469e-01,
      -         -9.1255e-01,  7.8983e-01,  9.9860e-01,  1.9710e+00, -1.6573e+00,
      -         -2.3531e-01,  1.6550e+00,  1.1838e+00,  5.3774e-01, -9.1968e-01,
      -          1.7055e+00,  3.6812e-01,  8.6294e-01, -2.2358e-01,  1.1974e+00,
      -          2.9422e-01, -4.8689e-01,  1.4062e+00,  7.3636e-02,  5.1858e-01,
      -          5.9091e-02, -3.1016e-01, -4.6061e-01, -5.5650e-01,  1.8944e+00,
      -         -2.7191e-01, -1.6477e+00, -8.2427e-02, -5.3126e-01, -1.4490e+00,
      -         -2.8475e-01, -1.4242e-01, -1.5655e-01, -1.9377e-01,  4.2452e-01,
      -         -7.9707e-01, -4.7911e-02,  1.2162e+00, -1.2954e+00, -7.1323e-01,
      -         -9.1886e-02, -6.8725e-01,  6.8326e-01, -2.4196e+00,  1.1562e+00,
      -         -6.2696e-01, -1.4929e-01,  1.2520e+00,  2.3499e-01, -1.1631e+00,
      -          1.1985e+00,  1.0387e+00,  1.6107e+00,  2.3460e-01, -1.1774e+00,
      -         -4.5631e-01, -9.6812e-02,  4.3683e-01,  7.6207e-01,  6.6630e-01,
      -          6.1304e-01,  1.4385e+00, -9.2283e-02,  2.2134e-01,  1.9894e+00,
      -         -1.0083e+00,  3.1629e-01, -3.7910e-01, -3.0020e-01, -1.2140e+00,
      -         -1.2119e+00,  8.7350e-01, -2.6245e-01, -4.0845e-01,  2.2876e-02,
      -          5.4943e-01,  3.0356e+00, -2.2716e-01,  2.3177e+00,  8.1530e-01,
      -          1.0255e+00,  8.8934e-01,  7.2537e-01, -7.7751e-01, -1.5182e+00,
      -          3.0939e-01,  2.4421e+00, -1.0992e+00, -1.1820e+00,  9.1661e-02,
      -         -2.6329e-02, -1.7906e+00, -9.5431e-01, -6.0600e-02,  8.5362e-01,
      -         -1.2807e-01, -2.3067e+00,  1.0340e+00, -6.5805e-02, -1.2994e+00,
      -         -1.1485e+00,  1.3171e+00, -2.3089e+00,  1.7166e+00,  3.5738e-01,
      -         -1.3704e+00,  1.2566e+00,  1.0203e-01, -3.2334e-01, -2.6631e+00,
      -         -1.1810e+00, -3.7269e-01, -4.3711e-01,  1.4026e+00,  1.1268e+00,
      -         -1.7976e+00, -1.4919e+00,  6.0385e-01,  2.6113e-01, -1.3251e+00,
      -          1.3404e+00, -1.3001e+00,  8.0537e-02, -2.8631e-01, -1.7914e+00,
      -         -5.3078e-01,  6.7947e-01,  7.4981e-01,  8.8413e-01, -6.4224e-01,
      -         -1.4126e+00, -1.1577e-01,  1.1998e+00,  7.2862e-01,  1.7041e-02,
      -          2.8471e-01,  8.4786e-01,  6.8959e-01,  1.4119e+00, -8.6827e-01,
      -         -1.6089e-01,  2.5429e+00,  5.5062e-01, -1.6139e-01, -2.5417e-02,
      -         -1.0755e-01, -1.5396e+00, -2.1070e-01,  4.2776e-01, -8.6397e-01,
      -         -2.4382e-01,  5.0070e-01,  2.6897e-01,  1.5593e+00,  6.5184e-01,
      -          2.1756e-01, -2.2441e-02, -1.7424e-01, -1.5440e+00,  2.3788e+00,
      -         -1.4316e+00,  5.1255e-01,  5.3844e-01,  5.8018e-01,  2.5060e-02,
      -          1.1007e+00, -1.9351e+00, -1.8010e+00,  7.8928e-01, -9.2259e-01,
      -          6.4621e-02,  9.3937e-02, -3.4790e-01, -7.4525e-01, -1.0826e-01,
      -         -1.4474e-01, -1.0763e+00,  4.6092e-02, -5.5614e-01,  2.9919e-01,
      -          1.4691e+00, -6.6381e-01, -1.5767e+00, -6.5960e-01,  1.2578e-01,
      -          3.4798e-01,  1.4833e+00,  9.9141e-01,  2.7336e-01, -8.5228e-01,
      -         -2.2314e-01,  2.6133e-01,  1.6839e-01, -1.7386e+00, -1.9338e-02,
      -         -4.8300e-01, -9.8531e-01,  1.3883e+00,  9.7957e-01,  2.0083e+00,
      -          2.3327e-02, -2.5442e-01, -1.2641e+00, -3.4508e-02, -1.9746e+00,
      -          1.4957e-01,  5.4871e-01, -4.2219e-01,  1.9523e+00, -8.6015e-02,
      -         -3.9797e-01,  8.4455e-01, -8.6461e-02,  3.9966e-01,  1.1195e+00,
      -          4.2529e-01, -8.4349e-01,  8.5807e-02, -3.6730e-01,  9.1504e-01,
      -          7.8051e-01,  8.9708e-01,  2.4914e-01,  2.5350e+00,  3.9394e-01,
      -         -8.0195e-01, -3.7096e-03, -1.0031e+00,  5.9966e-01, -2.0533e-01,
      -          3.8755e-01, -1.3943e+00, -4.3212e-01, -1.2661e-01, -7.5577e-01,
      -          5.7222e-01,  5.9325e-01, -3.1851e-01, -1.3438e+00,  3.0230e-01,
      -         -6.0730e-01,  7.6001e-01, -6.5611e-01, -1.5885e+00, -1.5775e+00,
      -         -1.2699e+00, -1.0967e+00, -4.4865e-01,  3.1030e-01, -1.5095e+00,
      -          1.4972e-01, -4.8658e-01, -3.6570e-01, -2.3305e-02, -1.0732e+00,
      -          1.2988e+00, -6.0982e-01,  8.2455e-01,  8.1180e-01,  7.4935e-01,
      -         -1.9441e-01,  2.1475e-03, -8.0770e-01, -6.0658e-01, -1.2065e-01,
      -         -1.2777e+00,  2.3932e-01, -4.2566e-01,  1.1642e+00,  2.1596e+00,
      -         -6.2873e-01,  7.8855e-01,  9.1142e-01, -1.6857e+00, -7.3007e-01,
      -          2.3588e-01, -9.4746e-02,  2.1106e+00, -6.3258e-01, -2.0280e+00,
      -          1.0646e+00,  4.2602e-01,  4.3597e-01, -2.7987e-01,  1.2714e+00,
      -         -6.1984e-01,  2.4606e+00,  1.7196e+00, -6.7126e-01, -9.9209e-01,
      -          1.0909e+00,  1.6826e+00, -2.8668e-02,  9.7822e-01,  1.9729e-01,
      -         -6.7843e-01, -2.4376e-01, -1.2072e+00, -5.0489e-01, -5.0108e-01,
      -         -1.0216e-01, -1.1692e-01,  7.4856e-01,  2.4658e-01,  6.2535e-01,
      -          2.1638e-02, -3.1520e-01,  4.4839e-01, -2.1626e-01, -9.3775e-02,
      -          7.0002e-01, -2.5019e+00, -7.7786e-01, -3.9641e-02, -5.4744e-01,
      -          3.7562e-01, -7.5094e-01, -1.3001e+00,  2.3238e-02,  1.2704e+00,
      -          1.3815e+00,  6.0357e-03, -1.6531e+00, -6.8171e-01,  1.1415e+00,
      -          2.3220e-01,  5.9601e-01,  1.9388e-01,  7.4870e-02, -6.4632e-01,
      -         -1.4398e+00,  2.7655e-01, -2.7187e-01, -1.2168e+00,  5.0690e-01,
      -          4.4268e-01,  1.7414e-01, -4.1817e-01, -4.3985e-01, -5.0706e-01,
      -         -7.2339e-01, -7.0502e-01, -5.7868e-01,  7.6055e-01, -4.5735e-01,
      -          9.4839e-01, -1.1592e+00,  1.5828e+00,  2.3345e-02,  2.2787e+00,
      -          3.6168e-01,  2.5863e-01,  1.0019e-01,  5.1395e-01, -3.8158e-01,
      -         -1.2847e+00,  5.5433e-01, -1.1028e+00,  1.1044e+00, -4.2726e-01,
      -          2.5015e-01, -8.1460e-01,  1.2423e+00, -5.3711e-01,  4.4678e-01,
      -         -6.4427e-01, -3.4977e-01, -1.3482e+00,  1.0104e+00, -1.9768e-01,
      -          1.6396e-01,  3.9383e-01, -8.7763e-01, -5.2580e-01, -3.3469e-01,
      -          9.5396e-02, -1.0723e+00, -5.8898e-01,  3.3156e-01, -1.4083e-01,
      -          8.4656e-02,  7.8325e-02,  9.4254e-01, -6.6480e-01,  1.6898e+00,
      -          6.2235e-01, -1.5892e+00,  4.2527e-01,  6.4694e-01,  1.4192e+00,
      -          8.3867e-01, -1.5808e+00, -1.8029e+00, -1.1835e+00,  1.8374e-01,
      -         -6.9245e-01,  2.3551e+00, -1.1600e+00, -3.2604e-01,  1.4746e+00,
      -         -2.5410e-01, -5.1651e-01,  1.4917e+00, -3.3403e-02,  7.9683e-01,
      -          6.8811e-01, -7.1452e-01,  1.8577e+00, -1.1204e+00,  1.9924e-01,
      -         -9.2570e-02, -3.0331e-01, -1.6621e+00, -1.2753e+00,  5.2796e-01,
      -         -8.1753e-01, -6.4944e-01,  6.5299e-01, -1.9244e+00,  8.4906e-01,
      -         -1.7205e-01,  2.1614e+00], device='cuda:0',
        ).all
================== 1 failed, 1 passed, 10 deselected in 9.54s ==================

Generated call accuracy: True
Execution accuracy: False
Match percentage: 0.00%
Error: Value mismatch at test_reverse_range: Tensor-likes are not close!

Mismatched elements: 507 / 512 (99.0%)
Greatest absolute difference: 5.198028564453125 at index (199,) (up to 0.01 allowed)
Greatest relative difference: 405.75860595703125 at index (161,) (up to 0.01 allowed)
2025-08-08_10-38-18 => File: rmsnorm_fwd.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/rmsnorm_fwd_py_gen_triton_code_466373_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/rmsnorm_fwd_py_gen_triton_code_466373.py _
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/python.py:617: in _importtestmodule
    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/pathlib.py:564: in import_path
    importlib.import_module(module_name)
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:169: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:351: in _rewrite_test
    tree = ast.parse(source, filename=strfn)
/opt/conda/envs/py_3.12/lib/python3.12/ast.py:52: in parse
    return compile(source, filename, mode, flags,
E     File "/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/rmsnorm_fwd_py_gen_triton_code_466373.py", line 48
E       import argparse
E       ^^^^^^
E   IndentationError: expected an indented block after function definition on line 40
2025-08-08_10-50-33 => File: rmsnorm_bwd.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/rmsnorm_bwd_py_gen_triton_code_258856_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/rmsnorm_bwd_py_gen_triton_code_258856.py _
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/python.py:617: in _importtestmodule
    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/pathlib.py:564: in import_path
    importlib.import_module(module_name)
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:169: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:351: in _rewrite_test
    tree = ast.parse(source, filename=strfn)
/opt/conda/envs/py_3.12/lib/python3.12/ast.py:52: in parse
    return compile(source, filename, mode, flags,
E     File "/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/rmsnorm_bwd_py_gen_triton_code_258856.py", line 19
E       triton.Config(
E       ^
E   SyntaxError: did you forget parentheses around the comprehension target?
2025-08-08_10-51-09 => File: test_block_pointer_matmul.py, Call Status: True, Exec Status: True, difficulty: -1, stderr: None
2025-08-08_10-51-47 => File: test_block_copy.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_block_copy_dtypes_str71_512_nan', 'test_block_copy_dtypes_str12_1024_None', 'test_block_copy_dtypes_str63_128_None', 'test_block_copy_dtypes_str4_128_zero', 'test_block_copy_dtypes_str45_64_None', 'test_block_copy_dtypes_str49_128_zero', 'test_block_copy_dtypes_str80_128_nan', 'test_block_copy_dtypes_str39_512_None', 'test_block_copy_dtypes_str72_1024_None', 'test_block_copy_dtypes_str66_256_None', 'test_block_copy_dtypes_str87_1024_None', 'test_block_copy_dtypes_str77_64_nan', 'test_block_copy_dtypes_str74_1024_nan', 'test_block_copy_dtypes_str15_64_None', 'test_block_copy_dtypes_str54_512_None', 'test_block_copy_dtypes_str78_128_None', 'test_block_copy_dtypes_str25_512_zero', 'test_block_copy_dtypes_str40_512_zero', 'test_block_copy_dtypes_str75_64_None', 'test_block_copy_dtypes_str33_128_None', 'test_block_copy_dtypes_str21_256_None', 'test_block_copy_dtypes_str69_512_None', 'test_block_copy_dtypes_str55_512_zero', 'test_block_copy_dtypes_str30_64_None', 'test_block_copy_dtypes_str76_64_zero', 'test_block_copy_dtypes_str48_128_None', 'test_block_copy_dtypes_str22_256_zero', 'test_block_copy_dtypes_str65_128_nan', 'test_block_copy_dtypes_str16_64_zero', 'test_block_copy_dtypes_str36_256_None', 'test_block_copy_dtypes_str1_64_zero', 'test_block_copy_dtypes_str47_64_nan', 'test_block_copy_dtypes_str9_512_None', 'test_block_copy_dtypes_str24_512_None', 'test_block_copy_dtypes_str50_128_nan', 'test_block_copy_dtypes_str3_128_None', 'test_block_copy_dtypes_str7_256_zero', 'test_block_copy_dtypes_str67_256_zero', 'test_block_copy_dtypes_str88_1024_zero', 'test_block_copy_dtypes_str83_256_nan', 'test_block_copy_dtypes_str37_256_zero', 'test_block_copy_dtypes_str81_256_None', 'test_block_copy_dtypes_str52_256_zero', 'test_block_copy_dtypes_str56_512_nan', 'test_block_copy_dtypes_str70_512_zero', 'test_block_copy_dtypes_str31_64_zero', 'test_block_copy_dtypes_str13_1024_zero', 'test_block_copy_dtypes_str10_512_zero', 'test_block_copy_dtypes_str58_1024_zero', 'test_block_copy_dtypes_str0_64_None', 'test_block_copy_dtypes_str6_256_None', 'test_block_copy_dtypes_str18_128_None', 'test_block_copy_dtypes_str89_1024_nan', 'test_block_copy_dtypes_str53_256_nan', 'test_block_copy_dtypes_str73_1024_zero', 'test_block_copy_dtypes_str27_1024_None', 'test_block_copy_dtypes_str51_256_None', 'test_block_copy_dtypes_str85_512_zero', 'test_block_copy_dtypes_str43_1024_zero', 'test_block_copy_dtypes_str82_256_zero', 'test_block_copy_dtypes_str64_128_zero', 'test_block_copy_dtypes_str79_128_zero', 'test_block_copy_dtypes_str61_64_zero', 'test_block_copy_dtypes_str42_1024_None', 'test_block_copy_dtypes_str86_512_nan', 'test_block_copy_dtypes_str46_64_zero', 'test_block_copy_dtypes_str68_256_nan', 'test_block_copy_dtypes_str59_1024_nan', 'test_block_copy_dtypes_str84_512_None', 'test_block_copy_dtypes_str57_1024_None', 'test_block_copy_dtypes_str34_128_zero', 'test_block_copy_dtypes_str62_64_nan', 'test_block_copy_dtypes_str28_1024_zero', 'test_block_copy_dtypes_str60_64_None', 'test_block_copy_dtypes_str19_128_zero'}_____________________ test_block_copy[dtypes_str0-64-None] _____________________

args = ()
kwargs = {'_builder': <triton._C.libtriton.ir.builder object at 0x7277d0311bb0>, 'base': <triton.language.core.tensor object at...sor object at 0x7277bff0a2a0>), 'offsets': (<triton.language.core.tensor object at 0x7277bff0a5d0>, constexpr[0]), ...}

    @wraps(fn)
    def wrapper(*args, **kwargs):
        if "_builder" not in kwargs or kwargs["_builder"] is None:
            print(kwargs)
            raise ValueError("Did you forget to add @triton.jit ? "
                             "(`_builder` argument must be provided outside of JIT functions.)")
>       return fn(*args, **kwargs)
E       TypeError: make_block_ptr() missing 1 required positional argument: 'order'

/var/lib/jenkins/triton/python/triton/language/core.py:35: TypeError

The above exception was the direct cause of the following exception:

dtypes_str = ('bool', 'bool'), n = 64, padding_option = None
request = <FixtureRequest for <Function test_block_copy[dtypes_str0-64-None]>>
device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("dtypes_str, n, padding_option", [  #
        (dtypes_str, n, padding)
        for dtypes_str in (("bool", "bool"), ("int16", "int16"), ("int32", "int32"), ("float16", "float16"),
                           ("float32", "float32"), ("bfloat16", "bfloat16"))
        for n in (64, 128, 256, 512, 1024)
        for padding in (None, "zero", "nan")  #
    ])
    def test_block_copy(dtypes_str, n, padding_option, request, device='cuda'):
        src_dtype_str = dtypes_str[0]
        dst_dtype_str = dtypes_str[1]
        src_dtype = getattr(torch, src_dtype_str)
        dst_dtype = getattr(torch, dst_dtype_str)
        check_type_supported(src_dtype, device)
        check_type_supported(dst_dtype, device)
        if src_dtype_str in ("bool", "int16", "int32"):
            if padding_option == "nan":
                pytest.skip("Padding with NaN is not supported for integer types")
            a = torch.randint(0, 2, (n, ), device=device, dtype=src_dtype)
        else:
            a = torch.randn((n, ), device=device, dtype=src_dtype)
        b = torch.zeros((n, ), device=device, dtype=dst_dtype)
    
        grid = lambda meta: (triton.cdiv(n, meta["BLOCK_SIZE"]), )
>       block_copy_kernel[grid](a_ptr=a, b_ptr=b, N=n, BLOCK_SIZE=64, padding_option=padding_option)

sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_block_copy_py_gen_triton_code_280998.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/var/lib/jenkins/triton/python/triton/runtime/jit.py:330: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
/var/lib/jenkins/triton/python/triton/runtime/jit.py:623: in run
    kernel = self.compile(
/var/lib/jenkins/triton/python/triton/compiler/compiler.py:273: in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <triton.compiler.compiler.ASTSource object at 0x7277bff09b80>
options = HIPOptions(num_warps=4, waves_per_eu=1, num_stages=2, num_ctas=1, num_buffers_warp_spec=0, num_consumer_groups=0, reg_...=1, allow_flush_denorm=False, max_num_imprecise_acc_default=0, backend_name='hip', instruction_sched_variant='default')
codegen_fns = {'min_dot_size': <function min_dot_size.<locals>.<lambda> at 0x7277bff249a0>}
module_map = {'triton.language.extra.libdevice': <module 'triton.language.extra.hip.libdevice' from '/var/lib/jenkins/triton/python/triton/language/extra/hip/libdevice.py'>}
context = <triton._C.libtriton.ir.context object at 0x7277d0321db0>

    def make_ir(self, options, codegen_fns, module_map, context):
>       return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
                           module_map=module_map)
E       triton.compiler.errors.CompilationError: at 3:18:
E       def block_copy_kernel(a_ptr, b_ptr, N, BLOCK_SIZE: tl.constexpr, padding_option: tl.constexpr):
E           pid = tl.program_id(0)
E           a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(N // 2, N), strides=(N, 1), offsets=(pid * BLOCK_SIZE, 0), block_shape=(BLOCK_SIZE, N))
E                         ^

/var/lib/jenkins/triton/python/triton/compiler/compiler.py:100: CompilationError
2025-08-08_10-52-10 => File: test_tma_store_gemm.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_tma_store_gemm_py_gen_triton_code_772068_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_tma_store_gemm_py_gen_triton_code_772068.py _
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/python.py:617: in _importtestmodule
    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/pathlib.py:564: in import_path
    importlib.import_module(module_name)
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:169: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:351: in _rewrite_test
    tree = ast.parse(source, filename=strfn)
/opt/conda/envs/py_3.12/lib/python3.12/ast.py:52: in parse
    return compile(source, filename, mode, flags,
E     File "/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_tma_store_gemm_py_gen_triton_code_772068.py", line 292
E       * (elapsed_
E         ^
E   SyntaxError: '(' was never closed
2025-08-08_10-52-40 => File: test_kernel_dot.py, Call Status: True, Exec Status: True, difficulty: -1, stderr: None
2025-08-08_10-53-09 => File: test_kernel_sub.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_kernel_sub_py_gen_triton_code_536776_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_kernel_sub_py_gen_triton_code_536776.py _
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_kernel_sub_py_gen_triton_code_536776.py:21: in <module>
    def fused_gemm_heuristics_kernel(output_dtype, compute_capability=cc_major * 10 + cc_minor):
E   NameError: name 'cc_major' is not defined
2025-08-08_10-53-31 => File: test_triton_flip.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_triton_flip_py_gen_triton_code_590696_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_triton_flip_py_gen_triton_code_590696.py _
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/python.py:617: in _importtestmodule
    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/pathlib.py:564: in import_path
    importlib.import_module(module_name)
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:169: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:351: in _rewrite_test
    tree = ast.parse(source, filename=strfn)
/opt/conda/envs/py_3.12/lib/python3.12/ast.py:52: in parse
    return compile(source, filename, mode, flags,
E     File "/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_triton_flip_py_gen_triton_code_590696.py", line 135
E       1,
E       ^
E   SyntaxError: invalid syntax
2025-08-08_10-54-15 => File: test_triton_sort.py, Call Status: True, Exec Status: False, difficulty: -1, stderr: Value mismatch at test_sort_bfloat16_True_8_512: Tensor-likes are not close!

Mismatched elements: 4096 / 4096 (100.0%)
Greatest absolute difference: nan at index (0, 2) (up to 0.01 allowed)
Greatest relative difference: nan at index (0, 2) (up to 0.01 allowed)_________________________ test_sort[int32-False-512-1] _________________________

N_rows = 1, M_cols = 512, descending = False, dtype_str = 'int32'
request = <FixtureRequest for <Function test_sort[int32-False-512-1]>>
device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("M_cols, N_rows", [[512, 1], [64, 8], [16, 256], [8, 512]])
    @pytest.mark.parametrize("descending", [False, True])
    @pytest.mark.parametrize("dtype_str", ['int32', 'float16', 'float32', 'bfloat16'])
    def test_sort(N_rows, M_cols, descending, dtype_str, request, device='cuda'):
        set_seed()
    
        x_np = gen_numpy_array_for_torch_conversion((N_rows, M_cols), dtype_str=dtype_str)
    
        torch_dtype = torch_dtype_from_str(dtype_str)
        x = torch.from_numpy(x_np).to(dtype=torch_dtype, device=device) # Ensure correct torch dtype
    
        y_ref = torch.sort(x, dim=1, descending=descending)[0]
        z_triton = torch.empty_like(x)
    
        # Grid is (N_rows,) since each program sorts one row
        grid = (N_rows,)
        sort_kernel[grid](x, z_triton, N_rows, M_cols, descending) # num_warps removed, let triton decide or set default
    
        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])
    
        ################### save tri_out in result_gold ###################
        test_case_name = request.node.name
        sanitized_key_name = test_case_name.replace("::", "_").replace("[", "_").replace("]", "").replace("-", "_")
        result_gold[sanitized_key_name] = z_triton.clone().detach().cpu()
        ###################################################################
    
>       assert torch.allclose(y_ref.float(), z_triton.float(), atol=1e-2, rtol=1e-2), \
            f"Sort mismatch for dtype {dtype_str}, shape ({N_rows}, {M_cols}), descending={descending}.\nReference: {y_ref}\nTriton: {z_triton}"
E       AssertionError: Sort mismatch for dtype int32, shape (1, 512), descending=False.
E         Reference: tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   1,   1,
E                    1,   1,   1,   1,   1,   2,   2,   2,   2,   2,   2,   2,   3,   3,
E                    3,   3,   3,   4,   4,   4,   5,   5,   5,   5,   6,   6,   6,   6,
E                    6,   7,   7,   7,   7,   7,   7,   7,   8,   8,   8,   8,   8,   8,
E                    8,  10,  10,  10,  11,  11,  11,  11,  12,  12,  12,  13,  13,  13,
E                   13,  13,  14,  14,  14,  14,  14,  14,  14,  14,  15,  15,  15,  15,
E                   15,  15,  16,  16,  16,  16,  16,  17,  17,  17,  17,  18,  18,  18,
E                   18,  18,  18,  18,  19,  19,  19,  19,  19,  20,  20,  20,  20,  20,
E                   21,  21,  21,  21,  21,  22,  22,  22,  22,  22,  23,  23,  23,  23,
E                   23,  23,  23,  23,  23,  24,  24,  24,  24,  25,  25,  25,  26,  26,
E                   26,  26,  26,  26,  27,  27,  27,  27,  27,  28,  28,  29,  29,  29,
E                   29,  29,  29,  30,  31,  31,  31,  31,  31,  31,  31,  31,  32,  32,
E                   32,  32,  32,  32,  32,  32,  32,  32,  33,  33,  33,  34,  34,  34,
E                   34,  34,  34,  35,  35,  35,  35,  35,  36,  36,  36,  36,  37,  37,
E                   37,  37,  38,  38,  38,  38,  39,  39,  39,  40,  40,  40,  41,  41,
E                   41,  41,  41,  41,  42,  42,  43,  43,  43,  43,  43,  43,  43,  44,
E                   44,  44,  45,  45,  45,  46,  46,  46,  46,  46,  47,  47,  47,  47,
E                   47,  47,  47,  47,  47,  48,  48,  48,  49,  49,  49,  50,  50,  50,
E                   50,  50,  50,  50,  51,  51,  51,  51,  51,  51,  51,  52,  52,  52,
E                   52,  52,  52,  53,  53,  53,  53,  53,  53,  53,  53,  54,  54,  54,
E                   54,  55,  55,  56,  56,  56,  57,  57,  57,  57,  57,  57,  57,  57,
E                   58,  58,  58,  58,  58,  58,  58,  59,  59,  59,  59,  59,  59,  59,
E                   59,  59,  59,  60,  60,  60,  60,  61,  61,  61,  61,  61,  61,  61,
E                   61,  61,  61,  61,  61,  61,  61,  62,  62,  62,  62,  62,  62,  62,
E                   62,  63,  63,  63,  63,  63,  64,  64,  64,  65,  65,  65,  65,  66,
E                   66,  66,  66,  66,  67,  67,  67,  68,  68,  68,  68,  69,  69,  69,
E                   69,  69,  70,  70,  70,  70,  71,  71,  71,  71,  71,  72,  72,  72,
E                   73,  74,  74,  74,  74,  74,  75,  75,  75,  75,  76,  76,  77,  77,
E                   77,  78,  78,  79,  79,  79,  79,  79,  80,  80,  80,  81,  81,  81,
E                   82,  83,  83,  83,  84,  84,  85,  85,  85,  85,  85,  85,  85,  86,
E                   86,  86,  86,  86,  86,  87,  87,  87,  87,  87,  87,  88,  88,  88,
E                   88,  88,  88,  88,  88,  89,  89,  89,  89,  89,  89,  89,  89,  89,
E                   89,  90,  90,  91,  91,  91,  91,  91,  91,  91,  91,  92,  92,  92,
E                   92,  92,  92,  92,  92,  93,  93,  93,  94,  94,  94,  95,  95,  95,
E                   95,  95,  95,  95,  96,  96,  96,  96,  96,  96,  96,  96,  97,  97,
E                   97,  98,  98,  98,  98,  98,  98,  98,  98,  98,  98,  99,  99,  99,
E                  100, 100, 100, 100, 100, 100, 100, 100]], device='cuda:0',
E                dtype=torch.int32)
E         Triton: tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
E                    0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
E                    0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
E                    0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
E                    0,   0,   0,   0,   0,   0,   0,   0, 149,   0, 464,   0,  74,   0,
E                  183,   0, 376,   0, 466,   0,  47,   0, 126,   0, 290,   0, 339,   0,
E                  424,   0,  65,   0, 114,   0, 119,   0, 338,   0, 352,   0, 430,   0,
E                  461,   0,  56,   0, 111,   0, 153,   0, 203,   0, 301,   0, 411,   0,
E                  489,   0, 117,   0, 255,   0, 280,   0, 129,   0, 219,   0, 297,   0,
E                  490,   0, 172,   0, 422,   0, 475,   0,  55,   0,  87,   0, 151,   0,
E                  334,   0, 432,   0,   2,   0,  36,   0,  90,   0, 106,   0, 155,   0,
E                  168,   0, 435,   0, 456,   0, 256,   0, 291,   0, 431,   0, 474,   0,
E                  499,   0, 509,   0, 248,   0, 341,   0, 370,   0, 409,   0, 478,   0,
E                   51,   0,  80,   0, 322,   0, 393,   0, 236,   0, 267,   0, 270,   0,
E                  303,   0, 365,   0, 477,   0, 480,   0, 264,   0, 271,   0, 307,   0,
E                  337,   0, 428,   0,   5,   0,  23,   0,  48,   0, 335,   0, 356,   0,
E                   14,   0,  27,   0, 347,   0, 448,   0, 504,   0, 133,   0, 167,   0,
E                  285,   0, 404,   0, 488,   0,  12,   0, 100,   0, 209,   0, 250,   0,
E                  308,   0, 311,   0, 379,   0, 400,   0, 421,   0, 320,   0, 394,   0,
E                  420,   0, 444,   0, 101,   0, 417,   0, 508,   0, 152,   0, 196,   0,
E                  202,   0, 353,   0, 354,   0, 406,   0, 125,   0, 177,   0, 184,   0,
E                  185,   0, 359,   0, 105,   0, 170,   0,  18,   0, 188,   0, 348,   0,
E                  357,   0, 373,   0, 476,   0, 286,   0, 173,   0, 212,   0, 241,   0,
E                  243,   0, 300,   0, 389,   0, 401,   0, 439,   0,  24,   0, 122,   0,
E                  131,   0, 275,   0, 310,   0, 331,   0, 333,   0, 342,   0, 391,   0,
E                  410,   0,  83,   0, 130,   0, 494,   0,  67,   0, 120,   0, 121,   0,
E                  141,   0, 326,   0, 367,   0,  70,   0, 171,   0, 266,   0, 314,   0,
E                  426,   0, 136,   0, 205,   0, 299,   0, 378,   0,  19,   0, 249,   0,
E                  315,   0, 349,   0,  50,   0, 220,   0, 277,   0, 414,   0,  95,   0,
E                  276,   0, 472,   0, 104,   0, 124,   0, 329,   0,  32,   0, 157,   0,
E                  179,   0, 288,   0, 416,   0, 454,   0, 169,   0, 412,   0,  64,   0,
E                   82,   0, 138,   0, 186,   0, 208,   0, 239,   0, 371,   0, 107,   0,
E                  180,   0, 427,   0, 375,   0, 381,   0, 399,   0,  39,   0,  66,   0,
E                  145,   0, 402,   0, 469,   0,  89,   0, 132,   0, 296,   0, 304,   0,
E                  336,   0, 343,   0, 364,   0, 413,   0, 496,   0,  29,   0, 369,   0,
E                  452,   0,  71,   0, 284,   0, 419,   0,  41,   0,  46,   0, 159,   0,
E                  207,   0, 350,   0, 443,   0, 510,   0]], device='cuda:0',
E                dtype=torch.int32)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x77c775e83640>(tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   1.,   1.,\n           1.,   1.,   1.,   1.,   1....,  98.,  98.,  98.,  98.,  99.,  99.,  99.,\n         100., 100., 100., 100., 100., 100., 100., 100.]], device='cuda:0'), tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0....,   0.,  41.,   0.,  46.,   0., 159.,   0.,\n         207.,   0., 350.,   0., 443.,   0., 510.,   0.]], device='cuda:0'), atol=0.01, rtol=0.01)
E        +    where <built-in method allclose of type object at 0x77c775e83640> = torch.allclose
E        +    and   tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   1.,   1.,\n           1.,   1.,   1.,   1.,   1....,  98.,  98.,  98.,  98.,  99.,  99.,  99.,\n         100., 100., 100., 100., 100., 100., 100., 100.]], device='cuda:0') = <built-in method float of Tensor object at 0x77c61089d9a0>()
E        +      where <built-in method float of Tensor object at 0x77c61089d9a0> = tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   1,   1,\n           1,   1,   1,   1,   1,   2, ...  98,  98,  99,  99,  99,\n         100, 100, 100, 100, 100, 100, 100, 100]], device='cuda:0',\n       dtype=torch.int32).float
E        +    and   tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n           0.,   0.,   0.,   0.,   0....,   0.,  41.,   0.,  46.,   0., 159.,   0.,\n         207.,   0., 350.,   0., 443.,   0., 510.,   0.]], device='cuda:0') = <built-in method float of Tensor object at 0x77c784ca7200>()
E        +      where <built-in method float of Tensor object at 0x77c784ca7200> = tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0, ...   0,  46,   0, 159,   0,\n         207,   0, 350,   0, 443,   0, 510,   0]], device='cuda:0',\n       dtype=torch.int32).float

sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_triton_sort_py_gen_triton_code_66460.py:166: AssertionError
2025-08-08_10-54-46 => File: test_triton_swizzle2d.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_swizzle2d_5_7_3'}____________________________ test_swizzle2d[5-7-3] _____________________________

args = (constexpr[0], <triton.language.core.tensor object at 0x71f3a9fe6630>)
kwargs = {'_builder': <triton._C.libtriton.ir.builder object at 0x71f3aa87fbf0>}

    @wraps(fn)
    def wrapper(*args, **kwargs):
        if "_builder" not in kwargs or kwargs["_builder"] is None:
            print(kwargs)
            raise ValueError("Did you forget to add @triton.jit ? "
                             "(`_builder` argument must be provided outside of JIT functions.)")
>       return fn(*args, **kwargs)

/var/lib/jenkins/triton/python/triton/language/core.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 0, end = <triton.language.core.tensor object at 0x71f3a9fe6630>
_builder = <triton._C.libtriton.ir.builder object at 0x71f3aa87fbf0>

    @builtin
    def arange(start, end, _builder=None):
        start = _constexpr_to_value(start)
        end = _constexpr_to_value(end)
>       return semantic.arange(start, end, _builder)

/var/lib/jenkins/triton/python/triton/language/core.py:1187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 0, end = <triton.language.core.tensor object at 0x71f3a9fe6630>
builder = <triton._C.libtriton.ir.builder object at 0x71f3aa87fbf0>

    def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:
        if not isinstance(start, int) or not isinstance(end, int):
>           raise ValueError("arange's arguments must be of type tl.constexpr")
E           ValueError: arange's arguments must be of type tl.constexpr

/var/lib/jenkins/triton/python/triton/language/semantic.py:604: ValueError

The above exception was the direct cause of the following exception:

size_i = 5, size_j = 7, size_g = 3
request = <FixtureRequest for <Function test_swizzle2d[5-7-3]>>, device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("size_i, size_j, size_g", [[5, 7, 3]])
    def test_swizzle2d(size_i, size_j, size_g, request, device='cuda'):
        # Output tensor to store results, initialized to a value like -1 to see what's written
    
        set_seed()
    
        output = torch.zeros(size_i, size_j).to(device)
>       swizzle2d_kernel[(1, )](output, size_i, size_j, size_g)

sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_865848.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/var/lib/jenkins/triton/python/triton/runtime/jit.py:330: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
/var/lib/jenkins/triton/python/triton/runtime/jit.py:623: in run
    kernel = self.compile(
/var/lib/jenkins/triton/python/triton/compiler/compiler.py:273: in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <triton.compiler.compiler.ASTSource object at 0x71f3a9fe62d0>
options = HIPOptions(num_warps=4, waves_per_eu=1, num_stages=2, num_ctas=1, num_buffers_warp_spec=0, num_consumer_groups=0, reg_...=1, allow_flush_denorm=False, max_num_imprecise_acc_default=0, backend_name='hip', instruction_sched_variant='default')
codegen_fns = {'min_dot_size': <function min_dot_size.<locals>.<lambda> at 0x71f3a9ff4ea0>}
module_map = {'triton.language.extra.libdevice': <module 'triton.language.extra.hip.libdevice' from '/var/lib/jenkins/triton/python/triton/language/extra/hip/libdevice.py'>}
context = <triton._C.libtriton.ir.context object at 0x71f3aa263330>

    def make_ir(self, options, codegen_fns, module_map, context):
>       return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
                           module_map=module_map)
E       triton.compiler.errors.CompilationError: at 2:16:
E       def swizzle2d_kernel(output, size_i, size_j, size_g):
E           offsets_i = tl.arange(0, size_i)
E                       ^

/var/lib/jenkins/triton/python/triton/compiler/compiler.py:100: CompilationError
=============================== warnings summary ===============================
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_865848.py:59
  /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_865848.py:59: PytestUnknownMarkWarning: Unknown pytest.mark.interpreter - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.interpreter

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_865848.py::test_swizzle2d[5-7-3] - triton.compiler.errors.CompilationError: at 2:16:
def swizzle2d_kernel(output, size_i, size_j, size_g):
    offsets_i = tl.arange(0, size_i)
                ^
============ 1 failed, 1 passed, 17 deselected, 1 warning in 8.26s =============

Generated call accuracy: False
Execution accuracy: False
Match percentage: 0.00%
Error: Key mismatch at : Missing keys: {'test_swizzle2d_5_7_3'} 
2025-08-08_10-55-10 => File: test_random_int.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_random_int_py_gen_triton_code_968418_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_random_int_py_gen_triton_code_968418.py _
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/python.py:617: in _importtestmodule
    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/pathlib.py:564: in import_path
    importlib.import_module(module_name)
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:169: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:351: in _rewrite_test
    tree = ast.parse(source, filename=strfn)
/opt/conda/envs/py_3.12/lib/python3.12/ast.py:52: in parse
    return compile(source, filename, mode, flags,
E     File "/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_random_int_py_gen_triton_code_968418.py", line 7
E       def custom_layer_pre_hook(kernel_state):
E       ^^^
E   IndentationError: expected an indented block after 'if' statement on line 5
2025-08-08_10-55-54 => File: test_randn.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_randn_py_gen_triton_code_369646_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_randn_py_gen_triton_code_369646.py _
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_randn_py_gen_triton_code_369646.py:79: in <module>
    def randn_kernel_runtime_seed(X: tl.pointer_type, N: tl.int32, seed: tl.int66, dtype: tl.constexpr):
E   AttributeError: module 'triton.language' has no attribute 'int66'. Did you mean: 'int16'?
2025-08-08_10-56-27 => File: test_matmul_MXFP.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_pipeline_matmul_False'}_________________________ test_pipeline_matmul[False] __________________________

args = (<triton.language.core.tensor object at 0x7968b0be3b30>,)
kwargs = {'_builder': <triton._C.libtriton.ir.builder object at 0x7968b16b0830>, 'mask': <triton.language.core.tensor object at 0x7968b0a380b0>, 'other': constexpr[0]}

    @wraps(fn)
    def wrapper(*args, **kwargs):
        if "_builder" not in kwargs or kwargs["_builder"] is None:
            print(kwargs)
            raise ValueError("Did you forget to add @triton.jit ? "
                             "(`_builder` argument must be provided outside of JIT functions.)")
>       return fn(*args, **kwargs)

/var/lib/jenkins/triton/python/triton/language/core.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pointer = <triton.language.core.tensor object at 0x7968b0be3b30>
mask = <triton.language.core.tensor object at 0x7968b0a380b0>
other = <triton.language.core.tensor object at 0x7968b0be38c0>
boundary_check = (), padding_option = '', cache_modifier = ''
eviction_policy = '', volatile = False
_builder = <triton._C.libtriton.ir.builder object at 0x7968b16b0830>

    @builtin
    def load(pointer, mask=None, other=None, boundary_check=(), padding_option="", cache_modifier="", eviction_policy="",
             volatile=False, _builder=None):
        """
        Return a tensor of data whose values are loaded from memory at location defined by `pointer`:
    
            (1) If `pointer` is a single element pointer, a scalar is be loaded.  In
                this case:
    
                - `mask` and `other` must also be scalars,
                - `other` is implicitly typecast to `pointer.dtype.element_ty`, and
                - `boundary_check` and `padding_option` must be empty.
    
            (2) If `pointer` is an N-dimensional tensor of pointers, an
                N-dimensional tensor is loaded.  In this case:
    
                - `mask` and `other` are implicitly broadcast to `pointer.shape`,
                - `other` is implicitly typecast to `pointer.dtype.element_ty`, and
                - `boundary_check` and `padding_option` must be empty.
    
            (3) If `pointer` is a block pointer defined by `make_block_ptr`, a
                tensor is loaded.  In this case:
    
                - `mask` and `other` must be `None`, and
                - `boundary_check` and `padding_option` can be specified to control the behavior of out-of-bound access.
    
        :param pointer: Pointer to the data to be loaded
        :type pointer: `triton.PointerType`, or block of `dtype=triton.PointerType`
        :param mask: if `mask[idx]` is false, do not load the data at address `pointer[idx]`
            (must be `None` with block pointers)
        :type mask: Block of `triton.int1`, optional
        :param other: if `mask[idx]` is false, return `other[idx]`
        :type other: Block, optional
        :param boundary_check: tuple of integers, indicating the dimensions which should do the boundary check
        :type boundary_check: tuple of ints, optional
        :param padding_option: should be one of {"", "zero", "nan"}, the padding value to use while out of bounds. "" means an undefined value.
        :param cache_modifier: changes cache option in NVIDIA PTX
        :type cache_modifier: str, optional, should be one of {"", "ca", "cg"}, where "ca" stands for
            cache at all levels and "cg" stands for cache at global level (cache in L2 and below, not L1), see
            `cache operator <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#cache-operators>`_ for more details.
        :param eviction_policy: changes eviction policy in NVIDIA PTX
        :type eviction_policy: str, optional
        :param volatile: changes volatile option in NVIDIA PTX
        :type volatile: bool, optional
        """
        # `mask` and `other` can be constexpr
        mask = _constexpr_to_value(mask)
        other = _constexpr_to_value(other)
        if mask is not None:
            mask = semantic.to_tensor(mask, _builder)
        if other is not None:
            other = semantic.to_tensor(other, _builder)
        padding_option = _constexpr_to_value(padding_option)
        cache_modifier = _constexpr_to_value(cache_modifier)
        eviction_policy = _constexpr_to_value(eviction_policy)
        volatile = _constexpr_to_value(volatile)
>       return semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,
                             volatile, _builder)

/var/lib/jenkins/triton/python/triton/language/core.py:1635: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ptr = <triton.language.core.tensor object at 0x7968b0be3b30>
mask = <triton.language.core.tensor object at 0x7968b0a380b0>
other = <triton.language.core.tensor object at 0x7968b0be38c0>
boundary_check = (), padding_option = '', cache_modifier = ''
eviction_policy = '', is_volatile = False
builder = <triton._C.libtriton.ir.builder object at 0x7968b16b0830>

    def load(ptr: tl.tensor, mask: Optional[tl.tensor], other: Optional[tl.tensor], boundary_check: Tuple,
             padding_option: str, cache_modifier: str, eviction_policy: str, is_volatile: bool,
             builder: ir.builder) -> tl.tensor:
        # Cache, eviction and padding options
        cache = _str_to_load_cache_modifier(cache_modifier)
        eviction = _str_to_eviction_policy(eviction_policy)
        padding = _str_to_padding_option(padding_option)
    
        if ptr.type.is_ptr() and ptr.type.element_ty.is_block():
            # Load by a block pointer: `pointer_type<block_type<>>`
            return _load_block_pointer(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile, builder)
        else:
            # Load by a tensor of pointers or a pointer of scalar: `block_type<pointer_type<>>` or `pointer_type<>`
>           return _load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile, builder)

/var/lib/jenkins/triton/python/triton/language/semantic.py:1141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ptr = <triton.language.core.tensor object at 0x7968b0be3b30>
mask = <triton.language.core.tensor object at 0x7968b0a380b0>
other = <triton.language.core.tensor object at 0x7968b0be38c0>
boundary_check = (), padding = None, cache = <CACHE_MODIFIER.NONE: 1>
eviction = <EVICTION_POLICY.NORMAL: 1>, is_volatile = False
builder = <triton._C.libtriton.ir.builder object at 0x7968b16b0830>

    def _load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile, builder):
        # Load by a tensor of pointers or a pointer of scalar: `block_type<pointer_type<>>` or `pointer_type<>`
        if not ptr.type.scalar.is_ptr():
            raise ValueError(f"Unsupported ptr type {ptr.type.__repr__()} in `tl.load`")
    
        # Check `mask`, `other`, `boundary_check`, and `padding` arguments
        if mask is None and other is not None:
            raise ValueError("`other` cannot be provided without `mask`")
        if padding or boundary_check:
            raise ValueError("`padding_option` or `boundary_check` argument is not supported for loading a tensor of"
                             "pointers or loading a scalar. Because the compiler does not know the boundary; please "
                             "use block pointers (defined by `make_block_ptr`) instead")
    
        # For a pointer of scalar, check the type of `mask` and `other`
        if not ptr.type.is_block():
            if mask and mask.type.is_block():
                raise ValueError("Mask argument cannot be block type if pointer argument is not a block")
            if other and other.type.is_block():
                raise ValueError("Other argument cannot be block type if pointer argument is not a block")
    
        # Make `mask` and `other` into the same shape as `ptr`
        if ptr.type.is_block():
            if mask is not None:
>               mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)

/var/lib/jenkins/triton/python/triton/language/semantic.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <triton.language.core.tensor object at 0x7968b0a380b0>, shape = [32, 64]
builder = <triton._C.libtriton.ir.builder object at 0x7968b16b0830>

    def broadcast_impl_shape(input: tl.tensor, shape: List[int], builder: ir.builder) -> tl.tensor:
        if not input.type.is_block():
            ret_ty = tl.block_type(input.type, shape)
            return tl.tensor(builder.create_splat(input.handle, shape), ret_ty)
        src_shape = input.type.get_block_shapes()
        if len(src_shape) != len(shape):
>           raise ValueError(f"Cannot broadcast, rank mismatch: {src_shape}, {shape}")
E           ValueError: Cannot broadcast, rank mismatch: [1, 1, 32], [32, 64]

/var/lib/jenkins/triton/python/triton/language/semantic.py:732: ValueError

The above exception was the direct cause of the following exception:

scale = False
request = <FixtureRequest for <Function test_pipeline_matmul[False]>>
device = 'cuda'

    @pytest.mark.parametrize("scale", [True, False])
    def test_pipeline_matmul(scale, request, device='cuda'):
        check_capabilities()
        set_seed()
        if scale and not is_cuda():
            pytest.skip("NYI: scale_dot just implemented in CUDA")
        M, N, K = 512, 512, 128
        BLOCK_M, BLOCK_N, BLOCK_K = 64, 64, 32
        NUM_STAGES = 4
    
        if scale:
            # TODO Use e5m2 for Ampere, as it does not support fp_to_fp conversions for fp8e4m3
            BLOCK_K = 64  # 32 NYI
            K = BLOCK_K * NUM_STAGES
            a_type = "e2m1"
            DIV_FACTOR = 2 if a_type == "e2m1" else 1
            a = torch.randint(256, (M, K // DIV_FACTOR), device=device, dtype=torch.uint8)
            # Sample small-ish scales to avoid overflow
            scale_a = torch.randint(74, (M, K // 32), device=device, dtype=torch.uint8)
            # Ampere does not support fp8e4m3
            b_type = "e4m3" if is_hopper() else "e5m2"
            b = torch.randint(256, (K, N), device=device, dtype=torch.uint8)
            # e5m2 has too many non-finite values when sampled uniformly (1 / 32) and
            # Fp8E5M2_to_Bf16 doesn't preserve NaNs (fixme)
            if b_type == "e5m2":
                finite = torch.arange(K * N, device=device, dtype=torch.uint8).reshape(K, N) % 0x7C
                b = torch.where(b & 0x7C == 0x7C, finite | (0x80 & b), b)
            output = torch.empty((M, N), dtype=torch.bfloat16, device=device)
        else:
            a = torch.randn(M, K, device=device, dtype=torch.float16)
            b = torch.randn(K, N, device=device, dtype=torch.float16)
            scale_a = None
            a_type, b_type = None, None
            output = torch.empty((M, N), dtype=torch.float16, device=device)
        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N), 1)
    
    
        # Pass K_MXFP to make explicit that KB is multiple of 32 and KA is multiple of 16 or 32º
        if scale:
            K = scale_a.shape[-1]
        stride_sm, stride_sk = scale_a.stride() if scale else (0, 0)
>       handler = matmul_kernel[grid](a, scale_a, b, output, M, N, K, a.stride(0), a.stride(1), stride_sm, stride_sk,
                                        b.stride(0), b.stride(1), output.stride(0), output.stride(1), BLOCK_M, BLOCK_N,
                                        BLOCK_K, NUM_STAGES=NUM_STAGES, a_type=a_type, b_type=b_type)

sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_58001.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/var/lib/jenkins/triton/python/triton/runtime/jit.py:330: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
/var/lib/jenkins/triton/python/triton/runtime/jit.py:623: in run
    kernel = self.compile(
/var/lib/jenkins/triton/python/triton/compiler/compiler.py:273: in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <triton.compiler.compiler.ASTSource object at 0x7968b0be2060>
options = HIPOptions(num_warps=4, waves_per_eu=1, num_stages=2, num_ctas=1, num_buffers_warp_spec=0, num_consumer_groups=0, reg_...=1, allow_flush_denorm=False, max_num_imprecise_acc_default=0, backend_name='hip', instruction_sched_variant='default')
codegen_fns = {'min_dot_size': <function min_dot_size.<locals>.<lambda> at 0x7968b0bed4e0>}
module_map = {'triton.language.extra.libdevice': <module 'triton.language.extra.hip.libdevice' from '/var/lib/jenkins/triton/python/triton/language/extra/hip/libdevice.py'>}
context = <triton._C.libtriton.ir.context object at 0x7968b11c2b70>

    def make_ir(self, options, codegen_fns, module_map, context):
>       return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
                           module_map=module_map)
E       triton.compiler.errors.CompilationError: at 77:12:
E           a_mask = _m[:, None] < M
E           b_mask = _k[None, :] < K_MXFP
E           a_ptrs = a_ptr + (_m[:, None] * stride_am + _k[None, :] * stride_ak)
E           b_ptrs = b_ptr + (_k[:, None] * stride_bk + _n[None, :] * stride_bn)
E           if a_type:
E               a = tl.load(a_ptrs, mask=a_mask, other=0)
E               a = mxfp_decode(a, a_type, BLOCK_K, K_MXFP)
E           else:
E               a = tl.load(a_ptrs, mask=a_mask, other=0)
E           acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
E           for k in tl.range(0, tl.cdiv(K_MXFP, BLOCK_K), num_stages=NUM_STAGES):
E               b = tl.load(b_ptrs, mask=b_mask[None, :], other=0)
E                   ^

/var/lib/jenkins/triton/python/triton/compiler/compiler.py:100: CompilationError
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_58001.py::test_pipeline_matmul[False] - triton.compiler.errors.CompilationError: at 77:12:
    a_mask = _m[:, None] < M
    b_mask = _k[None, :] < K_MXFP
    a_ptrs = a_ptr + (_m[:, None] * stride_am + _k[None, :] * stride_ak)
    b_ptrs = b_ptr + (_k[:, None] * stride_bk + _n[None, :] * stride_bn)
    if a_type:
        a = tl.load(a_ptrs, mask=a_mask, other=0)
        a = mxfp_decode(a, a_type, BLOCK_K, K_MXFP)
    else:
        a = tl.load(a_ptrs, mask=a_mask, other=0)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    for k in tl.range(0, tl.cdiv(K_MXFP, BLOCK_K), num_stages=NUM_STAGES):
        b = tl.load(b_ptrs, mask=b_mask[None, :], other=0)
            ^
============= 1 failed, 1 passed, 1 skipped, 7 deselected in 8.30s =============

Generated call accuracy: False
Execution accuracy: False
Match percentage: 0.00%
Error: Key mismatch at : Missing keys: {'test_pipeline_matmul_False'} 
2025-08-08_10-56-57 => File: test_load_reduce.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_load_reduce_128_64_float16'}_______________________ test_load_reduce[128-64-float16] _______________________

args = (<triton.language.core.tensor object at 0x72d7b99383b0>, <triton.language.core.tensor object at 0x72d7b9aebdd0>)
kwargs = {'_builder': <triton._C.libtriton.ir.builder object at 0x72d7ba5d1b50>}

    @wraps(fn)
    def wrapper(*args, **kwargs):
        if "_builder" not in kwargs or kwargs["_builder"] is None:
            print(kwargs)
            raise ValueError("Did you forget to add @triton.jit ? "
                             "(`_builder` argument must be provided outside of JIT functions.)")
>       return fn(*args, **kwargs)

/var/lib/jenkins/triton/python/triton/language/core.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pointer = <triton.language.core.tensor object at 0x72d7b99383b0>
value = <triton.language.core.tensor object at 0x72d7b9aebdd0>, mask = None
boundary_check = (), cache_modifier = '', eviction_policy = ''
_builder = <triton._C.libtriton.ir.builder object at 0x72d7ba5d1b50>

    @_tensor_member_fn
    @builtin
    def store(pointer, value, mask=None, boundary_check=(), cache_modifier="", eviction_policy="", _builder=None):
        """
        Store a tensor of data into memory locations defined by `pointer`.
    
            (1) If `pointer` is a single element pointer, a scalar is stored.  In
                this case:
    
                - `mask` must also be scalar, and
                - `boundary_check` and `padding_option` must be empty.
    
            (2) If `pointer` is an N-dimensional tensor of pointers, an
                N-dimensional block is stored.  In this case:
    
                - `mask` is implicitly broadcast to `pointer.shape`, and
                - `boundary_check` must be empty.
    
            (3) If `pointer` is a block pointer defined by `make_block_ptr`, a block
                of data is stored.  In this case:
    
                - `mask` must be None, and
                - `boundary_check` can be specified to control the behavior of out-of-bound access.
    
        `value` is implicitly broadcast to `pointer.shape` and typecast to `pointer.dtype.element_ty`.
    
        :param pointer: The memory location where the elements of `value` are stored
        :type pointer: `triton.PointerType`, or block of `dtype=triton.PointerType`
        :param value: The tensor of elements to be stored
        :type value: Block
        :param mask: If `mask[idx]` is false, do not store `value[idx]` at `pointer[idx]`
        :type mask: Block of triton.int1, optional
        :param boundary_check: tuple of integers, indicating the dimensions which should do the boundary check
        :type boundary_check: tuple of ints, optional
        :param cache_modifier: changes cache option in NVIDIA PTX
        :type cache_modifier: str, optional, should be one of {"", ".wb", ".cg", ".cs", ".wt"}, where ".wb" stands for
            cache write-back all coherent levels, ".cg" stands for cache global, ".cs" stands for cache streaming, ".wt"
            stands for cache write-through, see `cache operator <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#cache-operators>`_ for more details.
        :param eviction_policy: changes eviction policy in NVIDIA PTX
        :type eviction_policy: str, optional, should be one of {"", "evict_first", "evict_last"}
        """
        # `value` can be constexpr
        value = semantic.to_tensor(value, _builder)
        mask = _constexpr_to_value(mask)
        if mask is not None:
            mask = semantic.to_tensor(mask, _builder)
        cache_modifier = _constexpr_to_value(cache_modifier)
        eviction_policy = _constexpr_to_value(eviction_policy)
>       return semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy, _builder)

/var/lib/jenkins/triton/python/triton/language/core.py:1710: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ptr = <triton.language.core.tensor object at 0x72d7b99383b0>
val = <triton.language.core.tensor object at 0x72d7b9aebdd0>, mask = None
boundary_check = (), cache_modifier = '', eviction_policy = ''
builder = <triton._C.libtriton.ir.builder object at 0x72d7ba5d1b50>

    def store(ptr: tl.tensor, val: tl.tensor, mask: Optional[tl.tensor], boundary_check, cache_modifier: str,
              eviction_policy: str, builder: ir.builder) -> tl.tensor:
        # Cache and eviction options
        cache = _str_to_store_cache_modifier(cache_modifier)
        eviction = _str_to_eviction_policy(eviction_policy)
    
        if ptr.type.is_const() or ptr.type.scalar.is_const():
            raise ValueError("Cannot store to a constant pointer")
    
        if ptr.type.is_ptr() and ptr.type.element_ty.is_block():
            # Store by a block pointer: `pointer_type<block_type<>>`
            return _store_block_pointer(ptr, val, mask, boundary_check, cache, eviction, builder)
        else:
            # Store by a tensor of pointers or a pointer of scalar: `block_type<pointer_type<>>` or `pointer_type<>`
>           return _store_legacy(ptr, val, mask, boundary_check, cache, eviction, builder)

/var/lib/jenkins/triton/python/triton/language/semantic.py:1280: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ptr = <triton.language.core.tensor object at 0x72d7b99383b0>
val = <triton.language.core.tensor object at 0x72d7b9aebdd0>, mask = None
boundary_check = (), cache = <CACHE_MODIFIER.NONE: 1>
eviction = <EVICTION_POLICY.NORMAL: 1>
builder = <triton._C.libtriton.ir.builder object at 0x72d7ba5d1b50>

    def _store_legacy(ptr, val, mask, boundary_check, cache, eviction, builder):
        # Store by a tensor of pointers or a pointer of scalar: `block_type<pointer_type<>>` or `pointer_type<>`
        if not ptr.type.scalar.is_ptr():
            raise ValueError(f"Unsupported ptr type {ptr.type.__repr__()} in `tl.store`")
    
        # Check `boundary_check` argument
        if boundary_check:
            raise ValueError("`boundary_check` argument is not supported for storing a tensor of pointers or storing a "
                             "scalar. Because the compiler does not know the boundary; please use block pointers "
                             "(defined by `make_block_ptr`) instead")
    
        # For a pointer of scalar, check the type of `val` and `mask`
        if not ptr.type.is_block():
            if val.type.is_block():
>               raise ValueError("Value argument cannot be block type if pointer argument is not a block")
E               ValueError: Value argument cannot be block type if pointer argument is not a block

/var/lib/jenkins/triton/python/triton/language/semantic.py:1236: ValueError

The above exception was the direct cause of the following exception:

BLOCK_M = 128, BLOCK_N = 64, dtype_str = 'float16'
request = <FixtureRequest for <Function test_load_reduce[128-64-float16]>>

    @pytest.mark.parametrize('BLOCK_M,BLOCK_N,dtype_str', [(128, 64, dtype_str) for dtype_str in ['float16']])
    def test_load_reduce(BLOCK_M, BLOCK_N, dtype_str, request):
        set_seed()
    
        dtype = dtype_mapping[dtype_str]
        x = torch.randn((BLOCK_M, BLOCK_N), device='cuda', dtype=dtype)
        y = torch.empty((BLOCK_M, ), device='cuda', dtype=dtype)
    
>       load_reduce_kernel[(1, )](x, y, x.stride(0), x.stride(1), y.stride(0), BLOCK_M, BLOCK_N)

sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_load_reduce_py_gen_triton_code_300328.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/var/lib/jenkins/triton/python/triton/runtime/jit.py:330: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
/var/lib/jenkins/triton/python/triton/runtime/jit.py:623: in run
    kernel = self.compile(
/var/lib/jenkins/triton/python/triton/compiler/compiler.py:273: in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <triton.compiler.compiler.ASTSource object at 0x72d7ba3b56a0>
options = HIPOptions(num_warps=4, waves_per_eu=1, num_stages=2, num_ctas=1, num_buffers_warp_spec=0, num_consumer_groups=0, reg_...=1, allow_flush_denorm=False, max_num_imprecise_acc_default=0, backend_name='hip', instruction_sched_variant='default')
codegen_fns = {'min_dot_size': <function min_dot_size.<locals>.<lambda> at 0x72d7b9910d60>}
module_map = {'triton.language.extra.libdevice': <module 'triton.language.extra.hip.libdevice' from '/var/lib/jenkins/triton/python/triton/language/extra/hip/libdevice.py'>}
context = <triton._C.libtriton.ir.context object at 0x72d7ba5d54b0>

    def make_ir(self, options, codegen_fns, module_map, context):
>       return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
                           module_map=module_map)
E       triton.compiler.errors.CompilationError: at 40:4:
E               the number of output elements produced.
E           BLOCK_N: tl.constexpr
E               The size of the tile (or block) in the N dimension. This defines how many
E               "columns" of the input data are processed for each "row" in the M dimension,
E               and it's the dimension over which the reduction (max) is performed.
E           """
E           pid0 = tl.program_id(axis=0)
E           offset_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)
E           offsets_m = tl.max_contiguous(tl.multiple_of(offset_m % 8, BLOCK_M), BLOCK_M)
E           x = tl.load(x_ptr + offsets_m[:, None] * stride_xm + tl.arange(0, BLOCK_N) * stride_xn)
E           y = tl.max(x, axis=1)
E           tl.store(y_ptr + pid0 * stride_y, y)
E           ^

/var/lib/jenkins/triton/python/triton/compiler/compiler.py:100: CompilationError
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_load_reduce_py_gen_triton_code_300328.py::test_load_reduce[128-64-float16] - triton.compiler.errors.CompilationError: at 40:4:
        the number of output elements produced.
    BLOCK_N: tl.constexpr
        The size of the tile (or block) in the N dimension. This defines how many
        "columns" of the input data are processed for each "row" in the M dimension,
        and it's the dimension over which the reduction (max) is performed.
    """
    pid0 = tl.program_id(axis=0)
    offset_m = pid0 * BLOCK_M + tl.arange(0, BLOCK_M)
    offsets_m = tl.max_contiguous(tl.multiple_of(offset_m % 8, BLOCK_M), BLOCK_M)
    x = tl.load(x_ptr + offsets_m[:, None] * stride_xm + tl.arange(0, BLOCK_N) * stride_xn)
    y = tl.max(x, axis=1)
    tl.store(y_ptr + pid0 * stride_y, y)
    ^
================== 1 failed, 1 passed, 43 deselected in 8.40s ==================

Generated call accuracy: False
Execution accuracy: False
Match percentage: 0.00%
Error: Key mismatch at : Missing keys: {'test_load_reduce_128_64_float16'} 
2025-08-08_10-57-27 => File: test_add_kernel.py, Call Status: True, Exec Status: True, difficulty: -1, stderr: None
2025-08-08_10-57-50 => File: test_gemm_no_scf.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_gemm_no_scf_py_gen_triton_code_709960_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_gemm_no_scf_py_gen_triton_code_709960.py _
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/python.py:617: in _importtestmodule
    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/pathlib.py:564: in import_path
    importlib.import_module(module_name)
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:169: in exec_module
    source_stat, co = _rewrite_test(fn, self.config)
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:351: in _rewrite_test
    tree = ast.parse(source, filename=strfn)
/opt/conda/envs/py_3.12/lib/python3.12/ast.py:52: in parse
    return compile(source, filename, mode, flags,
E     File "/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_gemm_no_scf_py_gen_triton_code_709960.py", line 172
E       else:
E       ^^^^
E   SyntaxError: invalid syntax
2025-08-08_10-58-25 => File: test_flashattention_fwd.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1_5-7B-FT-new_2_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_604229_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_604229.py _
ImportError while importing test module '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_604229.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/opt/conda/envs/py_3.12/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_604229.py:5: in <module>
    import flash_attn_v2
E   ModuleNotFoundError: No module named 'flash_attn_v2'
2025-08-08_10-58-25 => File: sample/20250530_reflexion_oneshot_codeQwen-1.5-7B-FT-new_2_0.json, Call Accuracy: 0.16666666666666666, Exec Accuracy: 0.1
