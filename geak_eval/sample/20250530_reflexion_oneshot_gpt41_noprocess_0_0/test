2025-08-07_21-59-09 => File: moe_gemm.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/moe_gemm_py_gen_triton_code_606927_py.pt'
2025-08-07_22-39-10 => File: multreduce_matmul_dot_kernel.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Command '['python3 /root/sapmajum/dev/TB-eval/geak_eval/evaluators/ROCm_correctness.py --gen_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/multreduce_matmul_dot_kernel_py_gen_triton_code_161864.py --ref_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/multreduce_matmul_dot_kernel_py_ref_triton_code_983761.py --atol 0.01 --rtol 0.01 --global_timeout 2400 --verbose']' timed out after 2400 seconds
2025-08-07_23-19-10 => File: triton_multreduce_matmul_kernel.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Command '['python3 /root/sapmajum/dev/TB-eval/geak_eval/evaluators/ROCm_correctness.py --gen_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/triton_multreduce_matmul_kernel_py_gen_triton_code_293827.py --ref_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/triton_multreduce_matmul_kernel_py_ref_triton_code_746250.py --atol 0.01 --rtol 0.01 --global_timeout 2400 --verbose']' timed out after 2400 seconds
2025-08-07_23-59-10 => File: gemm.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Command '['python3 /root/sapmajum/dev/TB-eval/geak_eval/evaluators/ROCm_correctness.py --gen_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/gemm_py_gen_triton_code_951491.py --ref_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/gemm_py_ref_triton_code_708096.py --atol 0.01 --rtol 0.01 --global_timeout 2400 --verbose']' timed out after 2400 seconds
2025-08-08_00-00-45 => File: layernorm.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/layernorm_py_gen_triton_code_762339_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/layernorm_py_gen_triton_code_762339.py _
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/layernorm_py_gen_triton_code_762339.py:42: in <module>
    @triton.jit
E   NameError: name 'triton' is not defined
2025-08-08_00-02-11 => File: softmax.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/softmax_py_gen_triton_code_201472_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/softmax_py_gen_triton_code_201472.py _
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/softmax_py_gen_triton_code_201472.py:177: in <module>
    @pytest.mark.parametrize('M, N', [(1823, 781), (1, 1), (128, 1), (1, 128), (8192, 8192), (4096, 8192), (359, 1),
E   NameError: name 'pytest' is not defined
2025-08-08_00-02-45 => File: test_chained_dot_fp8.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_dot_fp8_py_gen_triton_code_941445_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_dot_fp8_py_gen_triton_code_941445.py _
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_dot_fp8_py_gen_triton_code_941445.py:13: in <module>
    float8: tl.constexpr = None if not TORCH_HAS_FP8E4 else torch.float8_e4m3fnuz
E   NameError: name 'tl' is not defined
2025-08-08_00-03-23 => File: test_cast_matmul.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_cast_matmul_py_gen_triton_code_521743_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_cast_matmul_py_gen_triton_code_521743.py _
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_cast_matmul_py_gen_triton_code_521743.py:41: in <module>
    @pytest.mark.parametrize("M, K, N, w_dtype, x_dtype, out_dtype",
E   NameError: name 'pytest' is not defined
2025-08-08_00-04-01 => File: test_gemm_fusion.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_gemm_fusion_py_gen_triton_code_111572_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_gemm_fusion_py_gen_triton_code_111572.py _
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_gemm_fusion_py_gen_triton_code_111572.py:41: in <module>
    @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason="not passed on ampere")
E   NameError: name 'pytest' is not defined
2025-08-08_00-04-38 => File: test_chained_matmul.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_chained_matmul'}_____________________________ test_chained_matmul ______________________________

request = <FixtureRequest for <Function test_chained_matmul>>, device = 'cuda'

    def test_chained_matmul(request, device='cuda'):
        # Regression test for issue #1601
        set_seed()
    
    
        m, n, k = 32, 64, 128
        block_m, block_n, block_k = 16, 32, k
    
>       grid = (triton.cdiv(m, block_m), )
E       NameError: name 'triton' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_matmul_py_gen_triton_code_381351.py:56: NameError
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_matmul_py_gen_triton_code_381351.py::test_chained_matmul - NameError: name 'triton' is not defined
================= 1 failed, 1 passed, 151 deselected in 8.30s ==================

Generated call accuracy: False
Execution accuracy: False
Match percentage: 0.00%
Error: Key mismatch at : Missing keys: {'test_chained_matmul'} 
2025-08-08_00-05-11 => File: test_batched_vecmat.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_vecmat'}_________________________________ test_vecmat __________________________________

request = <FixtureRequest for <Function test_vecmat>>, device = 'cuda'

    def test_vecmat(request, device='cuda'):
        """
        Test the batched vector-matrix multiplication kernel.
    
        Args:
            device: The device (e.g., 'cuda' or 'cpu') on which the test is executed.
            request: Pytest request object used to retrieve the test case name.
        """
        set_seed()
    
        M, N, K = 128, 128, 128
        block_m, block_n, block_k = 16, 32, 64
    
        rs = RandomState(17)
        A_vec = rs.randint(0, 4, (M, K)).astype('float32')
        B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')
        A = A_vec
        B = B_vec
    
        A_tri = torch.tensor(A, device=device)
        B_tri = torch.tensor(B, device=device)
        C_tri = torch.zeros((M, N), dtype=torch.float32, device=device)
    
        grid = (M // block_m, N // block_n)
    
>       batched_vecmat[grid](
            A_tri, B_tri, M, N, K, C_tri,  #
            block_m=block_m, block_n=block_n, block_k=block_k,  #
            num_warps=4, num_stages=1)
E       NameError: name 'batched_vecmat' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_batched_vecmat_py_gen_triton_code_523292.py:135: NameError
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_batched_vecmat_py_gen_triton_code_523292.py::test_vecmat - NameError: name 'batched_vecmat' is not defined
================== 1 failed, 1 passed, 31 deselected in 8.59s ==================

Generated call accuracy: False
Execution accuracy: False
Match percentage: 0.00%
Error: Key mismatch at : Missing keys: {'test_vecmat'} 
2025-08-08_00-05-46 => File: test_iv_dependent_matmul.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_iv_dependent_matmul_post_pre_mixed', 'test_iv_dependent_matmul_post_load', 'test_iv_dependent_matmul_post_load_three_iters', 'test_iv_dependent_matmul_pre_load', 'test_iv_dependent_matmul_post_load_two_iters'}______________________ test_iv_dependent_matmul[pre_load] ______________________

type = 'pre_load'
request = <FixtureRequest for <Function test_iv_dependent_matmul[pre_load]>>
device = 'cuda'

    @pytest.mark.parametrize("type",
                             ["pre_load", "post_load", "post_pre_mixed", "post_load_two_iters", "post_load_three_iters"])
    def test_iv_dependent_matmul(type, request, device='cuda'):
    
    
        set_seed()
    
        M = 256
        K = 256
        N = 256
        BLOCK_SIZE_K = 32
        BLOCK_SIZE_N = 32
        BLOCK_SIZE_M = 32
    
        a = torch.rand((M, K), device=device)
        b = torch.rand((K, N), device=device)
    
        torch_output = torch.mm(a, b)
        triton_output = torch.empty_like(torch_output, device=torch_output.device)
    
        def grid(META):
            return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )
    
        num_stages = 4 if type == "post_load_three_iters" else 3
>       iv_dependent_matmul[grid](
            a, b, triton_output, M, N, K,  #
            a.stride(0), a.stride(1), b.stride(0), b.stride(1),  #
            triton_output.stride(0), triton_output.stride(1),  #
            BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K, type=type,  #
            num_stages=num_stages)
E       NameError: name 'iv_dependent_matmul' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_iv_dependent_matmul_py_gen_triton_code_405615.py:69: NameError
2025-08-08_00-06-19 => File: test_reverse_range.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_reverse_range'}______________________________ test_reverse_range ______________________________

request = <FixtureRequest for <Function test_reverse_range>>, device = 'cuda'

    def test_reverse_range(request, device='cuda'):
        set_seed()
    
    
        data = torch.randn((516, ), dtype=torch.float32, device=device)
        res = torch.empty((512, ), dtype=torch.float32, device=device)
>       reverse_range[(1, )](data, res)
E       NameError: name 'reverse_range' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_reverse_range_py_gen_triton_code_733446.py:50: NameError
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_reverse_range_py_gen_triton_code_733446.py::test_reverse_range - NameError: name 'reverse_range' is not defined
================== 1 failed, 1 passed, 10 deselected in 8.65s ==================

Generated call accuracy: False
Execution accuracy: False
Match percentage: 0.00%
Error: Key mismatch at : Missing keys: {'test_reverse_range'} 
2025-08-08_00-11-06 => File: rmsnorm_fwd.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_rmsnorm_4096_8192_False_bf16_bf16_y_triton', 'test_rmsnorm_256_4096_True_fp16_bf16_rsigma', 'test_rmsnorm_1_4_False_fp16_bf16_y_triton', 'test_rmsnorm_2_10_True_fp16_bf16_rsigma', 'test_rmsnorm_2_10_False_bf16_fp16_y_triton', 'test_rmsnorm_8192_65536_True_bf16_bf16_rsigma', 'test_rmsnorm_256_4096_False_fp16_bf16_y_triton', 'test_rmsnorm_1_4_False_bf16_fp16_y_triton', 'test_rmsnorm_256_4096_False_bf16_fp16_rsigma', 'test_rmsnorm_1_4_True_bf16_fp16_rsigma', 'test_rmsnorm_1_4_True_fp16_fp16_rsigma', 'test_rmsnorm_8192_65536_False_fp16_bf16_rsigma', 'test_rmsnorm_1_31744_True_fp16_fp16_y_triton', 'test_rmsnorm_1_31744_False_bf16_bf16_y_triton', 'test_rmsnorm_1_4_True_fp16_bf16_y_triton', 'test_rmsnorm_256_4096_True_bf16_bf16_rsigma', 'test_rmsnorm_1_31744_False_fp16_bf16_y_triton', 'test_rmsnorm_4096_8192_True_fp16_fp16_rsigma', 'test_rmsnorm_4096_8192_True_fp16_bf16_y_triton', 'test_rmsnorm_256_4096_True_bf16_fp16_y_triton', 'test_rmsnorm_8192_65536_False_bf16_fp16_rsigma', 'test_rmsnorm_4096_8192_True_bf16_fp16_rsigma', 'test_rmsnorm_256_4096_True_fp16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_fp16_fp16_y_triton', 'test_rmsnorm_4096_8192_False_bf16_fp16_y_triton', 'test_rmsnorm_1_4_True_bf16_bf16_rsigma', 'test_rmsnorm_8192_65536_True_bf16_fp16_rsigma', 'test_rmsnorm_4096_8192_False_fp16_bf16_rsigma', 'test_rmsnorm_1_31744_True_bf16_fp16_y_triton', 'test_rmsnorm_1_31744_True_fp16_fp16_rsigma', 'test_rmsnorm_1_31744_True_bf16_fp16_rsigma', 'test_rmsnorm_8192_65536_True_fp16_fp16_y_triton', 'test_rmsnorm_2_10_False_bf16_fp16_rsigma', 'test_rmsnorm_8192_65536_True_bf16_bf16_y_triton', 'test_rmsnorm_1_31744_False_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_True_fp16_fp16_rsigma', 'test_rmsnorm_873_1245_False_fp16_fp16_rsigma', 'test_rmsnorm_873_1245_False_bf16_bf16_y_triton', 'test_rmsnorm_1_4_False_fp16_fp16_rsigma', 'test_rmsnorm_256_4096_True_fp16_fp16_rsigma', 'test_rmsnorm_256_4096_True_bf16_bf16_y_triton', 'test_rmsnorm_873_1245_True_bf16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_fp16_bf16_y_triton', 'test_rmsnorm_2_10_False_bf16_bf16_y_triton', 'test_rmsnorm_4096_8192_True_bf16_bf16_rsigma', 'test_rmsnorm_4096_8192_True_bf16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_bf16_bf16_rsigma', 'test_rmsnorm_256_4096_False_fp16_fp16_y_triton', 'test_rmsnorm_1_31744_True_bf16_bf16_rsigma', 'test_rmsnorm_1_4_False_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_False_fp16_bf16_y_triton', 'test_rmsnorm_256_4096_False_bf16_bf16_rsigma', 'test_rmsnorm_873_1245_True_bf16_fp16_rsigma', 'test_rmsnorm_256_4096_True_bf16_fp16_rsigma', 'test_rmsnorm_1_31744_True_fp16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_fp16_fp16_rsigma', 'test_rmsnorm_4096_8192_True_fp16_bf16_rsigma', 'test_rmsnorm_1_31744_True_fp16_bf16_rsigma', 'test_rmsnorm_1_4_True_fp16_bf16_rsigma', 'test_rmsnorm_1_31744_False_fp16_fp16_rsigma', 'test_rmsnorm_873_1245_True_fp16_bf16_y_triton', 'test_rmsnorm_4096_8192_False_fp16_bf16_y_triton', 'test_rmsnorm_4096_8192_False_bf16_bf16_rsigma', 'test_rmsnorm_1_4_False_fp16_bf16_rsigma', 'test_rmsnorm_2_10_True_fp16_fp16_rsigma', 'test_rmsnorm_873_1245_True_fp16_bf16_rsigma', 'test_rmsnorm_256_4096_False_bf16_bf16_y_triton', 'test_rmsnorm_4096_8192_True_bf16_fp16_y_triton', 'test_rmsnorm_2_10_False_fp16_fp16_rsigma', 'test_rmsnorm_8192_65536_True_fp16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_bf16_fp16_y_triton', 'test_rmsnorm_8192_65536_True_fp16_bf16_rsigma', 'test_rmsnorm_256_4096_False_fp16_bf16_rsigma', 'test_rmsnorm_1_4_False_bf16_bf16_rsigma', 'test_rmsnorm_873_1245_True_bf16_bf16_rsigma', 'test_rmsnorm_1_31744_False_bf16_fp16_y_triton', 'test_rmsnorm_8192_65536_False_bf16_bf16_y_triton', 'test_rmsnorm_1_4_True_bf16_fp16_y_triton', 'test_rmsnorm_2_10_True_bf16_bf16_rsigma', 'test_rmsnorm_2_10_False_fp16_bf16_rsigma', 'test_rmsnorm_8192_65536_True_bf16_fp16_y_triton', 'test_rmsnorm_2_10_True_bf16_bf16_y_triton', 'test_rmsnorm_1_4_False_bf16_fp16_rsigma', 'test_rmsnorm_2_10_True_bf16_fp16_rsigma', 'test_rmsnorm_2_10_True_fp16_bf16_y_triton', 'test_rmsnorm_873_1245_True_bf16_fp16_y_triton', 'test_rmsnorm_873_1245_False_fp16_bf16_rsigma', 'test_rmsnorm_1_4_False_bf16_bf16_y_triton', 'test_rmsnorm_2_10_True_fp16_fp16_y_triton', 'test_rmsnorm_4096_8192_False_fp16_fp16_rsigma', 'test_rmsnorm_2_10_False_bf16_bf16_rsigma', 'test_rmsnorm_4096_8192_False_fp16_fp16_y_triton', 'test_rmsnorm_2_10_False_fp16_bf16_y_triton', 'test_rmsnorm_873_1245_True_fp16_fp16_y_triton', 'test_rmsnorm_2_10_True_bf16_fp16_y_triton', 'test_rmsnorm_873_1245_False_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_False_bf16_fp16_rsigma', 'test_rmsnorm_1_31744_False_bf16_bf16_rsigma', 'test_rmsnorm_1_4_True_bf16_bf16_y_triton', 'test_rmsnorm_256_4096_False_bf16_fp16_y_triton', 'test_rmsnorm_4096_8192_True_fp16_fp16_y_triton', 'test_rmsnorm_2_10_False_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_False_bf16_fp16_y_triton', 'test_rmsnorm_1_4_True_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_False_bf16_bf16_rsigma', 'test_rmsnorm_4096_8192_False_bf16_fp16_rsigma', 'test_rmsnorm_8192_65536_True_fp16_fp16_rsigma', 'test_rmsnorm_1_31744_True_bf16_bf16_y_triton', 'test_rmsnorm_1_31744_False_fp16_bf16_rsigma', 'test_rmsnorm_256_4096_False_fp16_fp16_rsigma', 'test_rmsnorm_256_4096_True_fp16_fp16_y_triton', 'test_rmsnorm_1_31744_False_bf16_fp16_rsigma'}_______________________ test_rmsnorm[1-4-True-fp16-fp16] _______________________

M = 1, N = 4, ZERO_CENTERED_GAMMA = True, in_dtype_str = 'fp16'
out_dtype_str = 'fp16'
request = <FixtureRequest for <Function test_rmsnorm[1-4-True-fp16-fp16]>>

    @pytest.mark.parametrize("in_dtype_str", ["fp16", "bf16"])
    @pytest.mark.parametrize("out_dtype_str", ["fp16", "bf16"])
    @pytest.mark.parametrize('ZERO_CENTERED_GAMMA', [True, False])
    @pytest.mark.parametrize('M, N', [
        (1, 4),
        (2, 10),
        (256, 4096),
        (4096, 8192),
        (1, 31744),
        (8192, 65536),
        (873, 1245),
    ])
    def test_rmsnorm(M, N, ZERO_CENTERED_GAMMA, in_dtype_str, out_dtype_str, request):
        in_dtype = arg_to_torch_dtype[in_dtype_str]
        out_dtype = arg_to_torch_dtype[out_dtype_str]
        set_seed()
    
        x = torch.randn(M, N, device='cuda', dtype=in_dtype, requires_grad=True)
        g = torch.ones((1, N), device='cuda', dtype=in_dtype, requires_grad=True)
        y = torch.zeros_like(x, device='cuda', dtype=out_dtype)
        rsigma = torch.empty((M, ), device=x.device, dtype=torch.float32)
    
        dx = torch.empty_like(x, dtype=in_dtype, requires_grad=False)
        dg = torch.empty_like(g, dtype=in_dtype, requires_grad=False)
        dg_tmp = torch.zeros(M, N, device='cuda', dtype=torch.float32, requires_grad=False)
    
        n_rows, n_cols = x.shape
        MAX_FUSED_SIZE = 65536 // x.element_size()
        blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))
        USE_BLOCKED = n_cols > blk_size
>       NUM_PRGMS = min(n_rows, get_num_sms())
E       NameError: name 'get_num_sms' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/rmsnorm_fwd_py_gen_triton_code_11471.py:229: NameError
2025-08-08_00-30-58 => File: rmsnorm_bwd.py, Call Status: True, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_rmsnorm_2_10_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_873_1245_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_1_4_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_1_31744_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_873_1245_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_2_10_True_fp16_bf16_grad_x_triton', 'test_rmsnorm_1_4_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_2_10_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_2_10_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_256_4096_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_873_1245_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_2_10_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_873_1245_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_2_10_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_873_1245_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_4_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_True_fp16_bf16_grad_x_triton', 'test_rmsnorm_2_10_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_2_10_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_1_4_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_1_4_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_1_31744_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_4_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_1_4_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_2_10_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_256_4096_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_31744_True_fp16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_2_10_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_1_31744_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_873_1245_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_2_10_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_1_31744_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_873_1245_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_2_10_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_873_1245_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_256_4096_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_256_4096_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_2_10_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_1_4_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_True_fp16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_2_10_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_1_4_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_2_10_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_873_1245_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_2_10_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_873_1245_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_1_4_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_873_1245_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_1_31744_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_True_fp16_bf16_grad_x_triton'}_______________________ test_rmsnorm[1-4-True-fp16-fp16] _______________________

M = 1, N = 4, ZERO_CENTERED_GAMMA = True, in_dtype_str = 'fp16'
out_dtype_str = 'fp16'
request = <FixtureRequest for <Function test_rmsnorm[1-4-True-fp16-fp16]>>

    @pytest.mark.parametrize("in_dtype_str", ["fp16", "bf16"])
    @pytest.mark.parametrize("out_dtype_str", ["fp16", "bf16"])
    @pytest.mark.parametrize('ZERO_CENTERED_GAMMA', [True, False])
    @pytest.mark.parametrize('M, N', [
        (1, 4),
        (2, 10),
        (256, 4096),
        (1, 31744),
        (873, 1245),
    ])
    def test_rmsnorm(M, N, ZERO_CENTERED_GAMMA, in_dtype_str, out_dtype_str, request):
        in_dtype = arg_to_torch_dtype[in_dtype_str]
        out_dtype = arg_to_torch_dtype[out_dtype_str]
        set_seed()
    
        x = torch.randn(M, N, device='cuda', dtype=in_dtype, requires_grad=True)
        g = torch.ones((1, N), device='cuda', dtype=in_dtype, requires_grad=True)
        y = torch.zeros_like(x, device='cuda', dtype=out_dtype)
        rsigma = torch.empty((M, ), device=x.device, dtype=torch.float32)
    
        dx = torch.empty_like(x, dtype=in_dtype, requires_grad=False)
        dg = torch.empty_like(g, dtype=in_dtype, requires_grad=False)
        dg_tmp = torch.zeros(M, N, device='cuda', dtype=torch.float32, requires_grad=False)
    
        n_rows, n_cols = x.shape
        MAX_FUSED_SIZE = 65536 // x.element_size()
        blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))
        USE_BLOCKED = n_cols > blk_size
        NUM_PRGMS = min(n_rows, get_num_sms())
    
        y_triton = rmsnorm(x, g, y, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED,
                           NUM_PRGMS)
    
        y_torch, rsigma_torch = torch_rmsnorm_fwd(x, g, ZERO_CENTERED_GAMMA, out_dtype)
    
        if out_dtype in (torch.float16, torch.bfloat16):
            atol, rtol = 1e-3, 1e-2
        else:
            # float32 typically can be tighter
            atol, rtol = 1e-5, 1e-5
    
        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])
    
        assert y_triton.dtype == out_dtype, f"y_triton has dtype={y_triton.dtype}, expected {out_dtype}"
        assert y_torch.dtype == out_dtype, f"y_torch has dtype={y_torch.dtype}, expected {out_dtype}"
    
        assert torch.allclose(y_triton, y_torch, atol=atol, rtol=rtol), \
            f"Mismatch in 'y' (in={in_dtype_str}, out={out_dtype_str})"
        assert torch.allclose(rsigma, rsigma_torch, atol=atol, rtol=rtol), \
            f"Mismatch in 'rsigma' (in={in_dtype_str}, out={out_dtype_str})"
    
        grad_output = torch.randn_like(y_torch)
    
        # 1) PyTorch reference backward
        # We must clone and set requires_grad = True for backward
        x_ref = x.clone().detach().requires_grad_()
        g_ref = g.clone().detach().requires_grad_()
        y_ref, rsigma_ref = torch_rmsnorm_fwd(x_ref, g_ref, ZERO_CENTERED_GAMMA, out_dtype)
    
        # Backpropagate through PyTorch
        y_ref.backward(grad_output)
        grad_x_ref = x_ref.grad.to(out_dtype)
        grad_g_ref = g_ref.grad.to(out_dtype)
    
        # 2) Triton backward
        x_triton = x.clone().detach().requires_grad_()
        g_triton = g.clone().detach().requires_grad_()
    
        y_triton_buf = torch.empty_like(x_triton, dtype=out_dtype)
        rsigma_triton = torch.empty((M, ), device=x_triton.device, dtype=torch.float32)
    
        dx_b = torch.empty_like(x_triton, dtype=in_dtype, requires_grad=False)
        dg_b = torch.empty_like(g_triton, dtype=in_dtype, requires_grad=False)
        dg_tmp_b = torch.zeros(M, N, device=x_triton.device, dtype=torch.float32, requires_grad=False)
    
        # Run Triton forward pass to build the graph for backward.
        y_triton = rmsnorm(x_triton, g_triton, y_triton_buf, rsigma_triton, dx_b, dg_b, dg_tmp_b, n_rows, n_cols,
                           ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED, NUM_PRGMS)
>       y_triton.backward(grad_output, retain_graph=True)

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/rmsnorm_bwd_py_gen_triton_code_880259.py:457: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_tensor.py:626: in backward
    torch.autograd.backward(
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:347: in backward
    _engine_run_backward(
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/autograd/graph.py:823: in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/autograd/function.py:307: in apply
    return user_fn(self, *args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ctx = <torch.autograd.function.RMSNormBackward object at 0x727af4f67460>
grad_output = tensor([[ 0.1392, -0.1082, -0.7173,  0.7568]], device='cuda:0',
       dtype=torch.float16)

    @staticmethod
    def backward(ctx, grad_output):
        x, g, rsigma = ctx.saved_tensors
        dg_tmp = ctx.dg_tmp
        dx = ctx.dx
        dg = ctx.dg
        n_rows = ctx.n_rows
        n_cols = ctx.n_cols
        ZERO_CENTERED_GAMMA = ctx.ZERO_CENTERED_GAMMA
        blk_size = ctx.blk_size
        USE_BLOCKED = ctx.USE_BLOCKED
        NUM_PRGMS = ctx.NUM_PRGMS
    
        grid_bwd = lambda meta: (NUM_PRGMS, )
>       rms_bwd_kernel[grid_bwd](grad_output, x, g, rsigma, dx, dg_tmp, x.stride(0), grad_output.stride(0), n_rows,
                                 n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED, NUM_PRGMS, num_warps=ctx.num_warps)
E       NameError: name 'rms_bwd_kernel' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/rmsnorm_bwd_py_gen_triton_code_880259.py:344: NameError
2025-08-08_00-31-35 => File: test_block_pointer_matmul.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_block_ptr_matmul_no_scf_shape5_8', 'test_block_ptr_matmul_no_scf_shape4_4', 'test_block_ptr_matmul_no_scf_shape2_4', 'test_block_ptr_matmul_no_scf_shape0_4', 'test_block_ptr_matmul_no_scf_shape3_8', 'test_block_ptr_matmul_no_scf_shape1_8'}____________________ test_block_ptr_matmul_no_scf[shape0-4] ____________________

shape = [64, 64, 16], num_warps = 4
request = <FixtureRequest for <Function test_block_ptr_matmul_no_scf[shape0-4]>>
device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("shape, num_warps", [  #
        (shape, num_warps) for shape in [
            [64, 64, 16],
            [64, 64, 32],
            [64, 64, 64],
        ] for num_warps in [4, 8]
    ])
    def test_block_ptr_matmul_no_scf(shape, num_warps, request, device='cuda'):
        set_seed()
    
        m, n, k = shape
        a = torch.randn((m, k), device=device, dtype=torch.float16)
        b = torch.randn((k, n), device=device, dtype=torch.float16)
        c = torch.empty((m, n), device=device, dtype=torch.float32)
    
        grid = lambda META: (1, )
>       matmul_no_scf_with_advance_kernel[grid](
            a_ptr=a, b_ptr=b, c_ptr=c,  #
            M=m, N=n, K=k,  #
            stride_am=a.stride(0), stride_ak=a.stride(1),  #
            stride_bk=b.stride(0), stride_bn=b.stride(1),  #
            stride_cm=c.stride(0), stride_cn=c.stride(1),  #
            BLOCK_M=m, BLOCK_N=n, BLOCK_K=k,  #
            num_warps=num_warps)
E       NameError: name 'matmul_no_scf_with_advance_kernel' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_block_pointer_matmul_py_gen_triton_code_457622.py:60: NameError
2025-08-08_00-32-10 => File: test_block_copy.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_block_copy_dtypes_str74_1024_nan', 'test_block_copy_dtypes_str69_512_None', 'test_block_copy_dtypes_str40_512_zero', 'test_block_copy_dtypes_str13_1024_zero', 'test_block_copy_dtypes_str22_256_zero', 'test_block_copy_dtypes_str50_128_nan', 'test_block_copy_dtypes_str72_1024_None', 'test_block_copy_dtypes_str6_256_None', 'test_block_copy_dtypes_str61_64_zero', 'test_block_copy_dtypes_str19_128_zero', 'test_block_copy_dtypes_str0_64_None', 'test_block_copy_dtypes_str64_128_zero', 'test_block_copy_dtypes_str53_256_nan', 'test_block_copy_dtypes_str48_128_None', 'test_block_copy_dtypes_str79_128_zero', 'test_block_copy_dtypes_str49_128_zero', 'test_block_copy_dtypes_str82_256_zero', 'test_block_copy_dtypes_str68_256_nan', 'test_block_copy_dtypes_str62_64_nan', 'test_block_copy_dtypes_str54_512_None', 'test_block_copy_dtypes_str70_512_zero', 'test_block_copy_dtypes_str39_512_None', 'test_block_copy_dtypes_str56_512_nan', 'test_block_copy_dtypes_str71_512_nan', 'test_block_copy_dtypes_str84_512_None', 'test_block_copy_dtypes_str43_1024_zero', 'test_block_copy_dtypes_str52_256_zero', 'test_block_copy_dtypes_str89_1024_nan', 'test_block_copy_dtypes_str45_64_None', 'test_block_copy_dtypes_str88_1024_zero', 'test_block_copy_dtypes_str24_512_None', 'test_block_copy_dtypes_str86_512_nan', 'test_block_copy_dtypes_str55_512_zero', 'test_block_copy_dtypes_str66_256_None', 'test_block_copy_dtypes_str27_1024_None', 'test_block_copy_dtypes_str76_64_zero', 'test_block_copy_dtypes_str25_512_zero', 'test_block_copy_dtypes_str80_128_nan', 'test_block_copy_dtypes_str18_128_None', 'test_block_copy_dtypes_str31_64_zero', 'test_block_copy_dtypes_str30_64_None', 'test_block_copy_dtypes_str58_1024_zero', 'test_block_copy_dtypes_str77_64_nan', 'test_block_copy_dtypes_str12_1024_None', 'test_block_copy_dtypes_str85_512_zero', 'test_block_copy_dtypes_str34_128_zero', 'test_block_copy_dtypes_str75_64_None', 'test_block_copy_dtypes_str81_256_None', 'test_block_copy_dtypes_str87_1024_None', 'test_block_copy_dtypes_str57_1024_None', 'test_block_copy_dtypes_str21_256_None', 'test_block_copy_dtypes_str47_64_nan', 'test_block_copy_dtypes_str9_512_None', 'test_block_copy_dtypes_str15_64_None', 'test_block_copy_dtypes_str28_1024_zero', 'test_block_copy_dtypes_str73_1024_zero', 'test_block_copy_dtypes_str33_128_None', 'test_block_copy_dtypes_str67_256_zero', 'test_block_copy_dtypes_str1_64_zero', 'test_block_copy_dtypes_str78_128_None', 'test_block_copy_dtypes_str51_256_None', 'test_block_copy_dtypes_str10_512_zero', 'test_block_copy_dtypes_str3_128_None', 'test_block_copy_dtypes_str36_256_None', 'test_block_copy_dtypes_str37_256_zero', 'test_block_copy_dtypes_str7_256_zero', 'test_block_copy_dtypes_str63_128_None', 'test_block_copy_dtypes_str65_128_nan', 'test_block_copy_dtypes_str59_1024_nan', 'test_block_copy_dtypes_str16_64_zero', 'test_block_copy_dtypes_str83_256_nan', 'test_block_copy_dtypes_str42_1024_None', 'test_block_copy_dtypes_str60_64_None', 'test_block_copy_dtypes_str46_64_zero', 'test_block_copy_dtypes_str4_128_zero'}_____________________ test_block_copy[dtypes_str0-64-None] _____________________

dtypes_str = ('bool', 'bool'), n = 64, padding_option = None
request = <FixtureRequest for <Function test_block_copy[dtypes_str0-64-None]>>
device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("dtypes_str, n, padding_option", [  #
        (dtypes_str, n, padding)
        for dtypes_str in (("bool", "bool"), ("int16", "int16"), ("int32", "int32"), ("float16", "float16"),
                           ("float32", "float32"), ("bfloat16", "bfloat16"))
        for n in (64, 128, 256, 512, 1024)
        for padding in (None, "zero", "nan")  #
    ])
    def test_block_copy(dtypes_str, n, padding_option, request, device='cuda'):
        src_dtype_str = dtypes_str[0]
        dst_dtype_str = dtypes_str[1]
        src_dtype = getattr(torch, src_dtype_str)
        dst_dtype = getattr(torch, dst_dtype_str)
        check_type_supported(src_dtype, device)
        check_type_supported(dst_dtype, device)
        if src_dtype_str in ("bool", "int16", "int32"):
            if padding_option == "nan":
                pytest.skip("Padding with NaN is not supported for integer types")
            a = torch.randint(0, 2, (n, ), device=device, dtype=src_dtype)
        else:
            a = torch.randn((n, ), device=device, dtype=src_dtype)
        b = torch.zeros((n, ), device=device, dtype=dst_dtype)
    
        grid = lambda meta: (triton.cdiv(n, meta["BLOCK_SIZE"]), )
>       block_copy_kernel[grid](a_ptr=a, b_ptr=b, N=n, BLOCK_SIZE=64, padding_option=padding_option)
E       NameError: name 'block_copy_kernel' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_block_copy_py_gen_triton_code_965927.py:86: NameError
2025-08-08_00-32-46 => File: test_tma_store_gemm.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_tma_load_store_128_128_64_1_4_False_True_True', 'test_tma_load_store_64_64_16_1_4_False_True_False', 'test_tma_load_store_64_128_32_1_4_False_True_False', 'test_tma_load_store_128_64_32_1_4_False_True_False', 'test_tma_load_store_128_64_32_1_4_False_True_True', 'test_tma_load_store_64_64_16_1_4_False_True_True', 'test_tma_load_store_64_128_32_1_4_False_True_True', 'test_tma_load_store_128_128_64_1_4_False_True_False'}______________ test_tma_load_store[64-64-16-1-4-False-True-False] ______________

M = 64, N = 64, K = 16, NUM_CTAS = 1, NUM_WARPS = 4, TRANS_A = False
TRANS_B = True, OUTPUT_F16 = False
request = <FixtureRequest for <Function test_tma_load_store[64-64-16-1-4-False-True-False]>>

    @pytest.mark.parametrize('M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_F16', [
        [64, 64, 16, 1, 4, False, True, False],
        [64, 64, 16, 1, 4, False, True, True],
        [128, 64, 32, 1, 4, False, True, False],
        [128, 64, 32, 1, 4, False, True, True],
        [64, 128, 32, 1, 4, False, True, False],
        [64, 128, 32, 1, 4, False, True, True],
        [128, 128, 64, 1, 4, False, True, False],
        [128, 128, 64, 1, 4, False, True, True],
    ])
    def test_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F16, request):
        set_seed()
    
        if (TRANS_A):
            a = torch.randn((K, M), device='cuda', dtype=torch.float16).T
        else:
            a = torch.randn((M, K), device='cuda', dtype=torch.float16)
        if (TRANS_B):
            b = torch.randn((N, K), device='cuda', dtype=torch.float16).T
        else:
            b = torch.randn((K, N), device='cuda', dtype=torch.float16)
    
        c = torch.empty((M, N), device=a.device, dtype=torch.float32)
        if OUTPUT_F16:
            c = torch.empty((M, N), device=a.device, dtype=torch.float16)
    
>       matmul_tma_load_store[(1, 1)](
            a_ptr=a, b_ptr=b, c_ptr=c,  #
            M=M, N=N, K=K,  #
            stride_am=a.stride(0), stride_ak=a.stride(1),  #
            stride_bk=b.stride(0), stride_bn=b.stride(1),  #
            stride_cm=c.stride(0), stride_cn=c.stride(1),  #
            BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,  #
            num_warps=NUM_WARPS, num_ctas=NUM_CTAS,  #
            OUTPUT_F16=OUTPUT_F16)
E       NameError: name 'matmul_tma_load_store' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_tma_store_gemm_py_gen_triton_code_283699.py:72: NameError
2025-08-08_00-33-18 => File: test_kernel_dot.py, Call Status: True, Exec Status: False, difficulty: -1, stderr: Value mismatch at test_compile_kernel_dot_in_forked_subproc: Tensor-likes are not close!

Mismatched elements: 1 / 1 (100.0%)
Greatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0) (up to 0.01 allowed)__________________ test_compile_kernel_dot_in_forked_subproc ___________________

fresh_triton_cache = '/tmp/tmpuj2ja4lj/.triton'
request = <FixtureRequest for <Function test_compile_kernel_dot_in_forked_subproc>>

    @skip_if_no_target
    def test_compile_kernel_dot_in_forked_subproc(fresh_triton_cache, request) -> None: # Test name updated for clarity
        config = AttrsDescriptor.from_hints({0: 16})
        current_start_method = multiprocessing.get_start_method(allow_none=True)
        if current_start_method is None:
            try:
                multiprocessing.set_start_method('fork', force=True)
            except RuntimeError:
                print("Warning: Could not force 'fork' start method. Using default.")
        if multiprocessing.get_start_method(allow_none=True) != 'fork':
            pytest.skip("Test requires 'fork' multiprocessing start method.")
    
        proc = multiprocessing.Process(target=compile_kernel_dot_for_test, args=(config, ))
        proc.start()
        proc.join(timeout=60)
        if proc.is_alive():
            proc.terminate()
            proc.join()
            pytest.fail("Process timed out")
    
        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])
    
        ################### save tri_out in result_gold ###################
        test_case_name = request.node.name
        sanitized_key_name = test_case_name.replace("::", "_").replace("[", "_").replace("]", "").replace("-", "_")
        result_gold[sanitized_key_name] = torch.tensor([[0.0]]).clone().detach().cpu()
        ###################################################################
    
>       assert proc.exitcode == 0
E       AssertionError: assert 1 == 0
E        +  where 1 = <Process name='Process-1' pid=78605 parent=78582 stopped exitcode=1>.exitcode

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_dot_py_gen_triton_code_956977.py:109: AssertionError
=============================== warnings summary ===============================
geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_dot_py_gen_triton_code_956977.py::test_compile_kernel_dot_in_forked_subproc
  /opt/conda/envs/py_3.12/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=78582) is multi-threaded, use of fork() may lead to deadlocks in the child.
    self.pid = os.fork()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_dot_py_gen_triton_code_956977.py::test_compile_kernel_dot_in_forked_subproc - AssertionError: assert 1 == 0
 +  where 1 = <Process name='Process-1' pid=78605 parent=78582 stopped exitcode=1>.exitcode
============= 1 failed, 1 passed, 7 deselected, 1 warning in 8.20s =============

Generated call accuracy: True
Execution accuracy: False
Match percentage: 0.00%
Error: Value mismatch at test_compile_kernel_dot_in_forked_subproc: Tensor-likes are not close!

Mismatched elements: 1 / 1 (100.0%)
Greatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0) (up to 0.01 allowed)
2025-08-08_00-33-49 => File: test_kernel_sub.py, Call Status: True, Exec Status: False, difficulty: -1, stderr: Value mismatch at test_compile_kernel_sub_in_subproc: Tensor-likes are not close!

Mismatched elements: 1 / 1 (100.0%)
Greatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0) (up to 0.01 allowed)______________________ test_compile_kernel_sub_in_subproc ______________________

fresh_triton_cache = '/tmp/tmph6_kdk_3/.triton'
request = <FixtureRequest for <Function test_compile_kernel_sub_in_subproc>>

    @skip_if_no_target
    def test_compile_kernel_sub_in_subproc(fresh_triton_cache, request) -> None: # Test name updated for clarity
    
        set_seed()
    
        config = AttrsDescriptor.from_hints({i: 16 for i in range(4)})
        try:
            multiprocessing.set_start_method('fork', force=True)
        except RuntimeError:
            print("Warning: Could not force 'fork' start method. Using default.")
            if multiprocessing.get_start_method(allow_none=True) != 'fork': # allow_none for safety
                pytest.skip("Test requires 'fork' multiprocessing start method.")
    
        proc = multiprocessing.Process(target=compile_kernel_sub_for_test, args=(config, ))
        proc.start()
        proc.join(timeout=60)
        if proc.is_alive():
            proc.terminate()
            proc.join()
            pytest.fail("Process timed out")
    
        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])
    
        ################### save tri_out in result_gold ###################
        test_case_name = request.node.name
        sanitized_key_name = test_case_name.replace("::", "_").replace("[", "_").replace("]", "").replace("-", "_")
        result_gold[sanitized_key_name] = torch.tensor([[0.0]]).clone().detach().cpu()
        ###################################################################
    
>       assert proc.exitcode == 0
E       AssertionError: assert 1 == 0
E        +  where 1 = <Process name='Process-1' pid=78699 parent=78676 stopped exitcode=1>.exitcode

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_sub_py_gen_triton_code_82768.py:116: AssertionError
=============================== warnings summary ===============================
geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_sub_py_gen_triton_code_82768.py::test_compile_kernel_sub_in_subproc
  /opt/conda/envs/py_3.12/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=78676) is multi-threaded, use of fork() may lead to deadlocks in the child.
    self.pid = os.fork()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_sub_py_gen_triton_code_82768.py::test_compile_kernel_sub_in_subproc - AssertionError: assert 1 == 0
 +  where 1 = <Process name='Process-1' pid=78699 parent=78676 stopped exitcode=1>.exitcode
============ 1 failed, 1 passed, 43 deselected, 1 warning in 8.16s =============

Generated call accuracy: True
Execution accuracy: False
Match percentage: 0.00%
Error: Value mismatch at test_compile_kernel_sub_in_subproc: Tensor-likes are not close!

Mismatched elements: 1 / 1 (100.0%)
Greatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)
Greatest relative difference: inf at index (0, 0) (up to 0.01 allowed)
2025-08-08_00-34-40 => File: test_triton_flip.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_flip_float32_64_8', 'test_flip_int32_64_8', 'test_flip_bfloat16_512_1', 'test_flip_int32_16_256', 'test_flip_bfloat16_64_8', 'test_flip_bfloat16_8_512', 'test_flip_float16_8_512', 'test_flip_float32_16_256', 'test_flip_int32_8_512', 'test_flip_int32_512_1', 'test_flip_bfloat16_16_256', 'test_flip_float32_512_1', 'test_flip_float16_16_256', 'test_flip_float16_512_1', 'test_flip_float16_64_8', 'test_flip_float32_8_512'}____________________________ test_flip[int32-512-1] ____________________________

N_rows = 1, M_cols = 512, dtype_str = 'int32'
request = <FixtureRequest for <Function test_flip[int32-512-1]>>
device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("M_cols, N_rows", [[512, 1], [64, 8], [16, 256], [8, 512]])
    @pytest.mark.parametrize("dtype_str", ['int32', 'float16', 'float32', 'bfloat16'])
    def test_flip(N_rows, M_cols, dtype_str, request, device='cuda'):
        set_seed()
    
        x_np = gen_numpy_array_for_torch_conversion((N_rows, M_cols), dtype_str=dtype_str)
    
        torch_dtype = torch_dtype_from_str(dtype_str)
        x = torch.from_numpy(x_np).to(dtype=torch_dtype, device=device)
    
        y_ref = torch.flip(x, dims=(1,)) # Flip along the columns (dimension 1)
        z_triton = torch.empty_like(x)
    
        grid = (N_rows,) # Each program flips one row
>       flip_kernel[grid](x, z_triton, N_rows, M_cols) # num_warps removed
E       NameError: name 'flip_kernel' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_flip_py_gen_triton_code_849431.py:94: NameError
2025-08-08_00-35-29 => File: test_triton_sort.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_sort_float16_False_8_512', 'test_sort_bfloat16_True_16_256', 'test_sort_int32_True_64_8', 'test_sort_float16_False_512_1', 'test_sort_float16_True_8_512', 'test_sort_int32_False_16_256', 'test_sort_float32_True_8_512', 'test_sort_int32_False_64_8', 'test_sort_float32_False_512_1', 'test_sort_int32_True_512_1', 'test_sort_float32_False_8_512', 'test_sort_float32_False_16_256', 'test_sort_bfloat16_False_512_1', 'test_sort_bfloat16_False_16_256', 'test_sort_float16_False_64_8', 'test_sort_int32_False_8_512', 'test_sort_int32_True_16_256', 'test_sort_int32_True_8_512', 'test_sort_float16_True_16_256', 'test_sort_float32_True_64_8', 'test_sort_bfloat16_False_64_8', 'test_sort_float32_False_64_8', 'test_sort_float32_True_16_256', 'test_sort_bfloat16_True_64_8', 'test_sort_float16_True_64_8', 'test_sort_bfloat16_True_512_1', 'test_sort_int32_False_512_1', 'test_sort_float32_True_512_1', 'test_sort_float16_False_16_256', 'test_sort_bfloat16_False_8_512', 'test_sort_bfloat16_True_8_512', 'test_sort_float16_True_512_1'}_________________________ test_sort[int32-False-512-1] _________________________

N_rows = 1, M_cols = 512, descending = False, dtype_str = 'int32'
request = <FixtureRequest for <Function test_sort[int32-False-512-1]>>
device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("M_cols, N_rows", [[512, 1], [64, 8], [16, 256], [8, 512]])
    @pytest.mark.parametrize("descending", [False, True])
    @pytest.mark.parametrize("dtype_str", ['int32', 'float16', 'float32', 'bfloat16'])
    def test_sort(N_rows, M_cols, descending, dtype_str, request, device='cuda'):
        set_seed()
    
        x_np = gen_numpy_array_for_torch_conversion((N_rows, M_cols), dtype_str=dtype_str)
    
        torch_dtype = torch_dtype_from_str(dtype_str)
        x = torch.from_numpy(x_np).to(dtype=torch_dtype, device=device) # Ensure correct torch dtype
    
        y_ref = torch.sort(x, dim=1, descending=descending)[0]
        z_triton = torch.empty_like(x)
    
        # Grid is (N_rows,) since each program sorts one row
        grid = (N_rows,)
>       sort_kernel[grid](x, z_triton, N_rows, M_cols, descending) # num_warps removed, let triton decide or set default
E       NameError: name 'sort_kernel' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_sort_py_gen_triton_code_602723.py:103: NameError
2025-08-08_00-36-02 => File: test_triton_swizzle2d.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_swizzle2d_5_7_3'}____________________________ test_swizzle2d[5-7-3] _____________________________

size_i = 5, size_j = 7, size_g = 3
request = <FixtureRequest for <Function test_swizzle2d[5-7-3]>>, device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize("size_i, size_j, size_g", [[5, 7, 3]])
    def test_swizzle2d(size_i, size_j, size_g, request, device='cuda'):
        # Output tensor to store results, initialized to a value like -1 to see what's written
    
        set_seed()
    
        output = torch.zeros(size_i, size_j).to(device)
>       swizzle2d_kernel[(1, )](output, size_i, size_j, size_g)
E       NameError: name 'swizzle2d_kernel' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_391304.py:54: NameError
=============================== warnings summary ===============================
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_391304.py:46
  /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_391304.py:46: PytestUnknownMarkWarning: Unknown pytest.mark.interpreter - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.interpreter

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_391304.py::test_swizzle2d[5-7-3] - NameError: name 'swizzle2d_kernel' is not defined
============ 1 failed, 1 passed, 17 deselected, 1 warning in 8.40s =============

Generated call accuracy: False
Execution accuracy: False
Match percentage: 0.00%
Error: Key mismatch at : Missing keys: {'test_swizzle2d_5_7_3'} 
2025-08-08_00-36-43 => File: test_random_int.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_randint_size1_0_int32_False', 'test_randint_size33_124_int32_False', 'test_randint_size48_0_int32_True', 'test_randint_size35_124_int64_False', 'test_randint_size47_67830198458_int64_False', 'test_randint_size66_4294967295_int64_True', 'test_randint_size54_42_int64_True', 'test_randint_size20_67830198458_int32_True', 'test_randint_size37_54_int32_False', 'test_randint_size32_124_int32_True', 'test_randint_size34_124_int64_True', 'test_randint_size70_67830198458_int64_True', 'test_randint_size13_54_int32_False', 'test_randint_size11_124_int64_False', 'test_randint_size62_54_int64_True', 'test_randint_size68_67830198458_int32_True', 'test_randint_size63_54_int64_False', 'test_randint_size8_124_int32_True', 'test_randint_size30_42_int64_True', 'test_randint_size59_124_int64_False', 'test_randint_size5_42_int32_False', 'test_randint_size18_4294967295_int64_True', 'test_randint_size44_67830198458_int32_True', 'test_randint_size45_67830198458_int32_False', 'test_randint_size27_0_int64_False', 'test_randint_size12_54_int32_True', 'test_randint_size52_42_int32_True', 'test_randint_size38_54_int64_True', 'test_randint_size58_124_int64_True', 'test_randint_size49_0_int32_False', 'test_randint_size4_42_int32_True', 'test_randint_size25_0_int32_False', 'test_randint_size42_4294967295_int64_True', 'test_randint_size0_0_int32_True', 'test_randint_size17_4294967295_int32_False', 'test_randint_size61_54_int32_False', 'test_randint_size50_0_int64_True', 'test_randint_size15_54_int64_False', 'test_randint_size57_124_int32_False', 'test_randint_size65_4294967295_int32_False', 'test_randint_size7_42_int64_False', 'test_randint_size16_4294967295_int32_True', 'test_randint_size36_54_int32_True', 'test_randint_size40_4294967295_int32_True', 'test_randint_size60_54_int32_True', 'test_randint_size28_42_int32_True', 'test_randint_size6_42_int64_True', 'test_randint_size43_4294967295_int64_False', 'test_randint_size51_0_int64_False', 'test_randint_size55_42_int64_False', 'test_randint_size39_54_int64_False', 'test_randint_size67_4294967295_int64_False', 'test_randint_size14_54_int64_True', 'test_randint_size21_67830198458_int32_False', 'test_randint_size53_42_int32_False', 'test_randint_size24_0_int32_True', 'test_randint_size3_0_int64_False', 'test_randint_size10_124_int64_True', 'test_randint_size31_42_int64_False', 'test_randint_size46_67830198458_int64_True', 'test_randint_size23_67830198458_int64_False', 'test_randint_size41_4294967295_int32_False', 'test_randint_size22_67830198458_int64_True', 'test_randint_size2_0_int64_True', 'test_randint_size9_124_int32_False', 'test_randint_size19_4294967295_int64_False', 'test_randint_size29_42_int32_False', 'test_randint_size56_124_int32_True', 'test_randint_size26_0_int64_True', 'test_randint_size64_4294967295_int32_True', 'test_randint_size71_67830198458_int64_False', 'test_randint_size69_67830198458_int32_False'}_______________________ test_randint[size0-0-int32-True] _______________________

size = [10], seed = 0, dtype = 'int32', const_seed = True
request = <FixtureRequest for <Function test_randint[size0-0-int32-True]>>
device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize('size, seed, dtype, const_seed', [(size, seed, dtype, const_seed)
                                                               for size_str in ['10', '4,53', '400'] # Renamed to size_str
                                                               for size in [[int(s) for s in size_str.split(',')]] # Process size here
                                                               for seed in [0, 42, 124, 54, 0xffffffff, 0x0000000fcafeb0ba]
                                                               for dtype in ['int32', 'int64']
                                                               for const_seed in [True, False]])
    def test_randint(size, seed, dtype, const_seed, request, device='cuda'):
        # size = list(map(int, size.split(','))) # Moved to parametrize
        set_seed()
    
        torch_dtype = getattr(torch, dtype)
        numpy_dtype = getattr(np, f"u{dtype}") # Philox generates unsigned integers
        config = {'int32': PHILOX_32, 'int64': PHILOX_64}[dtype]
    
        # triton result
        x = torch.empty(size, dtype=torch_dtype, device=device)
        N = x.numel()
        grid = (triton.cdiv(N, BLOCK), )
        if N > 0: # Ensure grid is not (0,) if N is 0
            if const_seed:
>               randint_kernel_const_seed[grid](x, N, seed_val=seed)
E               NameError: name 'randint_kernel_const_seed' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_random_int_py_gen_triton_code_498599.py:169: NameError
2025-08-08_00-37-41 => File: test_randn.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_rand_100000_42_int64_False', 'test_rand_100000_0_int32_False', 'test_rand_100000_42_int64_True', 'test_rand_100000_0_int64_True', 'test_rand_100000_0_int64_False', 'test_rand_100000_54_int32_True', 'test_rand_100000_0_int32_True', 'test_rand_100000_124_int32_False', 'test_rand_100000_42_int32_True', 'test_rand_100000_54_int64_False', 'test_rand_100000_124_int32_True', 'test_rand_100000_54_int64_True', 'test_rand_100000_124_int64_False', 'test_rand_100000_42_int32_False', 'test_rand_100000_54_int32_False', 'test_rand_100000_124_int64_True'}________________________ test_rand[100000-0-int32-True] ________________________

size = 100000, seed = 0, dtype = 'int32', const_seed = True
request = <FixtureRequest for <Function test_rand[100000-0-int32-True]>>
device = 'cuda'

    @pytest.mark.interpreter
    @pytest.mark.parametrize('size, seed, dtype, const_seed', [(size, seed, dtype, const_seed)
                                                               for size in [100000]
                                                               for seed in [0, 42, 124, 54]
                                                               for dtype in ['int32', 'int64']
                                                               for const_seed in [True, False]])
    def test_rand(size, seed, dtype, const_seed,  request, device='cuda'):
    
    
        # triton result
        set_seed(seed)
    
        x = torch.empty(size, dtype=torch.float32, device=device)
        N = x.numel()
        grid = (triton.cdiv(N, BLOCK), )
        if const_seed:
>           randn_kernel_const_seed[grid](x, N, seed=seed, dtype=getattr(tl, dtype))
E           NameError: name 'randn_kernel_const_seed' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_randn_py_gen_triton_code_82552.py:161: NameError
2025-08-08_00-38-17 => File: test_matmul_MXFP.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_485029_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_485029.py _
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_485029.py:252: in <module>
    if is_cuda() and torch.cuda.get_device_capability()[0] >=9 :
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_485029.py:44: in is_cuda
    return triton.runtime.driver.active.get_current_target().backend == "cuda"
E   NameError: name 'triton' is not defined
2025-08-08_00-38-50 => File: test_load_reduce.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_load_reduce_128_64_float16'}_______________________ test_load_reduce[128-64-float16] _______________________

BLOCK_M = 128, BLOCK_N = 64, dtype_str = 'float16'
request = <FixtureRequest for <Function test_load_reduce[128-64-float16]>>

    @pytest.mark.parametrize('BLOCK_M,BLOCK_N,dtype_str', [(128, 64, dtype_str) for dtype_str in ['float16']])
    def test_load_reduce(BLOCK_M, BLOCK_N, dtype_str, request):
        set_seed()
    
        dtype = dtype_mapping[dtype_str]
        x = torch.randn((BLOCK_M, BLOCK_N), device='cuda', dtype=dtype)
        y = torch.empty((BLOCK_M, ), device='cuda', dtype=dtype)
    
>       load_reduce_kernel[(1, )](x, y, x.stride(0), x.stride(1), y.stride(0), BLOCK_M, BLOCK_N)
E       NameError: name 'load_reduce_kernel' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_load_reduce_py_gen_triton_code_894444.py:61: NameError
=========================== short test summary info ============================
FAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_load_reduce_py_gen_triton_code_894444.py::test_load_reduce[128-64-float16] - NameError: name 'load_reduce_kernel' is not defined
================== 1 failed, 1 passed, 43 deselected in 8.39s ==================

Generated call accuracy: False
Execution accuracy: False
Match percentage: 0.00%
Error: Key mismatch at : Missing keys: {'test_load_reduce_128_64_float16'} 
2025-08-08_00-39-22 => File: naive_softmax.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/naive_softmax_py_gen_triton_code_389147_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/naive_softmax_py_gen_triton_code_389147.py _
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/naive_softmax_py_gen_triton_code_389147.py:175: in <module>
    @pytest.mark.parametrize('M, N', [(1823, 781), (1, 1), (128, 1), (1, 128), (8192, 8192), (4096, 8192), (359, 1),
E   NameError: name 'pytest' is not defined
2025-08-08_00-39-54 => File: test_add_kernel.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_add_98432_1024_float16', 'test_add_98432_1024_float32'}_________________________ test_add[98432-1024-float16] _________________________

SIZE = 98432, BLOCK_SIZE = 1024, dtype_str = 'float16'
request = <FixtureRequest for <Function test_add[98432-1024-float16]>>

    @pytest.mark.parametrize('SIZE,BLOCK_SIZE,dtype_str',
                             [(98432, 1024, dtype_str) for dtype_str in ['float16', 'float32']])
    def test_add(SIZE, BLOCK_SIZE, dtype_str, request):
        set_seed()
    
        dtype = dtype_mapping[dtype_str]
        output = torch.empty(SIZE, device='cuda', dtype=dtype)
        x = torch.randn(SIZE, device='cuda', dtype=dtype)
        y = torch.randn(SIZE, device='cuda', dtype=dtype)
    
        def grid(meta):
            return (triton.cdiv(SIZE, meta['BLOCK_SIZE']), )
    
>       add_kernel[grid](x, y, output, SIZE, BLOCK_SIZE=BLOCK_SIZE)
E       NameError: name 'add_kernel' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_add_kernel_py_gen_triton_code_777671.py:91: NameError
2025-08-08_00-40-30 => File: test_gemm_no_scf.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Key mismatch at : Missing keys: {'test_gemm_no_scf_128_128_16_1_4_False_True_float16_True', 'test_gemm_no_scf_64_16_16_1_4_False_True_float16_True', 'test_gemm_no_scf_64_64_64_1_4_False_True_float32_False', 'test_gemm_no_scf_32_16_16_1_4_False_True_float32_False', 'test_gemm_no_scf_64_64_16_1_4_False_True_float32_True', 'test_gemm_no_scf_32_16_16_1_4_False_True_float32_True', 'test_gemm_no_scf_16_16_16_1_4_False_True_float32_True', 'test_gemm_no_scf_64_16_16_1_4_False_True_float16_False', 'test_gemm_no_scf_16_16_16_1_4_False_True_float32_False', 'test_gemm_no_scf_128_128_16_1_4_False_True_float32_True', 'test_gemm_no_scf_32_32_16_1_4_False_True_float32_False', 'test_gemm_no_scf_16_32_16_1_4_False_True_float32_True', 'test_gemm_no_scf_16_32_16_1_4_False_True_float32_False', 'test_gemm_no_scf_64_32_16_1_4_False_True_float16_True', 'test_gemm_no_scf_64_64_64_1_4_False_True_float32_True', 'test_gemm_no_scf_64_64_16_1_4_False_True_float32_False', 'test_gemm_no_scf_64_64_16_1_4_False_True_float16_True', 'test_gemm_no_scf_64_64_32_1_4_False_True_float32_False', 'test_gemm_no_scf_64_64_16_1_4_False_True_float16_False', 'test_gemm_no_scf_64_64_32_1_4_False_True_float32_True', 'test_gemm_no_scf_32_32_16_1_4_False_True_float32_True', 'test_gemm_no_scf_128_128_16_1_4_False_True_float32_False', 'test_gemm_no_scf_128_128_16_1_4_False_True_float16_False', 'test_gemm_no_scf_64_32_16_1_4_False_True_float16_False'}____________ test_gemm_no_scf[64-16-16-1-4-False-True-float16-True] ____________

M = 64, N = 16, K = 16, NUM_CTAS = 1, NUM_WARPS = 4, TRANS_A = False
TRANS_B = True, OUTPUT_TYPE = 'float16', USE_TMA_EPILOGUE = True
request = <FixtureRequest for <Function test_gemm_no_scf[64-16-16-1-4-False-True-float16-True]>>

    @pytest.mark.parametrize(
        'M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_TYPE,USE_TMA_EPILOGUE',
        itertools.chain(*[[
            # numCTAs = 1, no TMA multicast:
            [64, 16, 16, 1, 4, False, True, "float16", USE_TMA_EPILOGUE],
            [64, 32, 16, 1, 4, False, True, "float16", USE_TMA_EPILOGUE],
            [64, 64, 16, 1, 4, False, True, "float16", USE_TMA_EPILOGUE],
            [64, 64, 16, 1, 4, False, True, "float32", USE_TMA_EPILOGUE],
            [64, 64, 32, 1, 4, False, True, "float32", USE_TMA_EPILOGUE],
            [64, 64, 64, 1, 4, False, True, "float32", USE_TMA_EPILOGUE],
            [128, 128, 16, 1, 4, False, True, "float16", USE_TMA_EPILOGUE],
            [128, 128, 16, 1, 4, False, True, "float32", USE_TMA_EPILOGUE],
            # static mask, cluster 4x1
            [256, 64, 16, 4, 4, False, True, "float16", USE_TMA_EPILOGUE],
            [256, 64, 16, 4, 4, False, True, "float32", USE_TMA_EPILOGUE],
            # dynamic mask, cluster 2x2
            [128, 128, 16, 4, 4, False, True, "float16", USE_TMA_EPILOGUE],
            [128, 128, 16, 4, 4, False, True, "float32", USE_TMA_EPILOGUE],
            # small M, N
            [16, 16, 16, 1, 4, False, True, "float32", USE_TMA_EPILOGUE],
            [16, 32, 16, 1, 4, False, True, "float32", USE_TMA_EPILOGUE],
            [32, 16, 16, 1, 4, False, True, "float32", USE_TMA_EPILOGUE],
            [32, 32, 16, 1, 4, False, True, "float32", USE_TMA_EPILOGUE],
        ] for USE_TMA_EPILOGUE in [True, False]]))
    @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason="Requires compute capability >= 9")
    def test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, request):
        set_seed()
    
        if is_hip() and NUM_CTAS > 1:
            pytest.skip("NUM_CTAS > 1 is not supported in HIP backend")
    
        if (TRANS_A):
            a = torch.randn((K, M), device='cuda', dtype=torch.float16).T
        else:
            a = torch.randn((M, K), device='cuda', dtype=torch.float16)
        if (TRANS_B):
            b = torch.randn((N, K), device='cuda', dtype=torch.float16).T
        else:
            b = torch.randn((K, N), device='cuda', dtype=torch.float16)
    
        if OUTPUT_TYPE == "float16":
            c = torch.empty((M, N), device=a.device, dtype=torch.float16)
        else:
            c = torch.empty((M, N), device=a.device, dtype=torch.float32)
    
>       matmul_no_scf_kernel[(1, 1)](
            a_ptr=a, b_ptr=b, c_ptr=c,  #
            M=M, N=N, K=K,  #
            stride_am=a.stride(0), stride_ak=a.stride(1),  #
            stride_bk=b.stride(0), stride_bn=b.stride(1),  #
            stride_cm=c.stride(0), stride_cn=c.stride(1),  #
            BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,  #
            num_warps=NUM_WARPS,  #
            num_ctas=NUM_CTAS,  #
            FLOAT16_OUTPUT=(OUTPUT_TYPE == "float16"),  #
            USE_TMA_EPILOGUE=USE_TMA_EPILOGUE)
E       NameError: name 'matmul_no_scf_kernel' is not defined

sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_gemm_no_scf_py_gen_triton_code_921302.py:101: NameError
2025-08-08_00-41-09 => File: test_flashattention_fwd.py, Call Status: False, Exec Status: False, difficulty: -1, stderr: Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_578436_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_578436.py _
sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_578436.py:244: in <module>
    triton.testing.Benchmark(
E   NameError: name 'triton' is not defined
2025-08-08_00-41-09 => File: sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0.json, Call Accuracy: 0.0967741935483871, Exec Accuracy: 0.0
