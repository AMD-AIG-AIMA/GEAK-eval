[
    {
        "pass_num": 0,
        "file_name": "moe_gemm.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "None",
        "stderr": "Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/moe_gemm_py_gen_triton_code_606927_py.pt'",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "multreduce_matmul_dot_kernel.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": null,
        "stderr": "Command '['python3 /root/sapmajum/dev/TB-eval/geak_eval/evaluators/ROCm_correctness.py --gen_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/multreduce_matmul_dot_kernel_py_gen_triton_code_161864.py --ref_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/multreduce_matmul_dot_kernel_py_ref_triton_code_983761.py --atol 0.01 --rtol 0.01 --global_timeout 2400 --verbose']' timed out after 2400 seconds",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "triton_multreduce_matmul_kernel.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": null,
        "stderr": "Command '['python3 /root/sapmajum/dev/TB-eval/geak_eval/evaluators/ROCm_correctness.py --gen_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/triton_multreduce_matmul_kernel_py_gen_triton_code_293827.py --ref_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/triton_multreduce_matmul_kernel_py_ref_triton_code_746250.py --atol 0.01 --rtol 0.01 --global_timeout 2400 --verbose']' timed out after 2400 seconds",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "gemm.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": null,
        "stderr": "Command '['python3 /root/sapmajum/dev/TB-eval/geak_eval/evaluators/ROCm_correctness.py --gen_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/gemm_py_gen_triton_code_951491.py --ref_file /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/gemm_py_ref_triton_code_708096.py --atol 0.01 --rtol 0.01 --global_timeout 2400 --verbose']' timed out after 2400 seconds",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "layernorm.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "None",
        "stderr": "Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/layernorm_py_gen_triton_code_762339_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/layernorm_py_gen_triton_code_762339.py _\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/layernorm_py_gen_triton_code_762339.py:42: in <module>\n    @triton.jit\nE   NameError: name 'triton' is not defined",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "softmax.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "None",
        "stderr": "Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/softmax_py_gen_triton_code_201472_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/softmax_py_gen_triton_code_201472.py _\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/softmax_py_gen_triton_code_201472.py:177: in <module>\n    @pytest.mark.parametrize('M, N', [(1823, 781), (1, 1), (128, 1), (1, 128), (8192, 8192), (4096, 8192), (359, 1),\nE   NameError: name 'pytest' is not defined",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_chained_dot_fp8.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "None",
        "stderr": "Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_dot_fp8_py_gen_triton_code_941445_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_dot_fp8_py_gen_triton_code_941445.py _\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_dot_fp8_py_gen_triton_code_941445.py:13: in <module>\n    float8: tl.constexpr = None if not TORCH_HAS_FP8E4 else torch.float8_e4m3fnuz\nE   NameError: name 'tl' is not defined",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_cast_matmul.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "None",
        "stderr": "Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_cast_matmul_py_gen_triton_code_521743_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_cast_matmul_py_gen_triton_code_521743.py _\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_cast_matmul_py_gen_triton_code_521743.py:41: in <module>\n    @pytest.mark.parametrize(\"M, K, N, w_dtype, x_dtype, out_dtype\",\nE   NameError: name 'pytest' is not defined",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_gemm_fusion.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "None",
        "stderr": "Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_gemm_fusion_py_gen_triton_code_111572_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_gemm_fusion_py_gen_triton_code_111572.py _\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_gemm_fusion_py_gen_triton_code_111572.py:41: in <module>\n    @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"not passed on ampere\")\nE   NameError: name 'pytest' is not defined",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_chained_matmul.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_chained_matmul'}_____________________________ test_chained_matmul ______________________________\n\nrequest = <FixtureRequest for <Function test_chained_matmul>>, device = 'cuda'\n\n    def test_chained_matmul(request, device='cuda'):\n        # Regression test for issue #1601\n        set_seed()\n    \n    \n        m, n, k = 32, 64, 128\n        block_m, block_n, block_k = 16, 32, k\n    \n>       grid = (triton.cdiv(m, block_m), )\nE       NameError: name 'triton' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_matmul_py_gen_triton_code_381351.py:56: NameError\n=========================== short test summary info ============================\nFAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_chained_matmul_py_gen_triton_code_381351.py::test_chained_matmul - NameError: name 'triton' is not defined\n================= 1 failed, 1 passed, 151 deselected in 8.30s ==================\n\nGenerated call accuracy: False\nExecution accuracy: False\nMatch percentage: 0.00%\nError: Key mismatch at : Missing keys: {'test_chained_matmul'} ",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_batched_vecmat.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_vecmat'}_________________________________ test_vecmat __________________________________\n\nrequest = <FixtureRequest for <Function test_vecmat>>, device = 'cuda'\n\n    def test_vecmat(request, device='cuda'):\n        \"\"\"\n        Test the batched vector-matrix multiplication kernel.\n    \n        Args:\n            device: The device (e.g., 'cuda' or 'cpu') on which the test is executed.\n            request: Pytest request object used to retrieve the test case name.\n        \"\"\"\n        set_seed()\n    \n        M, N, K = 128, 128, 128\n        block_m, block_n, block_k = 16, 32, 64\n    \n        rs = RandomState(17)\n        A_vec = rs.randint(0, 4, (M, K)).astype('float32')\n        B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')\n        A = A_vec\n        B = B_vec\n    \n        A_tri = torch.tensor(A, device=device)\n        B_tri = torch.tensor(B, device=device)\n        C_tri = torch.zeros((M, N), dtype=torch.float32, device=device)\n    \n        grid = (M // block_m, N // block_n)\n    \n>       batched_vecmat[grid](\n            A_tri, B_tri, M, N, K, C_tri,  #\n            block_m=block_m, block_n=block_n, block_k=block_k,  #\n            num_warps=4, num_stages=1)\nE       NameError: name 'batched_vecmat' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_batched_vecmat_py_gen_triton_code_523292.py:135: NameError\n=========================== short test summary info ============================\nFAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_batched_vecmat_py_gen_triton_code_523292.py::test_vecmat - NameError: name 'batched_vecmat' is not defined\n================== 1 failed, 1 passed, 31 deselected in 8.59s ==================\n\nGenerated call accuracy: False\nExecution accuracy: False\nMatch percentage: 0.00%\nError: Key mismatch at : Missing keys: {'test_vecmat'} ",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_iv_dependent_matmul.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_iv_dependent_matmul_post_pre_mixed', 'test_iv_dependent_matmul_post_load', 'test_iv_dependent_matmul_post_load_three_iters', 'test_iv_dependent_matmul_pre_load', 'test_iv_dependent_matmul_post_load_two_iters'}______________________ test_iv_dependent_matmul[pre_load] ______________________\n\ntype = 'pre_load'\nrequest = <FixtureRequest for <Function test_iv_dependent_matmul[pre_load]>>\ndevice = 'cuda'\n\n    @pytest.mark.parametrize(\"type\",\n                             [\"pre_load\", \"post_load\", \"post_pre_mixed\", \"post_load_two_iters\", \"post_load_three_iters\"])\n    def test_iv_dependent_matmul(type, request, device='cuda'):\n    \n    \n        set_seed()\n    \n        M = 256\n        K = 256\n        N = 256\n        BLOCK_SIZE_K = 32\n        BLOCK_SIZE_N = 32\n        BLOCK_SIZE_M = 32\n    \n        a = torch.rand((M, K), device=device)\n        b = torch.rand((K, N), device=device)\n    \n        torch_output = torch.mm(a, b)\n        triton_output = torch.empty_like(torch_output, device=torch_output.device)\n    \n        def grid(META):\n            return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n    \n        num_stages = 4 if type == \"post_load_three_iters\" else 3\n>       iv_dependent_matmul[grid](\n            a, b, triton_output, M, N, K,  #\n            a.stride(0), a.stride(1), b.stride(0), b.stride(1),  #\n            triton_output.stride(0), triton_output.stride(1),  #\n            BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K, type=type,  #\n            num_stages=num_stages)\nE       NameError: name 'iv_dependent_matmul' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_iv_dependent_matmul_py_gen_triton_code_405615.py:69: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_reverse_range.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_reverse_range'}______________________________ test_reverse_range ______________________________\n\nrequest = <FixtureRequest for <Function test_reverse_range>>, device = 'cuda'\n\n    def test_reverse_range(request, device='cuda'):\n        set_seed()\n    \n    \n        data = torch.randn((516, ), dtype=torch.float32, device=device)\n        res = torch.empty((512, ), dtype=torch.float32, device=device)\n>       reverse_range[(1, )](data, res)\nE       NameError: name 'reverse_range' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_reverse_range_py_gen_triton_code_733446.py:50: NameError\n=========================== short test summary info ============================\nFAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_reverse_range_py_gen_triton_code_733446.py::test_reverse_range - NameError: name 'reverse_range' is not defined\n================== 1 failed, 1 passed, 10 deselected in 8.65s ==================\n\nGenerated call accuracy: False\nExecution accuracy: False\nMatch percentage: 0.00%\nError: Key mismatch at : Missing keys: {'test_reverse_range'} ",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "rmsnorm_fwd.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_rmsnorm_4096_8192_False_bf16_bf16_y_triton', 'test_rmsnorm_256_4096_True_fp16_bf16_rsigma', 'test_rmsnorm_1_4_False_fp16_bf16_y_triton', 'test_rmsnorm_2_10_True_fp16_bf16_rsigma', 'test_rmsnorm_2_10_False_bf16_fp16_y_triton', 'test_rmsnorm_8192_65536_True_bf16_bf16_rsigma', 'test_rmsnorm_256_4096_False_fp16_bf16_y_triton', 'test_rmsnorm_1_4_False_bf16_fp16_y_triton', 'test_rmsnorm_256_4096_False_bf16_fp16_rsigma', 'test_rmsnorm_1_4_True_bf16_fp16_rsigma', 'test_rmsnorm_1_4_True_fp16_fp16_rsigma', 'test_rmsnorm_8192_65536_False_fp16_bf16_rsigma', 'test_rmsnorm_1_31744_True_fp16_fp16_y_triton', 'test_rmsnorm_1_31744_False_bf16_bf16_y_triton', 'test_rmsnorm_1_4_True_fp16_bf16_y_triton', 'test_rmsnorm_256_4096_True_bf16_bf16_rsigma', 'test_rmsnorm_1_31744_False_fp16_bf16_y_triton', 'test_rmsnorm_4096_8192_True_fp16_fp16_rsigma', 'test_rmsnorm_4096_8192_True_fp16_bf16_y_triton', 'test_rmsnorm_256_4096_True_bf16_fp16_y_triton', 'test_rmsnorm_8192_65536_False_bf16_fp16_rsigma', 'test_rmsnorm_4096_8192_True_bf16_fp16_rsigma', 'test_rmsnorm_256_4096_True_fp16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_fp16_fp16_y_triton', 'test_rmsnorm_4096_8192_False_bf16_fp16_y_triton', 'test_rmsnorm_1_4_True_bf16_bf16_rsigma', 'test_rmsnorm_8192_65536_True_bf16_fp16_rsigma', 'test_rmsnorm_4096_8192_False_fp16_bf16_rsigma', 'test_rmsnorm_1_31744_True_bf16_fp16_y_triton', 'test_rmsnorm_1_31744_True_fp16_fp16_rsigma', 'test_rmsnorm_1_31744_True_bf16_fp16_rsigma', 'test_rmsnorm_8192_65536_True_fp16_fp16_y_triton', 'test_rmsnorm_2_10_False_bf16_fp16_rsigma', 'test_rmsnorm_8192_65536_True_bf16_bf16_y_triton', 'test_rmsnorm_1_31744_False_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_True_fp16_fp16_rsigma', 'test_rmsnorm_873_1245_False_fp16_fp16_rsigma', 'test_rmsnorm_873_1245_False_bf16_bf16_y_triton', 'test_rmsnorm_1_4_False_fp16_fp16_rsigma', 'test_rmsnorm_256_4096_True_fp16_fp16_rsigma', 'test_rmsnorm_256_4096_True_bf16_bf16_y_triton', 'test_rmsnorm_873_1245_True_bf16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_fp16_bf16_y_triton', 'test_rmsnorm_2_10_False_bf16_bf16_y_triton', 'test_rmsnorm_4096_8192_True_bf16_bf16_rsigma', 'test_rmsnorm_4096_8192_True_bf16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_bf16_bf16_rsigma', 'test_rmsnorm_256_4096_False_fp16_fp16_y_triton', 'test_rmsnorm_1_31744_True_bf16_bf16_rsigma', 'test_rmsnorm_1_4_False_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_False_fp16_bf16_y_triton', 'test_rmsnorm_256_4096_False_bf16_bf16_rsigma', 'test_rmsnorm_873_1245_True_bf16_fp16_rsigma', 'test_rmsnorm_256_4096_True_bf16_fp16_rsigma', 'test_rmsnorm_1_31744_True_fp16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_fp16_fp16_rsigma', 'test_rmsnorm_4096_8192_True_fp16_bf16_rsigma', 'test_rmsnorm_1_31744_True_fp16_bf16_rsigma', 'test_rmsnorm_1_4_True_fp16_bf16_rsigma', 'test_rmsnorm_1_31744_False_fp16_fp16_rsigma', 'test_rmsnorm_873_1245_True_fp16_bf16_y_triton', 'test_rmsnorm_4096_8192_False_fp16_bf16_y_triton', 'test_rmsnorm_4096_8192_False_bf16_bf16_rsigma', 'test_rmsnorm_1_4_False_fp16_bf16_rsigma', 'test_rmsnorm_2_10_True_fp16_fp16_rsigma', 'test_rmsnorm_873_1245_True_fp16_bf16_rsigma', 'test_rmsnorm_256_4096_False_bf16_bf16_y_triton', 'test_rmsnorm_4096_8192_True_bf16_fp16_y_triton', 'test_rmsnorm_2_10_False_fp16_fp16_rsigma', 'test_rmsnorm_8192_65536_True_fp16_bf16_y_triton', 'test_rmsnorm_8192_65536_False_bf16_fp16_y_triton', 'test_rmsnorm_8192_65536_True_fp16_bf16_rsigma', 'test_rmsnorm_256_4096_False_fp16_bf16_rsigma', 'test_rmsnorm_1_4_False_bf16_bf16_rsigma', 'test_rmsnorm_873_1245_True_bf16_bf16_rsigma', 'test_rmsnorm_1_31744_False_bf16_fp16_y_triton', 'test_rmsnorm_8192_65536_False_bf16_bf16_y_triton', 'test_rmsnorm_1_4_True_bf16_fp16_y_triton', 'test_rmsnorm_2_10_True_bf16_bf16_rsigma', 'test_rmsnorm_2_10_False_fp16_bf16_rsigma', 'test_rmsnorm_8192_65536_True_bf16_fp16_y_triton', 'test_rmsnorm_2_10_True_bf16_bf16_y_triton', 'test_rmsnorm_1_4_False_bf16_fp16_rsigma', 'test_rmsnorm_2_10_True_bf16_fp16_rsigma', 'test_rmsnorm_2_10_True_fp16_bf16_y_triton', 'test_rmsnorm_873_1245_True_bf16_fp16_y_triton', 'test_rmsnorm_873_1245_False_fp16_bf16_rsigma', 'test_rmsnorm_1_4_False_bf16_bf16_y_triton', 'test_rmsnorm_2_10_True_fp16_fp16_y_triton', 'test_rmsnorm_4096_8192_False_fp16_fp16_rsigma', 'test_rmsnorm_2_10_False_bf16_bf16_rsigma', 'test_rmsnorm_4096_8192_False_fp16_fp16_y_triton', 'test_rmsnorm_2_10_False_fp16_bf16_y_triton', 'test_rmsnorm_873_1245_True_fp16_fp16_y_triton', 'test_rmsnorm_2_10_True_bf16_fp16_y_triton', 'test_rmsnorm_873_1245_False_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_False_bf16_fp16_rsigma', 'test_rmsnorm_1_31744_False_bf16_bf16_rsigma', 'test_rmsnorm_1_4_True_bf16_bf16_y_triton', 'test_rmsnorm_256_4096_False_bf16_fp16_y_triton', 'test_rmsnorm_4096_8192_True_fp16_fp16_y_triton', 'test_rmsnorm_2_10_False_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_False_bf16_fp16_y_triton', 'test_rmsnorm_1_4_True_fp16_fp16_y_triton', 'test_rmsnorm_873_1245_False_bf16_bf16_rsigma', 'test_rmsnorm_4096_8192_False_bf16_fp16_rsigma', 'test_rmsnorm_8192_65536_True_fp16_fp16_rsigma', 'test_rmsnorm_1_31744_True_bf16_bf16_y_triton', 'test_rmsnorm_1_31744_False_fp16_bf16_rsigma', 'test_rmsnorm_256_4096_False_fp16_fp16_rsigma', 'test_rmsnorm_256_4096_True_fp16_fp16_y_triton', 'test_rmsnorm_1_31744_False_bf16_fp16_rsigma'}_______________________ test_rmsnorm[1-4-True-fp16-fp16] _______________________\n\nM = 1, N = 4, ZERO_CENTERED_GAMMA = True, in_dtype_str = 'fp16'\nout_dtype_str = 'fp16'\nrequest = <FixtureRequest for <Function test_rmsnorm[1-4-True-fp16-fp16]>>\n\n    @pytest.mark.parametrize(\"in_dtype_str\", [\"fp16\", \"bf16\"])\n    @pytest.mark.parametrize(\"out_dtype_str\", [\"fp16\", \"bf16\"])\n    @pytest.mark.parametrize('ZERO_CENTERED_GAMMA', [True, False])\n    @pytest.mark.parametrize('M, N', [\n        (1, 4),\n        (2, 10),\n        (256, 4096),\n        (4096, 8192),\n        (1, 31744),\n        (8192, 65536),\n        (873, 1245),\n    ])\n    def test_rmsnorm(M, N, ZERO_CENTERED_GAMMA, in_dtype_str, out_dtype_str, request):\n        in_dtype = arg_to_torch_dtype[in_dtype_str]\n        out_dtype = arg_to_torch_dtype[out_dtype_str]\n        set_seed()\n    \n        x = torch.randn(M, N, device='cuda', dtype=in_dtype, requires_grad=True)\n        g = torch.ones((1, N), device='cuda', dtype=in_dtype, requires_grad=True)\n        y = torch.zeros_like(x, device='cuda', dtype=out_dtype)\n        rsigma = torch.empty((M, ), device=x.device, dtype=torch.float32)\n    \n        dx = torch.empty_like(x, dtype=in_dtype, requires_grad=False)\n        dg = torch.empty_like(g, dtype=in_dtype, requires_grad=False)\n        dg_tmp = torch.zeros(M, N, device='cuda', dtype=torch.float32, requires_grad=False)\n    \n        n_rows, n_cols = x.shape\n        MAX_FUSED_SIZE = 65536 // x.element_size()\n        blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))\n        USE_BLOCKED = n_cols > blk_size\n>       NUM_PRGMS = min(n_rows, get_num_sms())\nE       NameError: name 'get_num_sms' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/rmsnorm_fwd_py_gen_triton_code_11471.py:229: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "rmsnorm_bwd.py",
        "call_status": 1,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_rmsnorm_2_10_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_873_1245_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_1_4_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_1_31744_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_873_1245_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_2_10_True_fp16_bf16_grad_x_triton', 'test_rmsnorm_1_4_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_2_10_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_2_10_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_256_4096_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_873_1245_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_2_10_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_873_1245_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_2_10_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_873_1245_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_4_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_True_fp16_bf16_grad_x_triton', 'test_rmsnorm_2_10_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_2_10_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_1_4_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_1_4_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_1_31744_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_4_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_1_4_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_2_10_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_256_4096_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_31744_True_fp16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_2_10_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_1_31744_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_True_bf16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_873_1245_True_bf16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_2_10_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_1_31744_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_873_1245_True_bf16_fp16_grad_g_triton', 'test_rmsnorm_2_10_False_bf16_fp16_grad_g_triton', 'test_rmsnorm_873_1245_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_256_4096_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_256_4096_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_2_10_False_fp16_bf16_grad_g_triton', 'test_rmsnorm_1_4_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_256_4096_True_fp16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_False_fp16_fp16_grad_g_triton', 'test_rmsnorm_2_10_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_1_4_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_True_bf16_bf16_grad_g_triton', 'test_rmsnorm_2_10_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_False_bf16_fp16_grad_x_triton', 'test_rmsnorm_1_31744_False_bf16_bf16_grad_g_triton', 'test_rmsnorm_1_31744_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_873_1245_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_2_10_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_873_1245_False_fp16_fp16_grad_x_triton', 'test_rmsnorm_1_4_False_bf16_bf16_grad_x_triton', 'test_rmsnorm_256_4096_True_fp16_fp16_grad_x_triton', 'test_rmsnorm_873_1245_False_fp16_bf16_grad_x_triton', 'test_rmsnorm_1_31744_True_fp16_fp16_grad_g_triton', 'test_rmsnorm_1_4_True_fp16_bf16_grad_g_triton', 'test_rmsnorm_873_1245_True_fp16_bf16_grad_x_triton'}_______________________ test_rmsnorm[1-4-True-fp16-fp16] _______________________\n\nM = 1, N = 4, ZERO_CENTERED_GAMMA = True, in_dtype_str = 'fp16'\nout_dtype_str = 'fp16'\nrequest = <FixtureRequest for <Function test_rmsnorm[1-4-True-fp16-fp16]>>\n\n    @pytest.mark.parametrize(\"in_dtype_str\", [\"fp16\", \"bf16\"])\n    @pytest.mark.parametrize(\"out_dtype_str\", [\"fp16\", \"bf16\"])\n    @pytest.mark.parametrize('ZERO_CENTERED_GAMMA', [True, False])\n    @pytest.mark.parametrize('M, N', [\n        (1, 4),\n        (2, 10),\n        (256, 4096),\n        (1, 31744),\n        (873, 1245),\n    ])\n    def test_rmsnorm(M, N, ZERO_CENTERED_GAMMA, in_dtype_str, out_dtype_str, request):\n        in_dtype = arg_to_torch_dtype[in_dtype_str]\n        out_dtype = arg_to_torch_dtype[out_dtype_str]\n        set_seed()\n    \n        x = torch.randn(M, N, device='cuda', dtype=in_dtype, requires_grad=True)\n        g = torch.ones((1, N), device='cuda', dtype=in_dtype, requires_grad=True)\n        y = torch.zeros_like(x, device='cuda', dtype=out_dtype)\n        rsigma = torch.empty((M, ), device=x.device, dtype=torch.float32)\n    \n        dx = torch.empty_like(x, dtype=in_dtype, requires_grad=False)\n        dg = torch.empty_like(g, dtype=in_dtype, requires_grad=False)\n        dg_tmp = torch.zeros(M, N, device='cuda', dtype=torch.float32, requires_grad=False)\n    \n        n_rows, n_cols = x.shape\n        MAX_FUSED_SIZE = 65536 // x.element_size()\n        blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))\n        USE_BLOCKED = n_cols > blk_size\n        NUM_PRGMS = min(n_rows, get_num_sms())\n    \n        y_triton = rmsnorm(x, g, y, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED,\n                           NUM_PRGMS)\n    \n        y_torch, rsigma_torch = torch_rmsnorm_fwd(x, g, ZERO_CENTERED_GAMMA, out_dtype)\n    \n        if out_dtype in (torch.float16, torch.bfloat16):\n            atol, rtol = 1e-3, 1e-2\n        else:\n            # float32 typically can be tighter\n            atol, rtol = 1e-5, 1e-5\n    \n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n        assert y_triton.dtype == out_dtype, f\"y_triton has dtype={y_triton.dtype}, expected {out_dtype}\"\n        assert y_torch.dtype == out_dtype, f\"y_torch has dtype={y_torch.dtype}, expected {out_dtype}\"\n    \n        assert torch.allclose(y_triton, y_torch, atol=atol, rtol=rtol), \\\n            f\"Mismatch in 'y' (in={in_dtype_str}, out={out_dtype_str})\"\n        assert torch.allclose(rsigma, rsigma_torch, atol=atol, rtol=rtol), \\\n            f\"Mismatch in 'rsigma' (in={in_dtype_str}, out={out_dtype_str})\"\n    \n        grad_output = torch.randn_like(y_torch)\n    \n        # 1) PyTorch reference backward\n        # We must clone and set requires_grad = True for backward\n        x_ref = x.clone().detach().requires_grad_()\n        g_ref = g.clone().detach().requires_grad_()\n        y_ref, rsigma_ref = torch_rmsnorm_fwd(x_ref, g_ref, ZERO_CENTERED_GAMMA, out_dtype)\n    \n        # Backpropagate through PyTorch\n        y_ref.backward(grad_output)\n        grad_x_ref = x_ref.grad.to(out_dtype)\n        grad_g_ref = g_ref.grad.to(out_dtype)\n    \n        # 2) Triton backward\n        x_triton = x.clone().detach().requires_grad_()\n        g_triton = g.clone().detach().requires_grad_()\n    \n        y_triton_buf = torch.empty_like(x_triton, dtype=out_dtype)\n        rsigma_triton = torch.empty((M, ), device=x_triton.device, dtype=torch.float32)\n    \n        dx_b = torch.empty_like(x_triton, dtype=in_dtype, requires_grad=False)\n        dg_b = torch.empty_like(g_triton, dtype=in_dtype, requires_grad=False)\n        dg_tmp_b = torch.zeros(M, N, device=x_triton.device, dtype=torch.float32, requires_grad=False)\n    \n        # Run Triton forward pass to build the graph for backward.\n        y_triton = rmsnorm(x_triton, g_triton, y_triton_buf, rsigma_triton, dx_b, dg_b, dg_tmp_b, n_rows, n_cols,\n                           ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED, NUM_PRGMS)\n>       y_triton.backward(grad_output, retain_graph=True)\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/rmsnorm_bwd_py_gen_triton_code_880259.py:457: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/_tensor.py:626: in backward\n    torch.autograd.backward(\n/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:347: in backward\n    _engine_run_backward(\n/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/autograd/graph.py:823: in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/autograd/function.py:307: in apply\n    return user_fn(self, *args)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nctx = <torch.autograd.function.RMSNormBackward object at 0x727af4f67460>\ngrad_output = tensor([[ 0.1392, -0.1082, -0.7173,  0.7568]], device='cuda:0',\n       dtype=torch.float16)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, g, rsigma = ctx.saved_tensors\n        dg_tmp = ctx.dg_tmp\n        dx = ctx.dx\n        dg = ctx.dg\n        n_rows = ctx.n_rows\n        n_cols = ctx.n_cols\n        ZERO_CENTERED_GAMMA = ctx.ZERO_CENTERED_GAMMA\n        blk_size = ctx.blk_size\n        USE_BLOCKED = ctx.USE_BLOCKED\n        NUM_PRGMS = ctx.NUM_PRGMS\n    \n        grid_bwd = lambda meta: (NUM_PRGMS, )\n>       rms_bwd_kernel[grid_bwd](grad_output, x, g, rsigma, dx, dg_tmp, x.stride(0), grad_output.stride(0), n_rows,\n                                 n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED, NUM_PRGMS, num_warps=ctx.num_warps)\nE       NameError: name 'rms_bwd_kernel' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/rmsnorm_bwd_py_gen_triton_code_880259.py:344: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_block_pointer_matmul.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_block_ptr_matmul_no_scf_shape5_8', 'test_block_ptr_matmul_no_scf_shape4_4', 'test_block_ptr_matmul_no_scf_shape2_4', 'test_block_ptr_matmul_no_scf_shape0_4', 'test_block_ptr_matmul_no_scf_shape3_8', 'test_block_ptr_matmul_no_scf_shape1_8'}____________________ test_block_ptr_matmul_no_scf[shape0-4] ____________________\n\nshape = [64, 64, 16], num_warps = 4\nrequest = <FixtureRequest for <Function test_block_ptr_matmul_no_scf[shape0-4]>>\ndevice = 'cuda'\n\n    @pytest.mark.interpreter\n    @pytest.mark.parametrize(\"shape, num_warps\", [  #\n        (shape, num_warps) for shape in [\n            [64, 64, 16],\n            [64, 64, 32],\n            [64, 64, 64],\n        ] for num_warps in [4, 8]\n    ])\n    def test_block_ptr_matmul_no_scf(shape, num_warps, request, device='cuda'):\n        set_seed()\n    \n        m, n, k = shape\n        a = torch.randn((m, k), device=device, dtype=torch.float16)\n        b = torch.randn((k, n), device=device, dtype=torch.float16)\n        c = torch.empty((m, n), device=device, dtype=torch.float32)\n    \n        grid = lambda META: (1, )\n>       matmul_no_scf_with_advance_kernel[grid](\n            a_ptr=a, b_ptr=b, c_ptr=c,  #\n            M=m, N=n, K=k,  #\n            stride_am=a.stride(0), stride_ak=a.stride(1),  #\n            stride_bk=b.stride(0), stride_bn=b.stride(1),  #\n            stride_cm=c.stride(0), stride_cn=c.stride(1),  #\n            BLOCK_M=m, BLOCK_N=n, BLOCK_K=k,  #\n            num_warps=num_warps)\nE       NameError: name 'matmul_no_scf_with_advance_kernel' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_block_pointer_matmul_py_gen_triton_code_457622.py:60: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_block_copy.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_block_copy_dtypes_str74_1024_nan', 'test_block_copy_dtypes_str69_512_None', 'test_block_copy_dtypes_str40_512_zero', 'test_block_copy_dtypes_str13_1024_zero', 'test_block_copy_dtypes_str22_256_zero', 'test_block_copy_dtypes_str50_128_nan', 'test_block_copy_dtypes_str72_1024_None', 'test_block_copy_dtypes_str6_256_None', 'test_block_copy_dtypes_str61_64_zero', 'test_block_copy_dtypes_str19_128_zero', 'test_block_copy_dtypes_str0_64_None', 'test_block_copy_dtypes_str64_128_zero', 'test_block_copy_dtypes_str53_256_nan', 'test_block_copy_dtypes_str48_128_None', 'test_block_copy_dtypes_str79_128_zero', 'test_block_copy_dtypes_str49_128_zero', 'test_block_copy_dtypes_str82_256_zero', 'test_block_copy_dtypes_str68_256_nan', 'test_block_copy_dtypes_str62_64_nan', 'test_block_copy_dtypes_str54_512_None', 'test_block_copy_dtypes_str70_512_zero', 'test_block_copy_dtypes_str39_512_None', 'test_block_copy_dtypes_str56_512_nan', 'test_block_copy_dtypes_str71_512_nan', 'test_block_copy_dtypes_str84_512_None', 'test_block_copy_dtypes_str43_1024_zero', 'test_block_copy_dtypes_str52_256_zero', 'test_block_copy_dtypes_str89_1024_nan', 'test_block_copy_dtypes_str45_64_None', 'test_block_copy_dtypes_str88_1024_zero', 'test_block_copy_dtypes_str24_512_None', 'test_block_copy_dtypes_str86_512_nan', 'test_block_copy_dtypes_str55_512_zero', 'test_block_copy_dtypes_str66_256_None', 'test_block_copy_dtypes_str27_1024_None', 'test_block_copy_dtypes_str76_64_zero', 'test_block_copy_dtypes_str25_512_zero', 'test_block_copy_dtypes_str80_128_nan', 'test_block_copy_dtypes_str18_128_None', 'test_block_copy_dtypes_str31_64_zero', 'test_block_copy_dtypes_str30_64_None', 'test_block_copy_dtypes_str58_1024_zero', 'test_block_copy_dtypes_str77_64_nan', 'test_block_copy_dtypes_str12_1024_None', 'test_block_copy_dtypes_str85_512_zero', 'test_block_copy_dtypes_str34_128_zero', 'test_block_copy_dtypes_str75_64_None', 'test_block_copy_dtypes_str81_256_None', 'test_block_copy_dtypes_str87_1024_None', 'test_block_copy_dtypes_str57_1024_None', 'test_block_copy_dtypes_str21_256_None', 'test_block_copy_dtypes_str47_64_nan', 'test_block_copy_dtypes_str9_512_None', 'test_block_copy_dtypes_str15_64_None', 'test_block_copy_dtypes_str28_1024_zero', 'test_block_copy_dtypes_str73_1024_zero', 'test_block_copy_dtypes_str33_128_None', 'test_block_copy_dtypes_str67_256_zero', 'test_block_copy_dtypes_str1_64_zero', 'test_block_copy_dtypes_str78_128_None', 'test_block_copy_dtypes_str51_256_None', 'test_block_copy_dtypes_str10_512_zero', 'test_block_copy_dtypes_str3_128_None', 'test_block_copy_dtypes_str36_256_None', 'test_block_copy_dtypes_str37_256_zero', 'test_block_copy_dtypes_str7_256_zero', 'test_block_copy_dtypes_str63_128_None', 'test_block_copy_dtypes_str65_128_nan', 'test_block_copy_dtypes_str59_1024_nan', 'test_block_copy_dtypes_str16_64_zero', 'test_block_copy_dtypes_str83_256_nan', 'test_block_copy_dtypes_str42_1024_None', 'test_block_copy_dtypes_str60_64_None', 'test_block_copy_dtypes_str46_64_zero', 'test_block_copy_dtypes_str4_128_zero'}_____________________ test_block_copy[dtypes_str0-64-None] _____________________\n\ndtypes_str = ('bool', 'bool'), n = 64, padding_option = None\nrequest = <FixtureRequest for <Function test_block_copy[dtypes_str0-64-None]>>\ndevice = 'cuda'\n\n    @pytest.mark.interpreter\n    @pytest.mark.parametrize(\"dtypes_str, n, padding_option\", [  #\n        (dtypes_str, n, padding)\n        for dtypes_str in ((\"bool\", \"bool\"), (\"int16\", \"int16\"), (\"int32\", \"int32\"), (\"float16\", \"float16\"),\n                           (\"float32\", \"float32\"), (\"bfloat16\", \"bfloat16\"))\n        for n in (64, 128, 256, 512, 1024)\n        for padding in (None, \"zero\", \"nan\")  #\n    ])\n    def test_block_copy(dtypes_str, n, padding_option, request, device='cuda'):\n        src_dtype_str = dtypes_str[0]\n        dst_dtype_str = dtypes_str[1]\n        src_dtype = getattr(torch, src_dtype_str)\n        dst_dtype = getattr(torch, dst_dtype_str)\n        check_type_supported(src_dtype, device)\n        check_type_supported(dst_dtype, device)\n        if src_dtype_str in (\"bool\", \"int16\", \"int32\"):\n            if padding_option == \"nan\":\n                pytest.skip(\"Padding with NaN is not supported for integer types\")\n            a = torch.randint(0, 2, (n, ), device=device, dtype=src_dtype)\n        else:\n            a = torch.randn((n, ), device=device, dtype=src_dtype)\n        b = torch.zeros((n, ), device=device, dtype=dst_dtype)\n    \n        grid = lambda meta: (triton.cdiv(n, meta[\"BLOCK_SIZE\"]), )\n>       block_copy_kernel[grid](a_ptr=a, b_ptr=b, N=n, BLOCK_SIZE=64, padding_option=padding_option)\nE       NameError: name 'block_copy_kernel' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_block_copy_py_gen_triton_code_965927.py:86: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_tma_store_gemm.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_tma_load_store_128_128_64_1_4_False_True_True', 'test_tma_load_store_64_64_16_1_4_False_True_False', 'test_tma_load_store_64_128_32_1_4_False_True_False', 'test_tma_load_store_128_64_32_1_4_False_True_False', 'test_tma_load_store_128_64_32_1_4_False_True_True', 'test_tma_load_store_64_64_16_1_4_False_True_True', 'test_tma_load_store_64_128_32_1_4_False_True_True', 'test_tma_load_store_128_128_64_1_4_False_True_False'}______________ test_tma_load_store[64-64-16-1-4-False-True-False] ______________\n\nM = 64, N = 64, K = 16, NUM_CTAS = 1, NUM_WARPS = 4, TRANS_A = False\nTRANS_B = True, OUTPUT_F16 = False\nrequest = <FixtureRequest for <Function test_tma_load_store[64-64-16-1-4-False-True-False]>>\n\n    @pytest.mark.parametrize('M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_F16', [\n        [64, 64, 16, 1, 4, False, True, False],\n        [64, 64, 16, 1, 4, False, True, True],\n        [128, 64, 32, 1, 4, False, True, False],\n        [128, 64, 32, 1, 4, False, True, True],\n        [64, 128, 32, 1, 4, False, True, False],\n        [64, 128, 32, 1, 4, False, True, True],\n        [128, 128, 64, 1, 4, False, True, False],\n        [128, 128, 64, 1, 4, False, True, True],\n    ])\n    def test_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F16, request):\n        set_seed()\n    \n        if (TRANS_A):\n            a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n        else:\n            a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n        if (TRANS_B):\n            b = torch.randn((N, K), device='cuda', dtype=torch.float16).T\n        else:\n            b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n    \n        c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n        if OUTPUT_F16:\n            c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    \n>       matmul_tma_load_store[(1, 1)](\n            a_ptr=a, b_ptr=b, c_ptr=c,  #\n            M=M, N=N, K=K,  #\n            stride_am=a.stride(0), stride_ak=a.stride(1),  #\n            stride_bk=b.stride(0), stride_bn=b.stride(1),  #\n            stride_cm=c.stride(0), stride_cn=c.stride(1),  #\n            BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,  #\n            num_warps=NUM_WARPS, num_ctas=NUM_CTAS,  #\n            OUTPUT_F16=OUTPUT_F16)\nE       NameError: name 'matmul_tma_load_store' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_tma_store_gemm_py_gen_triton_code_283699.py:72: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_kernel_dot.py",
        "call_status": 1,
        "exec_status": 0,
        "stdout": "{'total_tensors': 1, 'matched_tensors': 0, 'match_percentage': 0.0, 'detailed_results': [('test_compile_kernel_dot_in_forked_subproc', False, 'Tensor-likes are not close!\\n\\nMismatched elements: 1 / 1 (100.0%)\\nGreatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)\\nGreatest relative difference: inf at index (0, 0) (up to 0.01 allowed)')]}",
        "stderr": "Value mismatch at test_compile_kernel_dot_in_forked_subproc: Tensor-likes are not close!\n\nMismatched elements: 1 / 1 (100.0%)\nGreatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)\nGreatest relative difference: inf at index (0, 0) (up to 0.01 allowed)__________________ test_compile_kernel_dot_in_forked_subproc ___________________\n\nfresh_triton_cache = '/tmp/tmpuj2ja4lj/.triton'\nrequest = <FixtureRequest for <Function test_compile_kernel_dot_in_forked_subproc>>\n\n    @skip_if_no_target\n    def test_compile_kernel_dot_in_forked_subproc(fresh_triton_cache, request) -> None: # Test name updated for clarity\n        config = AttrsDescriptor.from_hints({0: 16})\n        current_start_method = multiprocessing.get_start_method(allow_none=True)\n        if current_start_method is None:\n            try:\n                multiprocessing.set_start_method('fork', force=True)\n            except RuntimeError:\n                print(\"Warning: Could not force 'fork' start method. Using default.\")\n        if multiprocessing.get_start_method(allow_none=True) != 'fork':\n            pytest.skip(\"Test requires 'fork' multiprocessing start method.\")\n    \n        proc = multiprocessing.Process(target=compile_kernel_dot_for_test, args=(config, ))\n        proc.start()\n        proc.join(timeout=60)\n        if proc.is_alive():\n            proc.terminate()\n            proc.join()\n            pytest.fail(\"Process timed out\")\n    \n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n        ################### save tri_out in result_gold ###################\n        test_case_name = request.node.name\n        sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n        result_gold[sanitized_key_name] = torch.tensor([[0.0]]).clone().detach().cpu()\n        ###################################################################\n    \n>       assert proc.exitcode == 0\nE       AssertionError: assert 1 == 0\nE        +  where 1 = <Process name='Process-1' pid=78605 parent=78582 stopped exitcode=1>.exitcode\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_dot_py_gen_triton_code_956977.py:109: AssertionError\n=============================== warnings summary ===============================\ngeak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_dot_py_gen_triton_code_956977.py::test_compile_kernel_dot_in_forked_subproc\n  /opt/conda/envs/py_3.12/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=78582) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    self.pid = os.fork()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_dot_py_gen_triton_code_956977.py::test_compile_kernel_dot_in_forked_subproc - AssertionError: assert 1 == 0\n +  where 1 = <Process name='Process-1' pid=78605 parent=78582 stopped exitcode=1>.exitcode\n============= 1 failed, 1 passed, 7 deselected, 1 warning in 8.20s =============\n\nGenerated call accuracy: True\nExecution accuracy: False\nMatch percentage: 0.00%\nError: Value mismatch at test_compile_kernel_dot_in_forked_subproc: Tensor-likes are not close!\n\nMismatched elements: 1 / 1 (100.0%)\nGreatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)\nGreatest relative difference: inf at index (0, 0) (up to 0.01 allowed)",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_kernel_sub.py",
        "call_status": 1,
        "exec_status": 0,
        "stdout": "{'total_tensors': 1, 'matched_tensors': 0, 'match_percentage': 0.0, 'detailed_results': [('test_compile_kernel_sub_in_subproc', False, 'Tensor-likes are not close!\\n\\nMismatched elements: 1 / 1 (100.0%)\\nGreatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)\\nGreatest relative difference: inf at index (0, 0) (up to 0.01 allowed)')]}",
        "stderr": "Value mismatch at test_compile_kernel_sub_in_subproc: Tensor-likes are not close!\n\nMismatched elements: 1 / 1 (100.0%)\nGreatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)\nGreatest relative difference: inf at index (0, 0) (up to 0.01 allowed)______________________ test_compile_kernel_sub_in_subproc ______________________\n\nfresh_triton_cache = '/tmp/tmph6_kdk_3/.triton'\nrequest = <FixtureRequest for <Function test_compile_kernel_sub_in_subproc>>\n\n    @skip_if_no_target\n    def test_compile_kernel_sub_in_subproc(fresh_triton_cache, request) -> None: # Test name updated for clarity\n    \n        set_seed()\n    \n        config = AttrsDescriptor.from_hints({i: 16 for i in range(4)})\n        try:\n            multiprocessing.set_start_method('fork', force=True)\n        except RuntimeError:\n            print(\"Warning: Could not force 'fork' start method. Using default.\")\n            if multiprocessing.get_start_method(allow_none=True) != 'fork': # allow_none for safety\n                pytest.skip(\"Test requires 'fork' multiprocessing start method.\")\n    \n        proc = multiprocessing.Process(target=compile_kernel_sub_for_test, args=(config, ))\n        proc.start()\n        proc.join(timeout=60)\n        if proc.is_alive():\n            proc.terminate()\n            proc.join()\n            pytest.fail(\"Process timed out\")\n    \n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n        ################### save tri_out in result_gold ###################\n        test_case_name = request.node.name\n        sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n        result_gold[sanitized_key_name] = torch.tensor([[0.0]]).clone().detach().cpu()\n        ###################################################################\n    \n>       assert proc.exitcode == 0\nE       AssertionError: assert 1 == 0\nE        +  where 1 = <Process name='Process-1' pid=78699 parent=78676 stopped exitcode=1>.exitcode\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_sub_py_gen_triton_code_82768.py:116: AssertionError\n=============================== warnings summary ===============================\ngeak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_sub_py_gen_triton_code_82768.py::test_compile_kernel_sub_in_subproc\n  /opt/conda/envs/py_3.12/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=78676) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    self.pid = os.fork()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_kernel_sub_py_gen_triton_code_82768.py::test_compile_kernel_sub_in_subproc - AssertionError: assert 1 == 0\n +  where 1 = <Process name='Process-1' pid=78699 parent=78676 stopped exitcode=1>.exitcode\n============ 1 failed, 1 passed, 43 deselected, 1 warning in 8.16s =============\n\nGenerated call accuracy: True\nExecution accuracy: False\nMatch percentage: 0.00%\nError: Value mismatch at test_compile_kernel_sub_in_subproc: Tensor-likes are not close!\n\nMismatched elements: 1 / 1 (100.0%)\nGreatest absolute difference: 1.0 at index (0, 0) (up to 0.01 allowed)\nGreatest relative difference: inf at index (0, 0) (up to 0.01 allowed)",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_triton_flip.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_flip_float32_64_8', 'test_flip_int32_64_8', 'test_flip_bfloat16_512_1', 'test_flip_int32_16_256', 'test_flip_bfloat16_64_8', 'test_flip_bfloat16_8_512', 'test_flip_float16_8_512', 'test_flip_float32_16_256', 'test_flip_int32_8_512', 'test_flip_int32_512_1', 'test_flip_bfloat16_16_256', 'test_flip_float32_512_1', 'test_flip_float16_16_256', 'test_flip_float16_512_1', 'test_flip_float16_64_8', 'test_flip_float32_8_512'}____________________________ test_flip[int32-512-1] ____________________________\n\nN_rows = 1, M_cols = 512, dtype_str = 'int32'\nrequest = <FixtureRequest for <Function test_flip[int32-512-1]>>\ndevice = 'cuda'\n\n    @pytest.mark.interpreter\n    @pytest.mark.parametrize(\"M_cols, N_rows\", [[512, 1], [64, 8], [16, 256], [8, 512]])\n    @pytest.mark.parametrize(\"dtype_str\", ['int32', 'float16', 'float32', 'bfloat16'])\n    def test_flip(N_rows, M_cols, dtype_str, request, device='cuda'):\n        set_seed()\n    \n        x_np = gen_numpy_array_for_torch_conversion((N_rows, M_cols), dtype_str=dtype_str)\n    \n        torch_dtype = torch_dtype_from_str(dtype_str)\n        x = torch.from_numpy(x_np).to(dtype=torch_dtype, device=device)\n    \n        y_ref = torch.flip(x, dims=(1,)) # Flip along the columns (dimension 1)\n        z_triton = torch.empty_like(x)\n    \n        grid = (N_rows,) # Each program flips one row\n>       flip_kernel[grid](x, z_triton, N_rows, M_cols) # num_warps removed\nE       NameError: name 'flip_kernel' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_flip_py_gen_triton_code_849431.py:94: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_triton_sort.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_sort_float16_False_8_512', 'test_sort_bfloat16_True_16_256', 'test_sort_int32_True_64_8', 'test_sort_float16_False_512_1', 'test_sort_float16_True_8_512', 'test_sort_int32_False_16_256', 'test_sort_float32_True_8_512', 'test_sort_int32_False_64_8', 'test_sort_float32_False_512_1', 'test_sort_int32_True_512_1', 'test_sort_float32_False_8_512', 'test_sort_float32_False_16_256', 'test_sort_bfloat16_False_512_1', 'test_sort_bfloat16_False_16_256', 'test_sort_float16_False_64_8', 'test_sort_int32_False_8_512', 'test_sort_int32_True_16_256', 'test_sort_int32_True_8_512', 'test_sort_float16_True_16_256', 'test_sort_float32_True_64_8', 'test_sort_bfloat16_False_64_8', 'test_sort_float32_False_64_8', 'test_sort_float32_True_16_256', 'test_sort_bfloat16_True_64_8', 'test_sort_float16_True_64_8', 'test_sort_bfloat16_True_512_1', 'test_sort_int32_False_512_1', 'test_sort_float32_True_512_1', 'test_sort_float16_False_16_256', 'test_sort_bfloat16_False_8_512', 'test_sort_bfloat16_True_8_512', 'test_sort_float16_True_512_1'}_________________________ test_sort[int32-False-512-1] _________________________\n\nN_rows = 1, M_cols = 512, descending = False, dtype_str = 'int32'\nrequest = <FixtureRequest for <Function test_sort[int32-False-512-1]>>\ndevice = 'cuda'\n\n    @pytest.mark.interpreter\n    @pytest.mark.parametrize(\"M_cols, N_rows\", [[512, 1], [64, 8], [16, 256], [8, 512]])\n    @pytest.mark.parametrize(\"descending\", [False, True])\n    @pytest.mark.parametrize(\"dtype_str\", ['int32', 'float16', 'float32', 'bfloat16'])\n    def test_sort(N_rows, M_cols, descending, dtype_str, request, device='cuda'):\n        set_seed()\n    \n        x_np = gen_numpy_array_for_torch_conversion((N_rows, M_cols), dtype_str=dtype_str)\n    \n        torch_dtype = torch_dtype_from_str(dtype_str)\n        x = torch.from_numpy(x_np).to(dtype=torch_dtype, device=device) # Ensure correct torch dtype\n    \n        y_ref = torch.sort(x, dim=1, descending=descending)[0]\n        z_triton = torch.empty_like(x)\n    \n        # Grid is (N_rows,) since each program sorts one row\n        grid = (N_rows,)\n>       sort_kernel[grid](x, z_triton, N_rows, M_cols, descending) # num_warps removed, let triton decide or set default\nE       NameError: name 'sort_kernel' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_sort_py_gen_triton_code_602723.py:103: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_triton_swizzle2d.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_swizzle2d_5_7_3'}____________________________ test_swizzle2d[5-7-3] _____________________________\n\nsize_i = 5, size_j = 7, size_g = 3\nrequest = <FixtureRequest for <Function test_swizzle2d[5-7-3]>>, device = 'cuda'\n\n    @pytest.mark.interpreter\n    @pytest.mark.parametrize(\"size_i, size_j, size_g\", [[5, 7, 3]])\n    def test_swizzle2d(size_i, size_j, size_g, request, device='cuda'):\n        # Output tensor to store results, initialized to a value like -1 to see what's written\n    \n        set_seed()\n    \n        output = torch.zeros(size_i, size_j).to(device)\n>       swizzle2d_kernel[(1, )](output, size_i, size_j, size_g)\nE       NameError: name 'swizzle2d_kernel' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_391304.py:54: NameError\n=============================== warnings summary ===============================\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_391304.py:46\n  /root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_391304.py:46: PytestUnknownMarkWarning: Unknown pytest.mark.interpreter - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.interpreter\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_triton_swizzle2d_py_gen_triton_code_391304.py::test_swizzle2d[5-7-3] - NameError: name 'swizzle2d_kernel' is not defined\n============ 1 failed, 1 passed, 17 deselected, 1 warning in 8.40s =============\n\nGenerated call accuracy: False\nExecution accuracy: False\nMatch percentage: 0.00%\nError: Key mismatch at : Missing keys: {'test_swizzle2d_5_7_3'} ",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_random_int.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_randint_size1_0_int32_False', 'test_randint_size33_124_int32_False', 'test_randint_size48_0_int32_True', 'test_randint_size35_124_int64_False', 'test_randint_size47_67830198458_int64_False', 'test_randint_size66_4294967295_int64_True', 'test_randint_size54_42_int64_True', 'test_randint_size20_67830198458_int32_True', 'test_randint_size37_54_int32_False', 'test_randint_size32_124_int32_True', 'test_randint_size34_124_int64_True', 'test_randint_size70_67830198458_int64_True', 'test_randint_size13_54_int32_False', 'test_randint_size11_124_int64_False', 'test_randint_size62_54_int64_True', 'test_randint_size68_67830198458_int32_True', 'test_randint_size63_54_int64_False', 'test_randint_size8_124_int32_True', 'test_randint_size30_42_int64_True', 'test_randint_size59_124_int64_False', 'test_randint_size5_42_int32_False', 'test_randint_size18_4294967295_int64_True', 'test_randint_size44_67830198458_int32_True', 'test_randint_size45_67830198458_int32_False', 'test_randint_size27_0_int64_False', 'test_randint_size12_54_int32_True', 'test_randint_size52_42_int32_True', 'test_randint_size38_54_int64_True', 'test_randint_size58_124_int64_True', 'test_randint_size49_0_int32_False', 'test_randint_size4_42_int32_True', 'test_randint_size25_0_int32_False', 'test_randint_size42_4294967295_int64_True', 'test_randint_size0_0_int32_True', 'test_randint_size17_4294967295_int32_False', 'test_randint_size61_54_int32_False', 'test_randint_size50_0_int64_True', 'test_randint_size15_54_int64_False', 'test_randint_size57_124_int32_False', 'test_randint_size65_4294967295_int32_False', 'test_randint_size7_42_int64_False', 'test_randint_size16_4294967295_int32_True', 'test_randint_size36_54_int32_True', 'test_randint_size40_4294967295_int32_True', 'test_randint_size60_54_int32_True', 'test_randint_size28_42_int32_True', 'test_randint_size6_42_int64_True', 'test_randint_size43_4294967295_int64_False', 'test_randint_size51_0_int64_False', 'test_randint_size55_42_int64_False', 'test_randint_size39_54_int64_False', 'test_randint_size67_4294967295_int64_False', 'test_randint_size14_54_int64_True', 'test_randint_size21_67830198458_int32_False', 'test_randint_size53_42_int32_False', 'test_randint_size24_0_int32_True', 'test_randint_size3_0_int64_False', 'test_randint_size10_124_int64_True', 'test_randint_size31_42_int64_False', 'test_randint_size46_67830198458_int64_True', 'test_randint_size23_67830198458_int64_False', 'test_randint_size41_4294967295_int32_False', 'test_randint_size22_67830198458_int64_True', 'test_randint_size2_0_int64_True', 'test_randint_size9_124_int32_False', 'test_randint_size19_4294967295_int64_False', 'test_randint_size29_42_int32_False', 'test_randint_size56_124_int32_True', 'test_randint_size26_0_int64_True', 'test_randint_size64_4294967295_int32_True', 'test_randint_size71_67830198458_int64_False', 'test_randint_size69_67830198458_int32_False'}_______________________ test_randint[size0-0-int32-True] _______________________\n\nsize = [10], seed = 0, dtype = 'int32', const_seed = True\nrequest = <FixtureRequest for <Function test_randint[size0-0-int32-True]>>\ndevice = 'cuda'\n\n    @pytest.mark.interpreter\n    @pytest.mark.parametrize('size, seed, dtype, const_seed', [(size, seed, dtype, const_seed)\n                                                               for size_str in ['10', '4,53', '400'] # Renamed to size_str\n                                                               for size in [[int(s) for s in size_str.split(',')]] # Process size here\n                                                               for seed in [0, 42, 124, 54, 0xffffffff, 0x0000000fcafeb0ba]\n                                                               for dtype in ['int32', 'int64']\n                                                               for const_seed in [True, False]])\n    def test_randint(size, seed, dtype, const_seed, request, device='cuda'):\n        # size = list(map(int, size.split(','))) # Moved to parametrize\n        set_seed()\n    \n        torch_dtype = getattr(torch, dtype)\n        numpy_dtype = getattr(np, f\"u{dtype}\") # Philox generates unsigned integers\n        config = {'int32': PHILOX_32, 'int64': PHILOX_64}[dtype]\n    \n        # triton result\n        x = torch.empty(size, dtype=torch_dtype, device=device)\n        N = x.numel()\n        grid = (triton.cdiv(N, BLOCK), )\n        if N > 0: # Ensure grid is not (0,) if N is 0\n            if const_seed:\n>               randint_kernel_const_seed[grid](x, N, seed_val=seed)\nE               NameError: name 'randint_kernel_const_seed' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_random_int_py_gen_triton_code_498599.py:169: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_randn.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_rand_100000_42_int64_False', 'test_rand_100000_0_int32_False', 'test_rand_100000_42_int64_True', 'test_rand_100000_0_int64_True', 'test_rand_100000_0_int64_False', 'test_rand_100000_54_int32_True', 'test_rand_100000_0_int32_True', 'test_rand_100000_124_int32_False', 'test_rand_100000_42_int32_True', 'test_rand_100000_54_int64_False', 'test_rand_100000_124_int32_True', 'test_rand_100000_54_int64_True', 'test_rand_100000_124_int64_False', 'test_rand_100000_42_int32_False', 'test_rand_100000_54_int32_False', 'test_rand_100000_124_int64_True'}________________________ test_rand[100000-0-int32-True] ________________________\n\nsize = 100000, seed = 0, dtype = 'int32', const_seed = True\nrequest = <FixtureRequest for <Function test_rand[100000-0-int32-True]>>\ndevice = 'cuda'\n\n    @pytest.mark.interpreter\n    @pytest.mark.parametrize('size, seed, dtype, const_seed', [(size, seed, dtype, const_seed)\n                                                               for size in [100000]\n                                                               for seed in [0, 42, 124, 54]\n                                                               for dtype in ['int32', 'int64']\n                                                               for const_seed in [True, False]])\n    def test_rand(size, seed, dtype, const_seed,  request, device='cuda'):\n    \n    \n        # triton result\n        set_seed(seed)\n    \n        x = torch.empty(size, dtype=torch.float32, device=device)\n        N = x.numel()\n        grid = (triton.cdiv(N, BLOCK), )\n        if const_seed:\n>           randn_kernel_const_seed[grid](x, N, seed=seed, dtype=getattr(tl, dtype))\nE           NameError: name 'randn_kernel_const_seed' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_randn_py_gen_triton_code_82552.py:161: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_matmul_MXFP.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "None",
        "stderr": "Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_485029_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_485029.py _\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_485029.py:252: in <module>\n    if is_cuda() and torch.cuda.get_device_capability()[0] >=9 :\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_matmul_MXFP_py_gen_triton_code_485029.py:44: in is_cuda\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\nE   NameError: name 'triton' is not defined",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_load_reduce.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_load_reduce_128_64_float16'}_______________________ test_load_reduce[128-64-float16] _______________________\n\nBLOCK_M = 128, BLOCK_N = 64, dtype_str = 'float16'\nrequest = <FixtureRequest for <Function test_load_reduce[128-64-float16]>>\n\n    @pytest.mark.parametrize('BLOCK_M,BLOCK_N,dtype_str', [(128, 64, dtype_str) for dtype_str in ['float16']])\n    def test_load_reduce(BLOCK_M, BLOCK_N, dtype_str, request):\n        set_seed()\n    \n        dtype = dtype_mapping[dtype_str]\n        x = torch.randn((BLOCK_M, BLOCK_N), device='cuda', dtype=dtype)\n        y = torch.empty((BLOCK_M, ), device='cuda', dtype=dtype)\n    \n>       load_reduce_kernel[(1, )](x, y, x.stride(0), x.stride(1), y.stride(0), BLOCK_M, BLOCK_N)\nE       NameError: name 'load_reduce_kernel' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_load_reduce_py_gen_triton_code_894444.py:61: NameError\n=========================== short test summary info ============================\nFAILED sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_load_reduce_py_gen_triton_code_894444.py::test_load_reduce[128-64-float16] - NameError: name 'load_reduce_kernel' is not defined\n================== 1 failed, 1 passed, 43 deselected in 8.39s ==================\n\nGenerated call accuracy: False\nExecution accuracy: False\nMatch percentage: 0.00%\nError: Key mismatch at : Missing keys: {'test_load_reduce_128_64_float16'} ",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "naive_softmax.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "None",
        "stderr": "Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/naive_softmax_py_gen_triton_code_389147_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/naive_softmax_py_gen_triton_code_389147.py _\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/naive_softmax_py_gen_triton_code_389147.py:175: in <module>\n    @pytest.mark.parametrize('M, N', [(1823, 781), (1, 1), (128, 1), (1, 128), (8192, 8192), (4096, 8192), (359, 1),\nE   NameError: name 'pytest' is not defined",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_add_kernel.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_add_98432_1024_float16', 'test_add_98432_1024_float32'}_________________________ test_add[98432-1024-float16] _________________________\n\nSIZE = 98432, BLOCK_SIZE = 1024, dtype_str = 'float16'\nrequest = <FixtureRequest for <Function test_add[98432-1024-float16]>>\n\n    @pytest.mark.parametrize('SIZE,BLOCK_SIZE,dtype_str',\n                             [(98432, 1024, dtype_str) for dtype_str in ['float16', 'float32']])\n    def test_add(SIZE, BLOCK_SIZE, dtype_str, request):\n        set_seed()\n    \n        dtype = dtype_mapping[dtype_str]\n        output = torch.empty(SIZE, device='cuda', dtype=dtype)\n        x = torch.randn(SIZE, device='cuda', dtype=dtype)\n        y = torch.randn(SIZE, device='cuda', dtype=dtype)\n    \n        def grid(meta):\n            return (triton.cdiv(SIZE, meta['BLOCK_SIZE']), )\n    \n>       add_kernel[grid](x, y, output, SIZE, BLOCK_SIZE=BLOCK_SIZE)\nE       NameError: name 'add_kernel' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_add_kernel_py_gen_triton_code_777671.py:91: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_gemm_no_scf.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "{'total_tensors': 0, 'matched_tensors': 0, 'match_percentage': 0, 'detailed_results': []}",
        "stderr": "Key mismatch at : Missing keys: {'test_gemm_no_scf_128_128_16_1_4_False_True_float16_True', 'test_gemm_no_scf_64_16_16_1_4_False_True_float16_True', 'test_gemm_no_scf_64_64_64_1_4_False_True_float32_False', 'test_gemm_no_scf_32_16_16_1_4_False_True_float32_False', 'test_gemm_no_scf_64_64_16_1_4_False_True_float32_True', 'test_gemm_no_scf_32_16_16_1_4_False_True_float32_True', 'test_gemm_no_scf_16_16_16_1_4_False_True_float32_True', 'test_gemm_no_scf_64_16_16_1_4_False_True_float16_False', 'test_gemm_no_scf_16_16_16_1_4_False_True_float32_False', 'test_gemm_no_scf_128_128_16_1_4_False_True_float32_True', 'test_gemm_no_scf_32_32_16_1_4_False_True_float32_False', 'test_gemm_no_scf_16_32_16_1_4_False_True_float32_True', 'test_gemm_no_scf_16_32_16_1_4_False_True_float32_False', 'test_gemm_no_scf_64_32_16_1_4_False_True_float16_True', 'test_gemm_no_scf_64_64_64_1_4_False_True_float32_True', 'test_gemm_no_scf_64_64_16_1_4_False_True_float32_False', 'test_gemm_no_scf_64_64_16_1_4_False_True_float16_True', 'test_gemm_no_scf_64_64_32_1_4_False_True_float32_False', 'test_gemm_no_scf_64_64_16_1_4_False_True_float16_False', 'test_gemm_no_scf_64_64_32_1_4_False_True_float32_True', 'test_gemm_no_scf_32_32_16_1_4_False_True_float32_True', 'test_gemm_no_scf_128_128_16_1_4_False_True_float32_False', 'test_gemm_no_scf_128_128_16_1_4_False_True_float16_False', 'test_gemm_no_scf_64_32_16_1_4_False_True_float16_False'}____________ test_gemm_no_scf[64-16-16-1-4-False-True-float16-True] ____________\n\nM = 64, N = 16, K = 16, NUM_CTAS = 1, NUM_WARPS = 4, TRANS_A = False\nTRANS_B = True, OUTPUT_TYPE = 'float16', USE_TMA_EPILOGUE = True\nrequest = <FixtureRequest for <Function test_gemm_no_scf[64-16-16-1-4-False-True-float16-True]>>\n\n    @pytest.mark.parametrize(\n        'M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_TYPE,USE_TMA_EPILOGUE',\n        itertools.chain(*[[\n            # numCTAs = 1, no TMA multicast:\n            [64, 16, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n            [64, 32, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n            [64, 64, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n            [64, 64, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n            [64, 64, 32, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n            [64, 64, 64, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n            [128, 128, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n            [128, 128, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n            # static mask, cluster 4x1\n            [256, 64, 16, 4, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n            [256, 64, 16, 4, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n            # dynamic mask, cluster 2x2\n            [128, 128, 16, 4, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n            [128, 128, 16, 4, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n            # small M, N\n            [16, 16, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n            [16, 32, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n            [32, 16, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n            [32, 32, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        ] for USE_TMA_EPILOGUE in [True, False]]))\n    @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n    def test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, request):\n        set_seed()\n    \n        if is_hip() and NUM_CTAS > 1:\n            pytest.skip(\"NUM_CTAS > 1 is not supported in HIP backend\")\n    \n        if (TRANS_A):\n            a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n        else:\n            a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n        if (TRANS_B):\n            b = torch.randn((N, K), device='cuda', dtype=torch.float16).T\n        else:\n            b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n    \n        if OUTPUT_TYPE == \"float16\":\n            c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n        else:\n            c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n    \n>       matmul_no_scf_kernel[(1, 1)](\n            a_ptr=a, b_ptr=b, c_ptr=c,  #\n            M=M, N=N, K=K,  #\n            stride_am=a.stride(0), stride_ak=a.stride(1),  #\n            stride_bk=b.stride(0), stride_bn=b.stride(1),  #\n            stride_cm=c.stride(0), stride_cn=c.stride(1),  #\n            BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,  #\n            num_warps=NUM_WARPS,  #\n            num_ctas=NUM_CTAS,  #\n            FLOAT16_OUTPUT=(OUTPUT_TYPE == \"float16\"),  #\n            USE_TMA_EPILOGUE=USE_TMA_EPILOGUE)\nE       NameError: name 'matmul_no_scf_kernel' is not defined\n\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_gemm_no_scf_py_gen_triton_code_921302.py:101: NameError",
        "difficulty": -1
    },
    {
        "pass_num": 0,
        "file_name": "test_flashattention_fwd.py",
        "call_status": 0,
        "exec_status": 0,
        "stdout": "None",
        "stderr": "Error loading generated PT file: [Errno 2] No such file or directory: '/root/sapmajum/dev/TB-eval/geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_578436_py.pt'_ ERROR collecting geak_eval/sample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_578436.py _\nsample/20250530_reflexion_oneshot_gpt41_noprocess_0_0/tmp/tmp/gen/test_flashattention_fwd_py_gen_triton_code_578436.py:244: in <module>\n    triton.testing.Benchmark(\nE   NameError: name 'triton' is not defined",
        "difficulty": -1
    }
]