[
    {
        "file": "moe_gemm.py",
        "target_kernel_name": "moe_gemm_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given a instruction/function definition of the required kernel : moe-gemm, you task is to complete the kernel code for the corresponding operator/function definition using triton programming language. Only complete the kernel code in the function definition, DONT remove any python imports in the instruction provided, DONT change/interfere with the provided function definition and parameter list ,only add if required. :\n\nimport triton  \nimport triton.language as tl  \n\n\n@triton.jit  \ndef moe_gemm_kernel(  \n    A,  \n    B,  \n    Out,  \n    A_scale,  \n    B_scale,  \n    stride_am,  \n    stride_ak,  \n    stride_be,  \n    stride_bn,  \n    stride_bk,  \n    stride_cm,  \n    stride_cn,  \n    stride_bse,  \n    stride_bsn,  \n    top_k: tl.constexpr,  \n    topk_weights_ptr,  \n    sorted_token_ids_ptr,  \n    expert_ids_ptr,  \n    EM: tl.constexpr,  \n    N: tl.constexpr,  \n    K: tl.constexpr,  \n    EVEN_K: tl.constexpr,  \n    MUL_ROUTED_WEIGHT: tl.constexpr,  \n    use_fp8_w8a8: tl.constexpr,  \n    use_int8_w8a16: tl.constexpr,  \n    use_int8_w8a8: tl.constexpr,  \n    BLOCK_SIZE_M: tl.constexpr,  \n    BLOCK_SIZE_N: tl.constexpr,  \n    BLOCK_SIZE_K: tl.constexpr,  \n    GROUP_SIZE_M: tl.constexpr,  \n):  \n    \"\"\"  \n    Implements the fused computation for a Mixture of Experts (MOE) using  \n    token and expert matrices.  \n\n    Key Parameters:  \n    - A: The input tensor representing tokens with shape (*, K), where '*' can  \n        be any shape representing batches and K is the feature dimension of  \n        each token.  \n    - B: The stacked MOE weight tensor with shape (E, N, K), where E is  \n        the number of experts, K is the input feature dimension, and N is  \n        the output feature dimension.  \n    - C: The output cache tensor with shape (M, topk, N), where M is the  \n        total number of tokens post padding, topk is the number of times  \n        each token is repeated, and N is the output feature dimension.  \n    - sorted_token_ids: A tensor containing the sorted indices of tokens,  \n        repeated topk times and arranged by the expert index they are  \n        assigned to.  \n    - expert_ids: A tensor containing the indices of the expert for each  \n        block. It determines which expert matrix from B should be used for  \n        each block in A.  \n    This kernel performs the multiplication of a token by its corresponding  \n    expert matrix as determined by `expert_ids`. The sorting of  \n    `sorted_token_ids` by expert index and padding ensures divisibility by  \n    BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix  \n    multiplication across different blocks processed by the same expert.  \n    \"\"\"  \n",
        "label": "#Usage Instruction: python3 -m pytest -v moe-gemm-TB.py  \nimport triton  \nimport triton.language as tl  \n\n\n@triton.jit  \ndef moe_gemm_kernel(  \n    A,  \n    B,  \n    Out,  \n    A_scale,  \n    B_scale,  \n    stride_am,  \n    stride_ak,  \n    stride_be,  \n    stride_bn,  \n    stride_bk,  \n    stride_cm,  \n    stride_cn,  \n    stride_bse,  \n    stride_bsn,  \n    top_k: tl.constexpr,  \n    topk_weights_ptr,  \n    sorted_token_ids_ptr,  \n    expert_ids_ptr,  \n    EM: tl.constexpr,  \n    N: tl.constexpr,  \n    K: tl.constexpr,  \n    EVEN_K: tl.constexpr,  \n    MUL_ROUTED_WEIGHT: tl.constexpr,  \n    use_fp8_w8a8: tl.constexpr,  \n    use_int8_w8a16: tl.constexpr,  \n    use_int8_w8a8: tl.constexpr,  \n    BLOCK_SIZE_M: tl.constexpr,  \n    BLOCK_SIZE_N: tl.constexpr,  \n    BLOCK_SIZE_K: tl.constexpr,  \n    GROUP_SIZE_M: tl.constexpr,  \n):  \n    \"\"\"  \n    Implements the fused computation for a Mixture of Experts (MOE) using  \n    token and expert matrices.  \n  \n    Key Parameters:  \n    - A: The input tensor representing tokens with shape (*, K), where '*' can  \n        be any shape representing batches and K is the feature dimension of  \n        each token.  \n    - B: The stacked MOE weight tensor with shape (E, N, K), where E is  \n        the number of experts, K is the input feature dimension, and N is  \n        the output feature dimension.  \n    - C: The output cache tensor with shape (M, topk, N), where M is the  \n        total number of tokens post padding, topk is the number of times  \n        each token is repeated, and N is the output feature dimension.  \n    - sorted_token_ids: A tensor containing the sorted indices of tokens,  \n        repeated topk times and arranged by the expert index they are  \n        assigned to.  \n    - expert_ids: A tensor containing the indices of the expert for each  \n        block. It determines which expert matrix from B should be used for  \n        each block in A.  \n    This kernel performs the multiplication of a token by its corresponding  \n    expert matrix as determined by `expert_ids`. The sorting of  \n    `sorted_token_ids` by expert index and padding ensures divisibility by  \n    BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix  \n    multiplication across different blocks processed by the same expert.  \n    \"\"\"  \n    pid = tl.program_id(axis=0)  \n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)  \n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)  \n    num_pid_in_group = GROUP_SIZE_M * num_pid_n  \n    group_id = pid // num_pid_in_group  \n    first_pid_m = group_id * GROUP_SIZE_M  \n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)  \n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)  \n    pid_n = (pid % num_pid_in_group) // group_size_m  \n  \n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)  \n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)  \n  \n    # Here we assume that valid tokens are in the range [0, M).  \n    token_mask = (offs_token >= 0) & (offs_token < EM)  \n  \n    off_experts = tl.load(expert_ids_ptr + pid_m)  \n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)) % N  \n    offs_k = tl.arange(0, BLOCK_SIZE_K)  \n    a_ptrs = A + (offs_token[:, None] // top_k * stride_am + offs_k[None, :] * stride_ak)  \n    b_ptrs = B + off_experts * stride_be + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)  \n  \n    if use_int8_w8a16:  \n        b_scale_ptrs = B_scale + off_experts * stride_bse + offs_bn[None, :] * stride_bsn  \n        b_scale = tl.load(b_scale_ptrs)  \n  \n    if use_fp8_w8a8 or use_int8_w8a8:  \n        a_scale = tl.load(A_scale)  \n        b_scale = tl.load(B_scale + off_experts)  \n  \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)  \n  \n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):  \n        # Masking ensures we don't load from invalid tokens or indices  \n        if EVEN_K:  \n            a = tl.load(a_ptrs, mask=(token_mask[:, None]), other=0.0)  \n            b = tl.load(b_ptrs)  \n        else:  \n            a = tl.load(a_ptrs, mask=(token_mask[:, None] & (offs_k[None, :] < K - k * BLOCK_SIZE_K)), other=0.0)  \n            b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K), other=0.0)  \n  \n        if use_int8_w8a16:  \n            accumulator = tl.dot(a, b.to(a.dtype), acc=accumulator)  \n        elif use_fp8_w8a8 or use_int8_w8a8:  \n            accumulator += tl.dot(a, b)  \n        else:  \n            accumulator = tl.dot(a, b, acc=accumulator)  \n  \n        a_ptrs += BLOCK_SIZE_K * stride_ak  \n        b_ptrs += BLOCK_SIZE_K * stride_bk  \n  \n    if MUL_ROUTED_WEIGHT:  \n        moe_weight = tl.load(topk_weights_ptr + offs_token, mask=token_mask, other=0)  \n        accumulator = accumulator * moe_weight[:, None]  \n  \n    if use_int8_w8a16:  \n        accumulator = (accumulator * b_scale).to(Out.dtype.element_ty)  \n    elif use_fp8_w8a8 or use_int8_w8a8:  \n        accumulator = (accumulator * a_scale * b_scale).to(Out.dtype.element_ty)  \n    else:  \n        accumulator = accumulator.to(Out.dtype.element_ty)  \n  \n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)  \n    out_ptrs = Out + stride_cm * offs_token[:, None] + stride_cn * offs_cn[None, :]  \n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)  \n    tl.store(out_ptrs, accumulator.to(Out.dtype.element_ty), mask=c_mask)\n\n\n##################################################################################################################################################\n\n  \nimport triton # Required for triton.testing utilities and launching kernel  \nimport torch  \nimport pytest  \nfrom typing import Any, Dict, Optional  \nimport os  \nimport json  \nimport functools  \nimport argparse  \nimport sys  \nimport triton.language as tl # Required for tl.constexpr in quantize_input and other places  \n\n\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n######################################## HELPERS for Eval ######################################## \nimport numpy as np\nimport random\nimport torch \n\nresult_gold = {}\nCONFIG = {\n  \"llama3\": {\n    \"8B\": {\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"vocab_size\": 128256\n    },\n    \"70B\": {\n      \"num_attention_heads\": 64,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 8192,\n      \"intermediate_size\": 28672,\n      \"vocab_size\": 128256\n    },\n    \"405B\": {\n      \"num_attention_heads\": 128,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 16384,\n      \"intermediate_size\": 53248,\n      \"vocab_size\": 128256\n    }\n  },\n  \"mistral\": {\n    \"7B\": {\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    },\n    \"22B\": {\n      \"hidden_size\": 6144,\n      \"intermediate_size\": 16384,\n      \"num_attention_heads\": 48,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    }\n\n  }\n}\n\n  \ndef get_model_configs(config_path='model_configs.json', model_families=[\"llama3\"], model=\"all\"):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_path (str): User-provided path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        dict: A dictionary of available models and their configurations for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    # Extract models and their configurations for the specified families  \n    filtered_configs = {}  \n  \n    for family in model_families:  \n        if family in configs:  \n            # Check if model filtering is required  \n            if model == \"all\":  \n                # Include all models in the family  \n                for model_size, model_configs in configs[family].items():  \n                    filtered_configs[f\"{family}-{model_size}\"] = model_configs  \n            else:  \n                # Parse the model string (e.g., llama3_8B or llama3-8B)  \n                delimiter = \"_\" if \"_\" in model else \"-\"  \n                model_parts = model.split(delimiter)  \n  \n                # Check if the family and size match  \n                if len(model_parts) == 2 and model_parts[0] == family:  \n                    model_size = model_parts[1]  \n                    if model_size in configs[family]:  \n                        filtered_configs[f\"{family}-{model_size}\"] = configs[family][model_size]  \n  \n    if not filtered_configs:  \n        print(f\"Warning: No models selected for families: {model_families} with filter: '{model}'\")  \n  \n    return filtered_configs  \n  \n  \ndef get_available_models(config_file='model_configs.json', model_families=[\"llama3\"]):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_file (str): Path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        list: A list of available models for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    models = [f\"{family}-{model}\" for family in model_families if family in configs for model in configs[family]]  \n  \n    return models  \n\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\nM_THRESHOLD_SMALL = 256  \nM_THRESHOLD_MEDIUM = 1024  \n  \ndtype_max = {  \n    dtype: (torch.finfo(dtype) if dtype.is_floating_point else torch.iinfo(dtype)).max  \n    for dtype in [  \n        torch.float8_e5m2fnuz,  \n        torch.float8_e4m3fnuz,  \n        torch.int8,  \n    ]  \n}  \n  \nsupported_fp8 = [torch.float8_e4m3fnuz, torch.float8_e5m2fnuz]  \n  \n  \nclass MetaData():  \n    use_fp8_w8a8 = False  \n    use_int8_w8a16 = False  \n    use_int8_w8a8 = False  \n  \n    def __init__(self, top_k, topk_weights, topk_ids, sorted_token_ids, expert_ids, num_tokens_post_padded, config):  \n        self.top_k = top_k  \n        self.topk_weights = topk_weights  \n        self.topk_ids = topk_ids  \n        self.sorted_token_ids = sorted_token_ids  \n        self.expert_ids = expert_ids  \n        self.num_tokens_post_padded = num_tokens_post_padded  \n        self.config = config  \n  \n    def set_use_fp8_w8a8(self, a_descale, b_descale, fp8_type):  \n        self.use_fp8_w8a8 = True  \n        self.a_descale = a_descale  \n        self.b_descale = b_descale  \n        self.fp8_type = fp8_type  \n  \n    def set_use_int8_w8a16(self, b_descale):  \n        self.use_int8_w8a16 = True  \n        self.b_descale = b_descale  \n        self.a_descale = None  \n  \n    def set_use_int8_w8a8(self, a_descale, b_descale):  \n        self.use_int8_w8a8 = True  \n        self.a_descale = a_descale  \n        self.b_descale = b_descale  \n  \n    def check_args(self, a, b, o):  \n        assert a.shape[-1] == b.shape[-1] and b.shape[1] == o.shape[-1]  \n  \n        assert not (self.use_fp8_w8a8 and self.use_int8_w8a16 and self.use_int8_w8a8)  \n        if self.use_fp8_w8a8:  \n            assert self.fp8_type in supported_fp8, f\"fp8 type {self.fp8_type} not supported\"  \n  \n  \ndef _moe_align_block_size(topk_ids: torch.Tensor, num_experts: int, top_k: int, block_size: int,  \n                          sorted_token_ids: torch.Tensor, expert_ids: torch.Tensor,  \n                          num_tokens_post_pad: torch.Tensor) -> None:  \n    M, top_k = topk_ids.shape  \n  \n    expert_to_tokens = [[] for _ in range(num_experts)]  \n    # For each token, for each selected expert, we append (token_id, expert)  \n    for token_id in range(M):  \n        for j in range(top_k):  \n            e_id = topk_ids[token_id, j].item()  \n            expert_to_tokens[e_id].append(token_id * top_k + j)  \n  \n    # Reorder tokens block by block, padding if needed  \n    reordered_token_ids = []  \n    reordered_expert_ids = []  \n  \n    for e_id in range(num_experts):  \n        tokens_for_expert = expert_to_tokens[e_id]  \n        num_tokens = len(tokens_for_expert)  \n  \n        n_blocks = ((num_tokens + block_size - 1) // block_size)  \n        # If not a multiple of block_size, pad up to the next multiple  \n        padded_size = n_blocks * block_size  \n  \n        # Reorder all actual tokens for expert e_id  \n        reordered_token_ids.extend(tokens_for_expert)  \n        # reordered_expert_ids.extend([e_id]*num_tokens)  \n        reordered_expert_ids.extend([e_id] * n_blocks)  \n  \n        # Pad with dummy token_id = -1 (or any sentinel), if needed  \n        if padded_size > num_tokens:  \n            pad_count = padded_size - num_tokens  \n            reordered_token_ids.extend([-1] * pad_count)  \n  \n    token_length = len(reordered_token_ids)  \n    expert_length = len(reordered_expert_ids)  \n  \n    sorted_token_ids[:token_length] = torch.tensor(reordered_token_ids, dtype=sorted_token_ids.dtype,  \n                                                   device=sorted_token_ids.device)  \n    expert_ids[:expert_length] = torch.tensor(reordered_expert_ids, dtype=expert_ids.dtype, device=expert_ids.device)  \n  \n    # Fill remainder with -1 if these arrays are bigger than total_length  \n    if token_length < sorted_token_ids.numel():  \n        sorted_token_ids[token_length:] = -1  \n    if expert_length < expert_ids.numel():  \n        expert_ids[expert_length:] = -1  \n  \n    num_tokens_post_pad.fill_(token_length)  \n  \n  \ndef moe_align_block_size(topk_ids: torch.Tensor, block_size: int,  \n                         num_experts: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:  \n    \"\"\"  \n    Aligns the token distribution across experts to be compatible with block size for matrix multiplication.  \n  \n    Parameters:  \n    - topk_ids: A tensor of shape [total_tokens, top_k] representing the top-k expert indices for each token.  \n    - block_size: The block size used in block matrix multiplication.  \n    - num_experts: The total number of experts.  \n  \n    Returns:  \n    - sorted_token_ids: A tensor containing the sorted token indices according to their allocated expert.  \n    - expert_ids: A tensor indicating the assigned expert index for each block.  \n    - num_tokens_post_padded: The total number of tokens after padding, ensuring divisibility by block_size.  \n  \n    This function pads the number of tokens that each expert needs to process so that it is divisible by block_size.  \n    Padding ensures that during block matrix multiplication, the dimensions align correctly.  \n  \n    Example:  \n    Given topk_ids = [[2, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 3]], block_size = 4, and num_experts = 4:  \n    - We initially have 12 tokens (after repeating 'top_k' times) and 4 experts, with each expert needing to process 3 tokens.  \n    - As block_size is 4, we pad 1 token for each expert.  \n    - First, flatten topk_ids to [2, 3, 4, 1, 2, 4, 1, 3, 4, 1, 2, 3].  \n    - Then append padding tokens [12, 12, 12, 12] for each block.  \n    - After sorting by expert index, we obtain token_ids [3, 6, 9, 12, 0, 4, 10, 12, 1, 7, 11, 12, 2, 5, 8, 12].  \n        Tokens 12 are non-existent (padding) and are ignored in the subsequent matrix multiplication.  \n    - The padding ensures that the total number of tokens is now divisible by block_size for proper block matrix operations.  \n    \"\"\"  \n    top_k = topk_ids.shape[1]  \n    sorted_ids = torch.empty((topk_ids.numel() + num_experts * (block_size - 1), ), dtype=torch.int32,  \n                             device=topk_ids.device)  \n    expert_ids = torch.empty((topk_ids.numel() + num_experts, ), dtype=torch.int32, device=topk_ids.device)  \n    sorted_ids.fill_(topk_ids.numel())  \n    num_tokens_post_pad = torch.empty((1), dtype=torch.int32, device=topk_ids.device)  \n    _moe_align_block_size(topk_ids, num_experts, top_k, block_size, sorted_ids, expert_ids, num_tokens_post_pad)  \n  \n    return sorted_ids, expert_ids, num_tokens_post_pad  \n  \n  \ndef get_config_dtype_str(dtype: torch.dtype, use_int8_w8a16: Optional[bool] = False,  \n                         use_int8_w8a8: Optional[bool] = False, use_fp8_w8a8: Optional[bool] = False):  \n    if use_fp8_w8a8:  \n        return \"fp8_w8a8\"  \n    elif use_int8_w8a16:  \n        return \"int8_w8a16\"  \n    elif use_int8_w8a8:  \n        return \"int8_w8a8\"  \n    elif dtype == torch.float:  \n        # avoiding cases where kernel fails when float32 MoE  \n        # use fp16/bfloat16 configs  \n        return \"float32\"  \n    return None  \n  \n  \ndef get_config_file_name(dtype: Optional[str]) -> str:  \n    device_name = torch.cuda.get_device_name(0).replace(\" \", \"_\")  \n    dtype_selector = \"\" if not dtype else f\",dtype={dtype}\"  \n    return f\"device_name={device_name}{dtype_selector}.json\"  \n  \n  \n@functools.lru_cache  \ndef get_moe_configs(dtype: Optional[str]) -> Optional[Dict[int, Any]]:  \n    \"\"\"  \n    Return optimized configurations for the fused MoE kernel.  \n  \n    The return value will be a dictionary that maps an irregular grid of  \n    batch sizes to configurations of the fused_moe kernel. To evaluate the  \n    kernel on a given batch size bs, the closest batch size in the grid should  \n    be picked and the associated configuration chosen to invoke the kernel.  \n    \"\"\"  \n    # First look up if an optimized configuration is available in the configs  \n    # directory  \n    json_file_name = get_config_file_name(dtype)  \n  \n    config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"configs\", json_file_name)  \n    if os.path.exists(config_file_path):  \n        with open(config_file_path) as f:  \n            # If a configuration has been found, return it  \n            return {key: val for key, val in json.load(f).items()}  \n  \n    # If no optimized configuration is available, we will use the default  \n    # configuration  \n    return None  \n  \n  \ndef get_default_config(  \n    M: int,  \n    E: int,  \n    is_marlin: bool,  \n) -> Dict[str, int]:  \n    config = {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}  \n    # A heuristic: fused marlin works faster with this config for small M  \n    if M <= E or (is_marlin and M <= 32):  \n        config = {'BLOCK_SIZE_M': 16, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1}  \n    return config  \n  \n  \ndef try_get_optimal_moe_config(  \n    E: int,  \n    dtype: Optional[str],  \n    M: int,  \n    is_marlin: bool = False,  \n):  \n    configs = get_moe_configs(dtype)  \n  \n    if configs:  \n        if configs: # This inner 'if configs:' is redundant  \n            if M < M_THRESHOLD_SMALL:  \n                config = configs[\"small_M\"]  \n            elif M < M_THRESHOLD_MEDIUM:  \n                config = configs[\"medium_M\"]  \n            else:  \n                config = configs[\"large_M\"]  \n    else:  \n        # Else use the default config  \n        config = get_default_config(M, E, is_marlin)  \n  \n    return config  \n  \n  \ndef moe_gemm(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, metadata: MetaData) -> None:  \n    # TODO shard M dim  \n    metadata.check_args(a, b, c)  \n  \n    num_tokens_post_padded, topk_weights, sorted_token_ids, expert_ids, config = metadata.num_tokens_post_padded, metadata.topk_weights, metadata.sorted_token_ids, metadata.expert_ids, metadata.config  \n  \n    use_fp8_w8a8, use_int8_w8a16, use_int8_w8a8 = metadata.use_fp8_w8a8, metadata.use_int8_w8a16, metadata.use_int8_w8a8  \n    a_descale, b_descale = None, None  \n    stride_bse = None  \n    stride_bsn = None  \n    if use_fp8_w8a8 or use_int8_w8a16 or use_int8_w8a8:  \n        a_descale, b_descale = metadata.a_descale, metadata.b_descale  \n        if use_int8_w8a16:  \n            stride_bse = b_descale.stride(0)  \n            stride_bsn = b_descale.stride(1)  \n  \n    top_k = metadata.top_k  \n  \n    EM = num_tokens_post_padded.item()  \n    _, N, K = b.shape  \n    grid = lambda META: (triton.cdiv(EM, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )  \n  \n    EVEN_K = K % config[\"BLOCK_SIZE_K\"] == 0  \n      \n    # This is where the kernel defined above is called  \n    # We need to ensure moe_gemm_kernel is in scope, which it is if defined globally or imported.  \n    # For this re-structuring, it's assumed the kernel from the <triton-kernel-code> block is accessible.  \n    # If running this directly, you might need to ensure the kernel definition is executed first.  \n    moe_gemm_kernel[grid](a, b, c, a_descale,  \n                          b_descale, a.stride(0), a.stride(1), b.stride(0), b.stride(1), b.stride(2), c.stride(1),  \n                          c.stride(2), stride_bse, stride_bsn, top_k, topk_weights, sorted_token_ids, expert_ids, EM, N,  \n                          K, EVEN_K, MUL_ROUTED_WEIGHT=topk_weights is not None, use_fp8_w8a8=use_fp8_w8a8,  \n                          use_int8_w8a16=use_int8_w8a16, use_int8_w8a8=use_int8_w8a8, **config)  \n    return c  \n  \n  \ndef quantize_tensor(tensor: torch.Tensor, dtype, dim=()) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:  \n    quantize_dim = [i for i in range(tensor.dim()) if i not in dim]  \n    max_vals = tensor.abs().amax(dim=quantize_dim, keepdim=True)  \n    max_repr_val = dtype_max[dtype]  \n    # Avoid division by zero  \n    max_vals[max_vals == 0] = 1e-8  \n  \n    # Compute scale factors for each channel  \n    scale: torch.Tensor = max_repr_val / max_vals.to(torch.float32)  \n  \n    # Quantize the tensor  \n    tensor = tensor * scale  \n    if dtype == torch.int8:  \n        tensor = tensor.round_()  \n    tensor.clamp_(-max_repr_val, max_repr_val)  \n    tensor_quantized = tensor.to(dtype)  \n  \n    scale = scale.squeeze(dim=quantize_dim)  \n  \n    return tensor_quantized, scale, 1 / scale  \n  \n  \ndef quantize_input(a, b, use_fp8_w8a8: tl.constexpr, use_int8_w8a16: tl.constexpr, use_int8_w8a8: tl.constexpr,  \n                   metatdata: MetaData, fp8_type=None):  \n    assert not (use_fp8_w8a8 and use_int8_w8a16 and use_int8_w8a8)  \n    assert not (use_fp8_w8a8 and fp8_type is None)  \n  \n    if use_fp8_w8a8:  \n        a_quantized, _, a_descale = quantize_tensor(a, dtype=fp8_type)  \n        b_quantized, _, b_descale = quantize_tensor(b, dim=(0, ), dtype=fp8_type)  \n        metatdata.set_use_fp8_w8a8(a_descale, b_descale, fp8_type)  \n        return a_quantized, b_quantized  \n  \n    if use_int8_w8a8:  \n        a_quantized, _, a_descale = quantize_tensor(a, dtype=torch.int8)  \n        b_quantized, _, b_descale = quantize_tensor(b, dim=(0, ), dtype=torch.int8)  \n        metatdata.set_use_int8_w8a8(a_descale, b_descale)  \n        return a_quantized, b_quantized  \n  \n    if use_int8_w8a16:  \n        b_quantized, _, b_descale = quantize_tensor(b, dim=(0, 1), dtype=torch.int8)  \n        metatdata.set_use_int8_w8a16(b_descale)  \n        return a, b_quantized  \n  \n  \ndef input_helper(M: int, N: int, K: int, top_k: int, E: int, routed_weight: bool, use_fp8_w8a8: bool,  \n                 use_int8_w8a16: bool, use_int8_w8a8: bool, fp8_type, dtype):  \n\n    set_seed()\n    a = torch.randn((M, K), dtype=dtype, device='cuda')  \n    b = torch.randn((E, N, K), dtype=dtype, device='cuda')  \n    c = torch.zeros((M, top_k, N), dtype=dtype, device='cuda')  \n  \n    values = torch.randn(M, E, device='cuda')  \n  \n    softmax_vals = torch.softmax(values, dim=1)  \n    topk_weights, topk_ids = torch.topk(softmax_vals, k=top_k, dim=1)  \n  \n    config_dtype = get_config_dtype_str(use_fp8_w8a8=use_fp8_w8a8, use_int8_w8a16=use_int8_w8a16,  \n                                        use_int8_w8a8=use_int8_w8a8, dtype=dtype)  \n    get_config_func = functools.partial(  \n        try_get_optimal_moe_config,  \n        E,  \n        config_dtype,  \n    )  \n    config = get_config_func(M)  \n    sorted_token_ids, expert_ids, num_tokens_post_padded = moe_align_block_size(topk_ids, config['BLOCK_SIZE_M'], E)  \n  \n    metadata = MetaData(top_k, topk_weights if routed_weight else None, topk_ids, sorted_token_ids, expert_ids,  \n                        num_tokens_post_padded, config)  \n  \n    if use_fp8_w8a8 or use_int8_w8a16 or use_int8_w8a8:  \n        a, b = quantize_input(a, b, use_fp8_w8a8, use_int8_w8a16, use_int8_w8a8, metadata, fp8_type)  \n  \n    return a, b, c, metadata  \n  \n  \n@pytest.mark.parametrize(\"M, N, K, top_k, E\", [  \n    (64, 14336, 4096, 2, 8),  \n    (16, 14336, 1, 2, 4),  \n    (1, 14336, 128, 2, 4),  \n    (3, 14336, 128, 2, 4),  \n    (16, 14336, 128, 1, 4),  \n    (16, 14336, 128, 1, 1),  \n    (64, 7186, 128, 2, 8),  \n    (64, 3584, 128, 2, 8),  \n    (64, 1792, 128, 2, 8),  \n    (64, 64, 128, 2, 8),  \n])  \n@pytest.mark.parametrize('routed_weight', [True, False])  \ndef test_correctness(M: int, N: int, K: int, top_k: int, E: int, routed_weight: bool, request, dtype=torch.float16):  \n    \n    a, b, c, metadata = input_helper(M, N, K, top_k, E, routed_weight=routed_weight, use_fp8_w8a8=False,  \n                                     use_int8_w8a16=False, use_int8_w8a8=False, fp8_type=None, dtype=dtype)  \n  \n    tri_out = moe_gemm(a, b, c, metadata)  \n\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = tri_out.clone().detach().cpu()\n    ###################################################################\n    \n    topk_ids = metadata.topk_ids  \n    topk_weights = metadata.topk_weights  \n    ref_out = torch.empty_like(c)  \n    # Repeat a -> (M, top_k, K)  \n    a_expanded = a.unsqueeze(1).repeat(1, top_k, 1)  \n    # (M, top_k, N, K)  \n    b_indexed = b[topk_ids]  \n    ref_out = torch.einsum(\"mek,menk->men\", a_expanded, b_indexed)  \n    if routed_weight:  \n        ref_out *= topk_weights.unsqueeze(-1)  \n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    # Validate correctness  \n    torch.testing.assert_close(tri_out, ref_out, atol=1e-2, rtol=1e-2)  \n  \n  \n@pytest.mark.parametrize(\"M, N, K, top_k, E\", [  \n    (64, 14336, 4096, 2, 8),  \n    (16, 14336, 1, 2, 4),  \n    (1, 14336, 128, 2, 4),  \n    (16, 14336, 128, 1, 4),  \n    (16, 14336, 128, 1, 1),  \n    (64, 7186, 128, 2, 8),  \n    (64, 3584, 128, 2, 8),  \n    (64, 1792, 128, 2, 8),  \n    (64, 64, 128, 2, 8),  \n])  \n@pytest.mark.parametrize('routed_weight', [True, False])  \n@pytest.mark.parametrize('use_fp8_w8a8', [True])  \n@pytest.mark.parametrize('fp8_type', [torch.float8_e4m3fnuz, torch.float8_e5m2fnuz])  \ndef test_correctness_fp8(M: int, N: int, K: int, top_k: int, E: int, routed_weight: bool, use_fp8_w8a8, fp8_type, request, dtype=torch.float16): \n\n    a, b, c, metadata = input_helper(M, N, K, top_k, E, routed_weight=routed_weight, use_fp8_w8a8=use_fp8_w8a8,  \n                                     use_int8_w8a16=False, fp8_type=fp8_type, use_int8_w8a8=False, dtype=dtype)  \n  \n    tri_out = moe_gemm(a, b, c, metadata)\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = tri_out.clone().detach().cpu()\n    ###################################################################\n\n    topk_ids = metadata.topk_ids  \n    topk_weights = metadata.topk_weights  \n    ref_out = torch.empty_like(c)  \n    # Repeat a -> (M, top_k, K)  \n    a_expanded = a.unsqueeze(1).repeat(1, top_k, 1)  \n    # (M, top_k, N, K)  \n    b_indexed = b.half()[topk_ids]  \n    ref_out = torch.einsum(\"mek,menk->men\", a_expanded.float(), b_indexed.float())  \n  \n    if routed_weight:  \n        ref_out *= topk_weights.unsqueeze(-1)  \n  \n    ref_out = ref_out * metadata.b_descale[topk_ids].unsqueeze(-1)  \n    ref_out = ref_out * metadata.a_descale  \n    ref_out = ref_out.to(dtype)  \n  \n    # Validate correctness  \n    torch.testing.assert_close(tri_out, ref_out, atol=1e-2, rtol=1e-2)  \n  \n  \n@pytest.mark.parametrize(\"M, N, K, top_k, E\", [  \n    (64, 14336, 4096, 2, 8),  \n    (16, 14336, 1, 2, 4),  \n    (1, 14336, 128, 2, 4),  \n    (16, 14336, 128, 1, 4),  \n    (16, 14336, 128, 1, 1),  \n    (64, 7186, 128, 2, 8),  \n    (64, 3584, 128, 2, 8),  \n    (64, 1792, 128, 2, 8),  \n    (64, 64, 128, 2, 8),  \n])  \n@pytest.mark.parametrize('routed_weight', [True, False])  \n@pytest.mark.parametrize('use_int8_w8a16', [True])  \ndef test_correctness_int8_w8a16(M: int, N: int, K: int, top_k: int, E: int, routed_weight: bool, use_int8_w8a16, request,  \n                                dtype=torch.float16):  \n    a, b, c, metadata = input_helper(M, N, K, top_k, E, routed_weight=routed_weight, use_fp8_w8a8=False,  \n                                     use_int8_w8a16=use_int8_w8a16, use_int8_w8a8=False, fp8_type=None, dtype=dtype)  \n  \n    tri_out = moe_gemm(a, b, c, metadata)  \n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = tri_out.clone().detach().cpu()\n    ###################################################################\n    \n    topk_ids = metadata.topk_ids  \n    topk_weights = metadata.topk_weights  \n    ref_out = torch.empty_like(c)  \n    # Repeat a -> (M, top_k, K)  \n    a_expanded = a.unsqueeze(1).repeat(1, top_k, 1)  \n    # (M, top_k, N, K)  \n    b_indexed = b[topk_ids]  \n    ref_out = torch.einsum(\"mek,menk->men\", a_expanded.float(), b_indexed.float())  \n    if routed_weight:  \n        ref_out *= topk_weights.unsqueeze(-1)  \n  \n    ref_out = ref_out * metadata.b_descale[topk_ids, :]  \n    ref_out = ref_out.to(dtype)  \n  \n    # Validate correctness  \n    torch.testing.assert_close(tri_out, ref_out, atol=1e-2, rtol=1e-2)  \n  \n  \n@pytest.mark.parametrize(\"M, N, K, top_k, E\", [  \n    (64, 14336, 4096, 2, 8),  \n    (16, 14336, 1, 2, 4),  \n    (1, 14336, 128, 2, 4),  \n    (16, 14336, 128, 1, 4),  \n    (16, 14336, 128, 1, 1),  \n    (64, 7186, 128, 2, 8),  \n    (64, 3584, 128, 2, 8),  \n    (64, 1792, 128, 2, 8),  \n    (64, 64, 128, 2, 8),  \n])  \n@pytest.mark.parametrize('routed_weight', [True, False])  \n@pytest.mark.parametrize('use_int8_w8a8', [True])  \ndef test_correctness_int8_w8a8(M: int, N: int, K: int, top_k: int, E: int, routed_weight: bool, use_int8_w8a8, request,  \n                               dtype=torch.float16):    \n    a, b, c, metadata = input_helper(M, N, K, top_k, E, routed_weight=routed_weight, use_fp8_w8a8=False,  \n                                     use_int8_w8a16=False, use_int8_w8a8=use_int8_w8a8, fp8_type=None, dtype=dtype)  \n  \n    tri_out = moe_gemm(a, b, c, metadata)  \n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = tri_out.clone().detach().cpu()\n    ###################################################################\n    \n    topk_ids = metadata.topk_ids  \n    topk_weights = metadata.topk_weights  \n    ref_out = torch.empty_like(c)  \n    # Repeat a -> (M, top_k, K)  \n    a_expanded = a.unsqueeze(1).repeat(1, top_k, 1)  \n    # (M, top_k, N, K)  \n    b_indexed = b[topk_ids]  \n    ref_out = torch.einsum(\"mek,menk->men\", a_expanded.float(), b_indexed.float())  \n    if routed_weight:  \n        ref_out *= topk_weights.unsqueeze(-1)  \n  \n    ref_out = ref_out * metadata.b_descale[topk_ids].unsqueeze(-1)  \n    ref_out = ref_out * metadata.a_descale  \n    ref_out = ref_out.to(dtype)  \n  \n    \n    # Validate correctness  \n    torch.testing.assert_close(tri_out, ref_out, atol=1e-2, rtol=1e-2)  \n\n# --- Define TFLOPS and GB/s calculators for MoE GEMM ---\ndef calculate_moe_gemm_tflops(params: dict, ms: float) -> float:\n    M = params['M_orig'] # Original number of tokens before top_k expansion\n    N = params['N']\n    K = params['K']\n    top_k = params['top_k']\n    # Each of M tokens interacts with top_k experts.\n    # For each such interaction, it's an M_slice * K @ K * N GEMM.\n    # Effective operations: M * top_k * (2 * K * N)\n    flops = M * top_k * (2 * K * N)\n    if params.get('routed_weight', False):\n        flops += M * top_k * N # Element-wise multiplication by routing weights\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_moe_gemm_gbps(params: dict, ms: float) -> float:\n    M_orig = params['M_orig']\n    N = params['N']\n    K = params['K']\n    E = params['E']\n    top_k = params['top_k']\n    dtype_str = params.get('dtype_str', 'fp16') # Default if not specified\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    # Memory I/O:\n    # Read A: (M_orig, K)\n    # Read B (all expert weights): (E, N, K) - worst case, all loaded\n    # Read routing info: topk_ids (M_orig, top_k), expert_ids (complex, related to padded M), topk_weights (M_orig, top_k)\n    # Write C: (M_orig, top_k, N) -> effectively M_orig * top_k rows of size N are written.\n\n    bytes_a = M_orig * K * element_size\n    bytes_b = E * N * K * element_size # All expert weights\n    # Output c_for_kernel is (EM_padded, N). EM_padded is roughly M_orig * top_k.\n    # Let's use M_orig * top_k for a cleaner estimate of useful data written.\n    bytes_c_written = M_orig * top_k * N * element_size\n\n    # Routing info bytes are usually smaller and sometimes omitted for simplicity,\n    # but can be significant for small M, N, K.\n    # topk_ids: M_orig * top_k * 4 (int32)\n    # expert_ids: (num_blocks_for_kernel) * 4 (int32) - harder to estimate precisely without full alignment logic\n    # topk_weights: M_orig * top_k * element_size (if routed_weight)\n    bytes_routing = M_orig * top_k * 4 # topk_ids\n    if params.get('routed_weight', False):\n        bytes_routing += M_orig * top_k * element_size\n\n    # For MoE, a common simplification is A + B_active + C\n    # B_active = M_orig * top_k * K * N (effectively) but data comes from (E,N,K)\n    # Let's use: Read A, Read all B, Write C_useful\n    total_bytes = bytes_a + bytes_b + bytes_c_written # Add bytes_routing if significant\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"moe_gemm_triton_perf\"\n\n# --- Pytest parametrize for performance testing (based on test_correctness) ---\nMOE_GEMM_PARAMS_FOR_PERF = [\n    # M, N, K, top_k, E\n    (64, 14336, 4096, 2, 8),\n    (256, 7168, 4096, 2, 8), # Example medium size\n    (1024, 14336, 4096, 2, 8), # Example larger M\n    (16, 1024, 512, 1, 4),   # Smaller K, N\n]\nMOE_DTYPES_FOR_PERF = ['fp16', 'bf16'] # Reduced set for faster perf testing\n# Quantization modes are complex to integrate here simply, focus on main dtypes first.\n\n@pytest.mark.parametrize(\"M_orig, N, K, top_k, E\", MOE_GEMM_PARAMS_FOR_PERF)\n@pytest.mark.parametrize('routed_weight', [True, False])\n@pytest.mark.parametrize('dtype_str', MOE_DTYPES_FOR_PERF)\ndef test_performance(M_orig, N, K, top_k, E, routed_weight, dtype_str, request):\n    # Determine torch dtype\n    set_seed()\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    # --- Input Setup using input_helper ---\n    # For performance, we are not testing quantization variants like fp8/int8 here for simplicity.\n    # That would require more parameters for use_fp8_w8a8, etc.\n    a, b, c_for_kernel, metadata = input_helper(\n        M=M_orig, N=N, K=K, top_k=top_k, E=E,\n        routed_weight=routed_weight,\n        use_fp8_w8a8=False, use_int8_w8a16=False, use_int8_w8a8=False,\n        fp8_type=None, dtype=current_dtype\n    )\n\n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: moe_gemm(a, b, c_for_kernel, metadata)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=10, repetition=50) # MoE can be slower\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M_orig\": M_orig, \"N\": N, \"K\": K, \"top_k\": top_k, \"E\": E,\n        \"routed_weight\": routed_weight, \"dtype_str\": dtype_str,\n        # Include relevant metadata.config if it affects performance\n        \"BLOCK_SIZE_M\": metadata.config['BLOCK_SIZE_M'],\n        \"BLOCK_SIZE_N\": metadata.config['BLOCK_SIZE_N'],\n        \"BLOCK_SIZE_K\": metadata.config['BLOCK_SIZE_K'],\n        \"GROUP_SIZE_M\": metadata.config['GROUP_SIZE_M'],\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_moe_gemm_gbps,\n                              tflops_calculator=calculate_moe_gemm_tflops)\n    \n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all tri_out results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} tri_out tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n######################################## HELPERS for Eval ######################################## \n\n\ndef get_configs():  \n    configs = [  \n        {\"M\": 64, \"N\": 256, \"K\": 128, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 64, \"N\": 1792, \"K\": 1024, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 64, \"N\": 7168, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 128, \"N\": 7168, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 1024, \"N\": 7168, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 4096, \"N\": 7168, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 64, \"N\": 14336, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 128, \"N\": 14336, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 256, \"N\": 14336, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 512, \"N\": 14336, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 1024, \"N\": 14336, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 2048, \"N\": 14336, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n        {\"M\": 4096, \"N\": 14336, \"K\": 4096, \"E\": 8, \"top_k\": 2},  \n    ]  \n    return configs  \n  \n  \ndef model_benchmark_configs(args):  \n    config_file = args.model_configs  \n    configs = get_model_configs(config_path=config_file, model_families=[\"mistral\"], model=args.model)  \n    moe_configs = []  \n    M = args.M if args.M else 4096  # check size  \n    # M, K, N, E, top_k  \n  \n    for model_name, config in configs.items():  \n        N1 = config[\"intermediate_size\"]  \n        K1 = config[\"hidden_size\"]  \n  \n        N2 = config[\"hidden_size\"]  \n        K2 = config[\"intermediate_size\"] // 2  \n  \n        E = 8  \n        top_k = 2  \n        # The first moe layer  \n        moe_configs.append((model_name, M, N1, K1, E, top_k))  \n        # The second moe layer  \n        moe_configs.append((model_name, M * top_k, N2, K2, E, 1))  \n  \n    return moe_configs  \n  \n  \ndef run_benchmark(custom, args):  \n    routed_weight = args.routed_weight  \n    use_int8_w8a16 = args.int8_w8a16  \n    use_fp8_w8a8 = args.fp8_w8a8  \n    use_int8_w8a8 = args.int8_w8a8  \n    dtype = arg_to_torch_dtype[args.dtype]  \n    fp8_type = arg_to_torch_dtype[args.fp8_type]  \n  \n    x_names = ['M', 'N', 'K', 'E', 'top_k']  \n    if custom:\n        assert args.M and args.N and args.K and args.E and args.top_k, \\\n            \"Please provide M, N, K, E, top_k for custom runs.\"\n        x_vals_list = [(args.M, args.N, args.K, args.E, args.top_k)] \n    else:  \n        if args.model:  \n            x_vals_list = model_benchmark_configs(args)  \n            x_names = ['model', 'M', 'N', 'K', 'E', 'top_k']  \n        else:  \n            configs = get_configs()  \n            x_vals_list = [(cfg['M'], cfg['N'], cfg['K'], cfg['E'], cfg['top_k']) for cfg in configs]  \n  \n    line_names = ['Time (ms)', 'TFLOPS', 'Bandwidth (GB/s)']  \n    line_vals = ['time', 'tflops', 'bandwidth']  \n  \n    benchmark = triton.testing.Benchmark(  \n        x_names=x_names, x_vals=x_vals_list, line_arg='metric', line_vals=line_vals, line_names=line_names,  \n        styles=[('red', '-'), ('blue', '-'),  \n                ('yellow', '-')], ylabel='ms / TFLOPS / GB/s', plot_name='moe-gemm-benchmark', args={  \n                    'dtype': dtype, 'routed_weight': routed_weight, 'use_fp8_w8a8': use_fp8_w8a8, 'use_int8_w8a16':  \n                    use_int8_w8a16, 'use_int8_w8a8': use_int8_w8a8, 'fp8_type': fp8_type  \n                })  \n  \n    @triton.testing.perf_report([benchmark])  \n    def bench_moe_gemm(M, N, K, E, top_k, dtype, routed_weight, metric, use_fp8_w8a8, use_int8_w8a16, use_int8_w8a8,  \n                       fp8_type, model=None):  \n        a, b, c, metadata = input_helper(M, N, K, top_k, E, routed_weight=routed_weight, use_fp8_w8a8=use_fp8_w8a8,  \n                                         use_int8_w8a16=use_int8_w8a16, use_int8_w8a8=use_int8_w8a8, fp8_type=fp8_type,  \n                                         dtype=dtype)  \n  \n        # (M, K) * (top_k, N, K) -> (M, top_k, N). 2 for multiplication and accumulation  \n        flops = 2.0 * M * top_k * K * N  \n        # The weight is applied on the gemm product which has the shape of (M, top_k, N)  \n        if routed_weight:  \n            flops += M * top_k * N  \n  \n        if use_fp8_w8a8:  \n            a_bytes = b_bytes = torch.tensor([], dtype=fp8_type).element_size()  \n            c_bytes = torch.tensor([], dtype=dtype).element_size()  \n        if use_int8_w8a8: # This should be elif  \n            a_bytes = b_bytes = torch.tensor([], dtype=torch.int8).element_size()  \n            c_bytes = torch.tensor([], dtype=torch.int8).element_size() # This was torch.int8, should be dtype for c  \n        elif use_int8_w8a16:  \n            b_bytes = torch.tensor([], dtype=torch.int8).element_size()  \n            a_bytes = c_bytes = torch.tensor([], dtype=dtype).element_size()  \n        else:  \n            a_bytes = b_bytes = c_bytes = torch.tensor([], dtype=dtype).element_size()  \n  \n        # (M, K) memory load for A (E,  N,  K) for B not (top_k,  N,  K) because we are in total bringing in all expert matrices into the chip from memory. It's just that not all multiply the same A.  \n        mem_read = (M * K) * a_bytes + (E * N * K) * b_bytes  \n        # Memory write for the gemm product  \n        mem_write = (M * top_k * N) * c_bytes  \n        mem = mem_read + mem_write  \n        fn = lambda: moe_gemm(a, b, c, metadata)  \n        ms = triton.testing.do_bench(fn)  \n  \n        bandwidth = mem / (ms * 1e-3) * 1e-9  # GB/s  \n        tflops = flops / ms * 1e-9  \n\n        ############ BENCHMARK LOGGING ############\n        log_entry = f\"ms: {ms} bandwidth: {bandwidth} tflops: {tflops}\" + '\\n'\n        OUTPUT_FILENAME = __file__.replace('.','_') + '_benchmark.log'\n        with open(OUTPUT_FILENAME, 'a', encoding='utf-8') as f:\n            f.write(log_entry)\n        ############ BENCHMARK LOGGING ############\n        \n        # Return exactly one scalar depending on which metric is active  \n        if metric == 'time':  \n            return ms  \n        elif metric == 'tflops':  \n            return tflops  \n        elif metric == 'bandwidth':  \n            return bandwidth  \n        else:  \n            raise ValueError(\"Unknown metric: \" + metric)  \n  \n    bench_moe_gemm.run(save_path=\".\", print_data=True)  \n  \n  \ndef parse_args():  \n    parser = argparse.ArgumentParser(  \n        prog=\"Benchmark MoE GEMM\",  \n        allow_abbrev=False,  \n    )  \n    parser.add_argument('-model_configs', type=str, default=\"model_configs.json\", help=\"Model config json file.\")  \n    available_models = get_available_models(model_families=[\"mistral\"])  # Dynamically load model names  \n    model_help = (\"Model name to benchmark. Select from: [\" + \", \".join(available_models) +  \n                  \"]. Use 'all' to benchmark all models or leave blank for the default benchmark script.\")  \n    parser.add_argument('-model', type=str, default=None, help=model_help)  \n    parser.add_argument(\"-M\", type=int, default=0, help=\"M dimension\")  \n    parser.add_argument(\"-K\", type=int, default=0, help=\"K dimension\")  \n    parser.add_argument(\"-N\", type=int, default=0, help=\"N dimension\")  \n    parser.add_argument(\"-E\", type=int, default=0, help=\"Number of experts\")  \n    parser.add_argument(\"-top_k\", type=int, default=0, help=\"top_k experts per token\")  \n    parser.add_argument(\"-routed_weight\", action='store_true', default=False)  \n    parser.add_argument(\"-int8_w8a16\", action='store_true', default=False)  \n    parser.add_argument(\"-int8_w8a8\", action='store_true', default=False)  \n    parser.add_argument(\"-fp8_w8a8\", action='store_true', default=False)  \n    parser.add_argument(\"-dtype\", default='fp16')  \n    parser.add_argument(\"-fp8_type\", default='e5m2fnuz')  \n    args = parser.parse_args()  \n    return args  \n  \n  \narg_to_torch_dtype = {  \n    'fp16': torch.float16, 'bf16': torch.bfloat16, 'fp32': torch.float32, \"e5m2fnuz\": torch.float8_e5m2fnuz, \"e4m3fnuz\":  \n    torch.float8_e4m3fnuz  \n}  \n  \n  \ndef main():  \n    args = parse_args()  \n    custom_config = False  \n    # If user provides all M,K,N,E,top_k we consider it custom  \n    if args.M and args.K and args.N and args.E and args.top_k:  \n        custom_config = True  \n    run_benchmark(custom_config, args)\n\nif __name__ == '__main__':  \n    sys.exit(main())  \n"
    },
    {
        "file": "multreduce_matmul_dot_kernel.py",
        "target_kernel_name": "triton_dot_matmul_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given a instruction/function definition of the required kernel : `triton_dot_matmul_kernel`, your task is to complete the kernel code for the corresponding operator/function definition using triton programming language. This kernel should implement a General Matrix Multiplication (GEMM) specifically using the tl.dot operation in triton and add necessary logic to use it. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list ,only add if required. :\nProvide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\n\nThe full definition for `triton_dot_matmul_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `triton_dot_matmul_kernel` whilst keeping other things.\n\n\nimport argparse\nimport itertools\nimport os\nimport sys\nfrom typing import Any, Callable, Optional\n\nimport pytest\nimport torch\nfrom torch import Tensor\n\nimport triton\nimport triton.language as tl\n\n######################## HELPER UTILS #####################\n\nAutotune configurations for Triton GEMM implemented with tl.dot.\n\ndef get_triton_dot_autotune_configs() -> list[triton.Config]:\nblock_size_n_range: list[int] = [16, 32]\nblock_size_k_range: list[int] = [128, 256, 512]\nkpack_range: list[int] = [1, 2]\nnum_warps_range: list[int] = [1, 2]\nreturn [\ntriton.Config(\n{\n\"BLOCK_SIZE_M\": 16, \"BLOCK_SIZE_N\": block_size_n, \"BLOCK_SIZE_K\": block_size_k, \"waves_per_eu\": 0,\n\"matrix_instr_nonkdim\": 16, \"kpack\": kpack\n}, num_warps=num_warps, num_stages=2) for block_size_n, block_size_k, kpack, num_warps in itertools.product(\nblock_size_n_range, block_size_k_range, kpack_range, num_warps_range)\n]\n\ndef get_triton_autotune_key() -> list[str]:\nreturn [\"M\", \"N\", \"K\"]\n\ndef get_triton_heuristics() -> dict[str, Callable[[dict[str, Any]], Any]]:\nreturn {\"EVEN_K\": lambda args: args[\"K\"] % args[\"BLOCK_SIZE_K\"] == 0}\n\n###############################################################\n\n@triton.autotune(configs=get_triton_dot_autotune_configs(), key=get_triton_autotune_key())\n@triton.heuristics(get_triton_heuristics())\n@triton.jit\ndef triton_dot_matmul_kernel(a_ptr, b_ptr, c_ptr, bias_ptr,  #\nM: int, N: int, K: int,  #\nstride_am: int, stride_ak: int,  #\nstride_bk: int, stride_bn: int,  #\nstride_cm: int, stride_cn: int,  #\nstride_bias: int,  #\nBLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\nUSE_BIAS: tl.constexpr, EVEN_K: tl.constexpr  #\n):\n\"\"\"\nPerforms a General Matrix Multiplication (GEMM) of the form C = A @ B + bias.\nThis kernel is specifically designed to use the tl.dot operation for the\ncore matrix multiplication.\n\nParameters:\n- a_ptr: Pointer to the A matrix (input).\n- b_ptr: Pointer to the B matrix (input).\n- c_ptr: Pointer to the C matrix (output).\n- bias_ptr: Pointer to the bias vector/matrix. Used only if USE_BIAS is True.\n- M: Number of rows in matrix A and C.\n- N: Number of columns in matrix B and C.\n- K: Number of columns in matrix A and rows in matrix B (common dimension).\n- stride_am: Stride for matrix A along the M dimension (row stride).\n- stride_ak: Stride for matrix A along the K dimension (column stride).\n- stride_bk: Stride for matrix B along the K dimension (row stride).\n- stride_bn: Stride for matrix B along the N dimension (column stride).\n- stride_cm: Stride for matrix C along the M dimension (row stride).\n- stride_cn: Stride for matrix C along the N dimension (column stride).\n- stride_bias: Stride for the bias. Interpretation depends on bias dimensions.\n- BLOCK_SIZE_M: tl.constexpr, tile size for the M dimension during computation.\n- BLOCK_SIZE_N: tl.constexpr, tile size for the N dimension during computation.\n- BLOCK_SIZE_K: tl.constexpr, tile size for the K dimension during computation.\n- USE_BIAS: tl.constexpr, boolean flag indicating whether to add the bias term.\n- EVEN_K: tl.constexpr, boolean flag indicating if K is perfectly divisible by BLOCK_SIZE_K,\n            allowing for potentially more efficient, unmasked loads along the K dimension.\n\"\"\"\n# Your code here.\n\n",
        "label": "# Imports:  \n# --------  \n  \nimport argparse  \nimport itertools  \nimport os  \nimport sys  \nfrom typing import Any, Callable, Optional  \n  \nimport pytest  \nimport torch  \nfrom torch import Tensor  \n  \nimport triton  \nimport triton.language as tl  \n \n\n######################## HELPER UTILS #####################\n# Autotune configurations for Triton GEMM implemented with `tl.dot`.  \ndef get_triton_dot_autotune_configs() -> list[triton.Config]:  \n    block_size_n_range: list[int] = [16, 32]  \n    block_size_k_range: list[int] = [128, 256, 512]  \n    kpack_range: list[int] = [1, 2]  \n    num_warps_range: list[int] = [1, 2]  \n    return [  \n        triton.Config(  \n            {  \n                \"BLOCK_SIZE_M\": 16, \"BLOCK_SIZE_N\": block_size_n, \"BLOCK_SIZE_K\": block_size_k, \"waves_per_eu\": 0,  \n                \"matrix_instr_nonkdim\": 16, \"kpack\": kpack  \n            }, num_warps=num_warps, num_stages=2) for block_size_n, block_size_k, kpack, num_warps in itertools.product(  \n                block_size_n_range, block_size_k_range, kpack_range, num_warps_range)  \n    ]  \n  \n  \ndef get_triton_autotune_key() -> list[str]:  \n    return [\"M\", \"N\", \"K\"]  \n  \n  \ndef get_triton_heuristics() -> dict[str, Callable[[dict[str, Any]], Any]]:  \n    return {\"EVEN_K\": lambda args: args[\"K\"] % args[\"BLOCK_SIZE_K\"] == 0}  \n\n###############################################################\n# Triton GEMM:  \n# ------------  \n# Core Triton GEMM kernel.  \n@triton.jit  \ndef triton_matmul_kernel(a_ptr, b_ptr, c_ptr, bias_ptr,  #  \n                         M: int, N: int, K: int,  #  \n                         stride_am: int, stride_ak: int,  #  \n                         stride_bk: int, stride_bn: int,  #  \n                         stride_cm: int, stride_cn: int,  #  \n                         stride_bias: int,  #  \n                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #  \n                         USE_BIAS: tl.constexpr, USE_DOT: tl.constexpr, EVEN_K: tl.constexpr  #  \n                         ):  \n    # Compute program ID:  \n    pid = tl.program_id(axis=0)  \n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)  \n    pid_m = pid // num_pid_n  \n    pid_n = pid % num_pid_n  \n  \n    # Compute A and B base pointers:  \n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)  \n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)  \n    offs_k = tl.arange(0, BLOCK_SIZE_K)  \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak  \n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn  \n  \n    # Load BIAS:  \n    if USE_BIAS:  \n        bias_ptrs = bias_ptr + offs_am * stride_bias  \n        bias = tl.load(bias_ptrs, mask=offs_am < M, other=0)  \n  \n    # Initialize accumulator:  \n    acc_dtype = tl.float32 if a_ptr.type.element_ty != tl.int8 else tl.int32  \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=acc_dtype)  \n  \n    # GEMM loop:  \n  \n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):  \n        if EVEN_K:  \n            # Unmasked load of A and B:  \n            a = tl.load(a_ptrs)  \n            b = tl.load(b_ptrs)  \n        else:  \n            # Masked load of A and B:  \n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0)  \n            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0)  \n        # Compute dot product:  \n        if USE_DOT:  \n            accumulator += tl.dot(a, b)  \n        else:  \n            a = tl.reshape(a, (BLOCK_SIZE_M, BLOCK_SIZE_K, 1)).to(acc_dtype)  \n            b = tl.reshape(b, (1, BLOCK_SIZE_K, BLOCK_SIZE_N)).to(acc_dtype)  \n            accumulator += tl.sum(a * b, axis=1)  \n        # Advance A and B pointers:  \n        a_ptrs += BLOCK_SIZE_K * stride_ak  \n        b_ptrs += BLOCK_SIZE_K * stride_bk  \n  \n    # Convert accumulator back to C's type:  \n    c = accumulator.to(c_ptr.type.element_ty)  \n  \n    # Add BIAS:  \n    if USE_BIAS:  \n        c += bias[:, None]  \n  \n    # Compute C pointers and store C:  \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)  \n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)  \n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn  \n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)  \n    tl.store(c_ptrs, c, mask=c_mask)  \n  \n  \n# Triton GEMM kernel implemented with `tl.dot`.  \n@triton.autotune(configs=get_triton_dot_autotune_configs(), key=get_triton_autotune_key())  \n@triton.heuristics(get_triton_heuristics())  \n@triton.jit  \ndef triton_dot_matmul_kernel(a_ptr, b_ptr, c_ptr, bias_ptr,  #  \n                             M: int, N: int, K: int,  #  \n                             stride_am: int, stride_ak: int,  #  \n                             stride_bk: int, stride_bn: int,  #  \n                             stride_cm: int, stride_cn: int,  #  \n                             stride_bias: int,  #  \n                             BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #  \n                             USE_BIAS: tl.constexpr, EVEN_K: tl.constexpr  #  \n                             ):  \n    triton_matmul_kernel(a_ptr, b_ptr, c_ptr, bias_ptr,  #  \n                         M, N, K,  #  \n                         stride_am, stride_ak,  #  \n                         stride_bk, stride_bn,  #  \n                         stride_cm, stride_cn,  #  \n                         stride_bias,  #  \n                         BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,  #  \n                         USE_BIAS=USE_BIAS, USE_DOT=True, EVEN_K=EVEN_K)  \n \n\n  \n  \n##################################################################################################################################################   \n  \n# Test Triton GEMM, comparing it to PyTorch GEMM reference implementation:  \n######################################## HELPERS for Eval ######################################## \nimport numpy as np\nimport random\nimport torch \nimport argparse  \nimport itertools  \nimport os  \nimport sys  \nfrom typing import Any, Callable, Optional  \nimport pytest  \nimport torch  \nfrom torch import Tensor  \nimport triton  \nimport triton.language as tl  \nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n    \ndef triton_matmul(triton_provider: str, a: Tensor, b: Tensor, bias: Optional[Tensor]) -> Tensor:  \n    assert triton_provider in [\"triton-dot\"]  \n  \n    M: int  \n    N: int  \n    K: int  \n    M, K = a.shape  \n    _, N = b.shape  \n  \n    c: Tensor = torch.empty((M, N), device=a.device, dtype=a.dtype)  \n  \n    def grid(args: dict[str, Any]) -> tuple[int]:  \n        return (triton.cdiv(M, args[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, args[\"BLOCK_SIZE_N\"]), )  \n  \n    matmult_kernel = triton_dot_matmul_kernel\n  \n    matmult_kernel[grid](  \n        # Data pointers  \n        a,  \n        b,  \n        c,  \n        bias,  \n        # Size of matrices  \n        M,  \n        N,  \n        K,  \n        # Strides  \n        a.stride(0),  \n        a.stride(1),  \n        b.stride(0),  \n        b.stride(1),  \n        c.stride(0),  \n        c.stride(1),  \n        bias.stride(0) if bias is not None else 0,  \n        # Other kernel parameters  \n        USE_BIAS=bias is not None,  \n    )  \n  \n    return c  \n\n\n\n# PyTorch GEMM:  \n# -------------  \n  \n  \ndef torch_matmul(a: Tensor, b: Tensor, bias: Optional[Tensor]) -> Tensor:  \n    c: Tensor = torch.matmul(a, b)  \n    if bias is not None:  \n        c += bias[:, None]  \n    return c  \n\n\n\n# Wrapper for calling PyTorch GEMM or Triton GEMM:  \n# ------------------------------------------------  \n  \n  \ndef matmul(provider: str, a: Tensor, b: Tensor, bias: Optional[Tensor]) -> Tensor:  \n    assert provider in [\"torch\", \"triton-dot\"]  \n  \n    assert a.is_cuda, \"Matrix A must be in GPU.\"  \n    assert a.is_contiguous(), \"Matrix A must be contiguous.\"  \n    assert b.is_cuda, \"Matrix B must be in GPU.\"  \n    assert a.device == b.device, \"Matrix A and matrix B must be in the same GPU.\"  \n    assert a.dtype == b.dtype, \"Matrix A and matrix B must have the same data type.\"  \n    assert a.dim() == b.dim() == 2, \"Matrix A and matrix B must be two-dimensional tensors.\"  \n    assert a.shape[1] == b.shape[0], \"Matrix A columns must be equal to matrix B rows.\"  \n  \n    if bias is not None:  \n        assert bias.is_cuda, \"Bias vector must be in GPU.\"  \n        assert bias.is_contiguous(), \"Bias vector must be continuous.\"  \n        assert bias.device == a.device, \"Matrix A and bias vector must be in the same GPU.\"  \n        assert bias.dtype == a.dtype, \"Matrix A and bias vector must have the same data type.\"  \n        assert bias.dim() == 1, \"Bias vector must be one-dimensional tensor.\"  \n        assert bias.shape == (a.shape[0], ), \"Bias vector length must be equal to matrix A rows.\"  \n  \n    if provider == \"torch\":  \n        return torch_matmul(a, b, bias)  \n  \n    return triton_matmul(provider, a, b, bias)  \n\n    \n# Input generation:  \n# -----------------  \n  \n  \ndef gen_input(M: int, N: int, K: int, use_bias: bool, device: str = \"cuda\") -> tuple[Tensor, Tensor, Optional[Tensor]]:  \n    set_seed()\n    assert M > 0, \"M for input generation must be positive.\"  \n    assert M <= 8, \"M for input generation must be less or equal to 8.\"  \n    assert N > 0, \"N for input generation must be positive.\"  \n    assert K > 0, \"K for input generation must be positive.\"  \n    a: Tensor = torch.randn((M, K), dtype=torch.float16, device=device)  \n    b: Tensor = torch.randn((N, K), dtype=a.dtype, device=a.device).T  \n    bias: Optional[Tensor] = torch.randn(M, dtype=a.dtype, device=a.device) if use_bias else None  \n  \n    return a, b, bias  \n\ndef gen_input_benchmark(M: int, N: int, K: int, use_bias: bool, device: str = \"cuda\", dtype=torch.float16) -> tuple[Tensor, Tensor, Optional[Tensor]]:\n    set_seed()\n    # Original gen_input had M <= 8 assertion, which might be too restrictive for general benchmarks.\n    # Let's remove it for performance testing, assuming M can be larger.\n    # assert M > 0, \"M for input generation must be positive.\"\n    # assert M <= 8, \"M for input generation must be less or equal to 8.\" # Removed for perf\n    # assert N > 0, \"N for input generation must be positive.\"\n    # assert K > 0, \"K for input generation must be positive.\"\n    a: Tensor = torch.randn((M, K), dtype=dtype, device=device)\n    # Original b was (N,K).T. For GEMM A(M,K) @ B(K,N), b should be (K,N)\n    b: Tensor = torch.randn((K, N), dtype=a.dtype, device=a.device) # Corrected shape for B\n    bias: Optional[Tensor] = torch.randn(M, dtype=a.dtype, device=a.device) if use_bias else None\n    return a, b, bias\n\ndef get_target_shapes() -> list[tuple[int, int, int]]:  \n    # yapf: disable  \n    return [  \n        (1, 8192, 28672),   # Llama 70B  \n        (1, 6144, 6144),    # Grok  \n        (1, 4096, 4096),    # Generic GEMM  \n        (2, 16384, 16384),  # Generic GEMM  \n        (1, 4096, 3078),    # Uneven K  \n        (1, 23, 31),        # Very small shape, uneven K  \n        (1, 23, 128),       # Very small shape, even K  \n    ]  \n    # yapf: enable  \n  \n  \ndef allclose(x: Tensor, y: Tensor) -> bool:  \n    return torch.allclose(x, y, atol=1e-3, rtol=1e-2)  \n  \n  \n@pytest.mark.parametrize(\"use_bias\", [False, True])  \n@pytest.mark.parametrize(\"M, N, K\", get_target_shapes())  \ndef test_matmul(M: int, N: int, K: int, use_bias: bool,request) -> None:  \n    a: Tensor  \n    b: Tensor  \n    bias: Optional[Tensor]  \n    a, b, bias = gen_input(M, N, K, use_bias)  \n  \n    c_torch: Tensor = matmul(\"torch\", a, b, bias)  \n    c_triton_dot: Tensor = matmul(\"triton-dot\", a, b, bias)  \n\n    ################### save c_triton_dot in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = c_triton_dot.clone().detach().cpu()\n    ###################################################################\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    assert allclose(c_torch, c_triton_dot), \"PyTorch and Triton Dot results don't match.\"  \n\n\n# --- Define TFLOPS and GB/s calculators for GEMM ---\ndef calculate_gemm_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    use_bias = params.get('use_bias', False)\n    flops = 2 * M * N * K\n    if use_bias: flops += M * N # Add M*N for bias addition\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_gemm_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    use_bias = params.get('use_bias', False)\n    dtype_str = params.get('dtype_str', 'fp16')\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    bytes_a = M * K * element_size\n    bytes_b = K * N * element_size\n    bytes_c = M * N * element_size\n    total_bytes = bytes_a + bytes_b + bytes_c\n    if use_bias: total_bytes += M * element_size # Read bias\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\ndef get_target_shapes_for_perf() -> list[tuple[int, int, int]]: # Renamed for clarity\n    return [\n        (128, 8192, 4096),   # Larger M\n        (512, 4096, 4096),\n        (1024, 1024, 1024),  # Square\n        (4096, 512, 2048),   # Different aspect ratios\n        (1, 4096, 3078),     # Uneven K from original\n        (16, 2048, 2048),    # Smaller M, larger N/K\n        # (1, 23, 31),       # Very small shapes might not be ideal for peak perf\n        # (1, 23, 128),\n    ]\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"multreduce_matmul_dot_perf\"\n\n# --- Pytest parametrize for performance testing ---\nGEMM_DTYPES_FOR_PERF = ['fp16', 'bf16'] # 'fp32' can be added\n\n@pytest.mark.parametrize(\"use_bias\", [False, True])\n@pytest.mark.parametrize(\"M, N, K\", get_target_shapes_for_perf())\n@pytest.mark.parametrize(\"dtype_str\", GEMM_DTYPES_FOR_PERF)\ndef test_performance(M: int, N: int, K: int, use_bias: bool, dtype_str: str, request) -> None:\n    set_seed()\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    a, b, bias = gen_input_benchmark(M, N, K, use_bias, dtype=current_dtype)\n\n    # --- Create op_lambda for benchmarking ---\n    # We want to benchmark the triton_dot_matmul_kernel directly via its triton_matmul wrapper\n    op_lambda = lambda: triton_matmul(\"triton-dot\", a, b, bias)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K, \"use_bias\": use_bias, \"dtype_str\": dtype_str,\n        \"provider\": \"triton-dot\"\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_gemm_gbps,\n                              tflops_calculator=calculate_gemm_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all c_triton_dot results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} c_triton_dot tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ######################################## \n  \n# Benchmark Triton GEMM, comparing it to PyTorch GEMM reference implementation:  \n# -----------------------------------------------------------------------------  \n  \n  \n# Convert milliseconds to GiB/s.  \ndef ms_to_gibps(M: int, N: int, K: int, milliseconds: float) -> float:  \n    read_elems: int = M * K + K * N  \n    write_elems: int = M * N  \n    transf_elems: int = read_elems + write_elems  \n    transf_bytes: int = 2 * transf_elems  # times 2 due to fp16  \n    transf_gibibytes: float = 2**-30 * transf_bytes  \n    seconds: float = 1e-3 * milliseconds  \n    return round(transf_gibibytes / seconds, 2)  \n  \n  \ndef run_benchmark(use_bias: bool) -> None:  \n    perf_unit: str = \"GiB/s\"  \n    line_vals: list[str] = [\"torch\", \"triton-dot\"]  \n    line_names: list[str] = [f\"{x.replace('-', ' ').title()} ({perf_unit})\" for x in line_vals]  \n  \n    # Triton benchmark:  \n    @triton.testing.perf_report(  \n        triton.testing.Benchmark(  \n            x_names=[\"M\", \"N\", \"K\"],  \n            x_vals=get_target_shapes(),  \n            line_arg=\"provider\",  \n            line_vals=line_vals,  \n            line_names=line_names,  \n            ylabel=perf_unit,  \n            args={},  \n            plot_name=f\"fp16_{os.path.splitext(os.path.basename(__file__))[0]}\",  \n        ))  \n    def benchmark(M: int, N: int, K: int, provider: str) -> tuple[float, float, float]:  \n  \n        def perf(milliseconds: float) -> float:  \n            return ms_to_gibps(M, N, K, milliseconds)  \n  \n        a: Tensor  \n        b: Tensor  \n        bias: Optional[Tensor]  \n        a, b, bias = gen_input(M, N, K, use_bias)  \n  \n        p20_ms: float  \n        p50_ms: float  \n        p80_ms: float  \n        p20_ms, p50_ms, p80_ms = triton.testing.do_bench(lambda: matmul(provider, a, b, bias),  \n                                                         quantiles=[0.2, 0.5, 0.8])  \n  \n        p20_gibps: float = perf(p80_ms)  \n        p50_gibps: float = perf(p50_ms)  \n        p80_gibps: float = perf(p20_ms)  \n  \n        print(\", \".join([  \n            f\"(M, N, K) = {(M, N, K)}\",  \n            f\"provider = {provider}\",  \n            f\"p20 = {p20_gibps} {perf_unit}\",  \n            f\"p50 = {p50_gibps} {perf_unit}\",  \n            f\"p80 = {p80_gibps} {perf_unit}\",  \n        ]))  \n  \n        if provider == \"triton-dot\":  \n            print(f\"Triton Dot kernel best config = {triton_dot_matmul_kernel.best_config}\")   \n  \n        return p50_gibps, p20_gibps, p80_gibps  \n  \n    print(f\"Running benchmark (use_bias = {use_bias})...\")  \n    benchmark.run(show_plots=False, print_data=True)  \n    print(\"Done.\")  \n  \n  \n# Script entry point:  \n# -------------------  \n  \n  \ndef positive_int(value: str) -> int:  \n    try:  \n        int_value = int(value)  \n    except ValueError:  \n        raise argparse.ArgumentTypeError(f\"{value} is not an integer.\")  \n    if int_value <= 0:  \n        raise argparse.ArgumentTypeError(f\"{value} is not a positive integer.\")  \n    return int_value  \n  \n  \ndef parse_args() -> argparse.Namespace:  \n    parser = argparse.ArgumentParser(  \n        description=\"C = A * B + BIAS matrix multiplication kernel for small matrices (M \u2264 8)\",  \n        formatter_class=argparse.RawTextHelpFormatter)  \n    parser.add_argument(  \n        \"mode\", choices=[\"bench\"], help=\"mode of operation:\\n\"  \n        \"  run: run Triton kernel for a given (M, N, K) shape\\n\"  \n        \"  bench: benchmark performance for target shapes\\n\")  \n    shape_group = parser.add_argument_group(\"kernel shape arguments\")  \n    shape_group.add_argument(\"-M\", type=positive_int, help=\"rows of matrix A (must be less or equal to 8)\")  \n    shape_group.add_argument(\"-N\", type=positive_int, help=\"columns of matrix A / rows of matrix B\")  \n    shape_group.add_argument(\"-K\", type=positive_int, help=\"columns of matrix B\")  \n    shape_group.add_argument(\"--use-bias\", default=False, action=\"store_true\", help=\"use BIAS vector\")  \n    shape_group.add_argument(\"--use-dot\", default=False, action=\"store_true\", help=\"use tl.dot for dot product\")  \n    args = parser.parse_args()  \n    if args.mode == \"run\":  \n        try:  \n            sizes: tuple[Optional[int], ...] = tuple(size for size in (args.M, args.N, args.K))  \n            if any(size is None for size in sizes):  \n                raise ValueError(f\"(M, N, K) = {sizes}, all sizes must be specified together.\")  \n            if args.M > 8:  \n                raise ValueError(f\"M = {args.M} is too big, this kernel was designed for M \u2264 8.\")  \n        except ValueError as arg_error:  \n            print(arg_error)  \n            sys.exit(1)  \n    return args  \n  \n  \ndef main() -> int:  \n    args: argparse.Namespace = parse_args()  \n    status: int = 0  \n    try:  \n        match args.mode:  \n            case \"bench\":  \n                run_benchmark(args.use_bias)  \n    except KeyboardInterrupt:  \n        print(\"\\nInterrupted.\")  \n    except Exception as error:  \n        print(f\"\\nUnexpected error: {error}\")  \n        status = 1  \n    return status  \n  \n  \nif __name__ == \"__main__\":  \n    sys.exit(main())  \n"
    },
    {
        "file": "triton_multreduce_matmul_kernel.py",
        "target_kernel_name": "triton_multreduce_matmul_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `triton_multreduce_matmul_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list ,\n\nThis kernel, `triton_multreduce_matmul_kernel`,  is designed to perform matrix multiplication by explicitly using element-wise multiplication followed by a reduction (summation), instead of relying on Triton's `tl.dot` intrinsic.\n\n**Your objective is to implement the body of `triton_multreduce_matmul_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `triton_multreduce_matmul_kernel` (i.e., `a_ptr`, `b_ptr`, `c_ptr`, `bias_ptr`, `M`, `N`, `K`, all stride arguments, `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, `BLOCK_SIZE_K`, `USE_BIAS`, and `EVEN_K`) are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\n\n\nThe full definition for `triton_multreduce_matmul_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `triton_multreduce_matmul_kernel` whilst keeping other things.\n\n\n# Imports:\n# --------\n\nimport argparse\nimport itertools\nimport os\nimport sys\nfrom typing import Any, Callable, Optional\n\nimport pytest\nimport torch\nfrom torch import Tensor\n\nimport triton\nimport triton.language as tl\n\n\n# Triton GEMM:\n# ------------\n\n######################## HELPER UTILS #####################\n# Autotune configurations for Triton GEMM implemented with explicit dot product.\ndef get_triton_multreduce_autotune_configs() -> list[triton.Config]:\n    block_size_k_range: list[int] = [128, 256, 512]\n    kpack_range: list[int] = [1, 2]\n    return [\n        triton.Config(\n            {\"BLOCK_SIZE_M\": 1, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": block_size_k, \"waves_per_eu\": 0, \"kpack\": kpack},\n            num_warps=8, num_stages=2) for block_size_k, kpack in itertools.product(block_size_k_range, kpack_range)\n    ]\n\n\ndef get_triton_autotune_key() -> list[str]:\n    return [\"M\", \"N\", \"K\"]\n\n\ndef get_triton_heuristics() -> dict[str, Callable[[dict[str, Any]], Any]]:\n    return {\"EVEN_K\": lambda args: args[\"K\"] % args[\"BLOCK_SIZE_K\"] == 0}\n\n######################## HELPER UTILS #####################\n\n\n# Triton GEMM kernel implemented with explicit dot product.\n@triton.autotune(configs=get_triton_multreduce_autotune_configs(), key=get_triton_autotune_key())\n@triton.heuristics(get_triton_heuristics())\n@triton.jit\ndef triton_multreduce_matmul_kernel(a_ptr, b_ptr, c_ptr, bias_ptr,  #\n                                    M: int, N: int, K: int,  #\n                                    stride_am: int, stride_ak: int,  #\n                                    stride_bk: int, stride_bn: int,  #\n                                    stride_cm: int, stride_cn: int,  #\n                                    stride_bias: int,  #\n                                    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n                                    BLOCK_SIZE_K: tl.constexpr,  #\n                                    USE_BIAS: tl.constexpr, EVEN_K: tl.constexpr  #\n                                    ):\n    \"\"\"  \n    Performs matrix multiplication (C = A @ B + bias) using an explicit  \n    element-wise multiplication followed by a reduction (summation) strategy,  \n    instead of `tl.dot()`.  \n\n    This kernel is a wrapper around `triton_matmul_kernel`, configured  \n    to use the non-`tl.dot` path by setting `USE_DOT=False`.  \n\n    Args:  \n        a_ptr: Pointer to the first input matrix A.  \n        b_ptr: Pointer to the second input matrix B.  \n        c_ptr: Pointer to the output matrix C.  \n        bias_ptr: Pointer to the bias vector/matrix.  \n        M: Number of rows in matrix A and C.  \n        N: Number of columns in matrix B and C.  \n        K: Number of columns in matrix A and rows in matrix B.  \n        stride_am: Stride for the M dimension of matrix A.  \n        stride_ak: Stride for the K dimension of matrix A.  \n        stride_bk: Stride for the K dimension of matrix B.  \n        stride_bn: Stride for the N dimension of matrix B.  \n        stride_cm: Stride for the M dimension of matrix C.  \n        stride_cn: Stride for the N dimension of matrix C.  \n        stride_bias: Stride for the bias.  \n        BLOCK_SIZE_M (tl.constexpr): Tile size for the M dimension.  \n        BLOCK_SIZE_N (tl.constexpr): Tile size for the N dimension.  \n        BLOCK_SIZE_K (tl.constexpr): Tile size for the K dimension.  \n        USE_BIAS (tl.constexpr): If True, add bias to the result.  \n        EVEN_K (tl.constexpr): If True, K is evenly divisible by BLOCK_SIZE_K,  \n                               allowing for unmasked loads in the K loop.  \n    \"\"\"\n    # Your code here.\n\n\n",
        "label": "# Imports:  \n# --------  \n  \nimport argparse  \nimport itertools  \nimport os  \nimport sys  \nfrom typing import Any, Callable, Optional  \n  \nimport pytest  \nimport torch  \nfrom torch import Tensor  \n  \nimport triton  \nimport triton.language as tl  \n  \n\n# Triton GEMM:  \n# ------------  \n\n######################## HELPER UTILS #####################  \n# Autotune configurations for Triton GEMM implemented with explicit dot product.  \ndef get_triton_multreduce_autotune_configs() -> list[triton.Config]:  \n    block_size_k_range: list[int] = [128, 256, 512]  \n    kpack_range: list[int] = [1, 2]  \n    return [  \n        triton.Config(  \n            {\"BLOCK_SIZE_M\": 1, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": block_size_k, \"waves_per_eu\": 0, \"kpack\": kpack},  \n            num_warps=8, num_stages=2) for block_size_k, kpack in itertools.product(block_size_k_range, kpack_range)  \n    ]  \n  \n  \ndef get_triton_autotune_key() -> list[str]:  \n    return [\"M\", \"N\", \"K\"]  \n  \n  \ndef get_triton_heuristics() -> dict[str, Callable[[dict[str, Any]], Any]]:  \n    return {\"EVEN_K\": lambda args: args[\"K\"] % args[\"BLOCK_SIZE_K\"] == 0}  \n  \n######################## HELPER UTILS #####################\n\n\n# Core Triton GEMM kernel.  \n@triton.jit  \ndef triton_matmul_kernel(a_ptr, b_ptr, c_ptr, bias_ptr,  #  \n                         M: int, N: int, K: int,  #  \n                         stride_am: int, stride_ak: int,  #  \n                         stride_bk: int, stride_bn: int,  #  \n                         stride_cm: int, stride_cn: int,  #  \n                         stride_bias: int,  #  \n                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #  \n                         USE_BIAS: tl.constexpr, USE_DOT: tl.constexpr, EVEN_K: tl.constexpr  #  \n                         ):  \n    # Compute program ID:  \n    pid = tl.program_id(axis=0)  \n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)  \n    pid_m = pid // num_pid_n  \n    pid_n = pid % num_pid_n  \n  \n    # Compute A and B base pointers:  \n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)  \n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)  \n    offs_k = tl.arange(0, BLOCK_SIZE_K)  \n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak  \n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn  \n  \n    # Load BIAS:  \n    if USE_BIAS:  \n        bias_ptrs = bias_ptr + offs_am * stride_bias  \n        bias = tl.load(bias_ptrs, mask=offs_am < M, other=0)  \n  \n    # Initialize accumulator:  \n    acc_dtype = tl.float32 if a_ptr.type.element_ty != tl.int8 else tl.int32  \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=acc_dtype)  \n  \n    # GEMM loop:  \n  \n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):  \n        if EVEN_K:  \n            # Unmasked load of A and B:  \n            a = tl.load(a_ptrs)  \n            b = tl.load(b_ptrs)  \n        else:  \n            # Masked load of A and B:  \n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0)  \n            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0)  \n        # Compute dot product:  \n        if USE_DOT:  \n            accumulator += tl.dot(a, b)  \n        else:  \n            a = tl.reshape(a, (BLOCK_SIZE_M, BLOCK_SIZE_K, 1)).to(acc_dtype)  \n            b = tl.reshape(b, (1, BLOCK_SIZE_K, BLOCK_SIZE_N)).to(acc_dtype)  \n            accumulator += tl.sum(a * b, axis=1)  \n        # Advance A and B pointers:  \n        a_ptrs += BLOCK_SIZE_K * stride_ak  \n        b_ptrs += BLOCK_SIZE_K * stride_bk  \n  \n    # Convert accumulator back to C's type:  \n    c = accumulator.to(c_ptr.type.element_ty)  \n  \n    # Add BIAS:  \n    if USE_BIAS:  \n        c += bias[:, None]  \n  \n    # Compute C pointers and store C:  \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)  \n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)  \n    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn  \n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)  \n    tl.store(c_ptrs, c, mask=c_mask)  \n  \n  \n# Triton GEMM kernel implemented with explicit dot product.  \n@triton.autotune(configs=get_triton_multreduce_autotune_configs(), key=get_triton_autotune_key())  \n@triton.heuristics(get_triton_heuristics())  \n@triton.jit  \ndef triton_multreduce_matmul_kernel(a_ptr, b_ptr, c_ptr, bias_ptr,  #  \n                                    M: int, N: int, K: int,  #  \n                                    stride_am: int, stride_ak: int,  #  \n                                    stride_bk: int, stride_bn: int,  #  \n                                    stride_cm: int, stride_cn: int,  #  \n                                    stride_bias: int,  #  \n                                    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,  \n                                    BLOCK_SIZE_K: tl.constexpr,  #  \n                                    USE_BIAS: tl.constexpr, EVEN_K: tl.constexpr  #  \n                                    ):  \n    triton_matmul_kernel(a_ptr, b_ptr, c_ptr, bias_ptr,  #  \n                         M, N, K,  #  \n                         stride_am, stride_ak,  #  \n                         stride_bk, stride_bn,  #  \n                         stride_cm, stride_cn,  #  \n                         stride_bias,  #  \n                         BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,  #  \n                         USE_BIAS=USE_BIAS, USE_DOT=False, EVEN_K=EVEN_K)  \n  \n  \n\n  \n\n  \n  \n##################################################################################################################################################  \n\n######################################## HELPERS for Eval ######################################## \nimport numpy as np\nimport random\nimport torch \nimport argparse  \nimport itertools  \nimport os  \nimport sys  \nfrom typing import Any, Callable, Optional  \nimport pytest  \nimport torch  \nfrom torch import Tensor  \nimport triton  \nimport triton.language as tl  \nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n# PyTorch GEMM:  \n# -------------  \n  \n  \ndef torch_matmul(a: Tensor, b: Tensor, bias: Optional[Tensor]) -> Tensor:  \n    c: Tensor = torch.matmul(a, b)  \n    if bias is not None:  \n        c += bias[:, None]  \n    return c  \n  \n  \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n# Wrapper for calling PyTorch GEMM or Triton GEMM:  \n# ------------------------------------------------  \n  \n\n\ndef triton_matmul(triton_provider: str, a: Tensor, b: Tensor, bias: Optional[Tensor]) -> Tensor:  \n    assert triton_provider in [\"triton-multreduce\"]  \n  \n    M: int  \n    N: int  \n    K: int  \n    M, K = a.shape  \n    _, N = b.shape  \n  \n    c: Tensor = torch.empty((M, N), device=a.device, dtype=a.dtype)  \n  \n    def grid(args: dict[str, Any]) -> tuple[int]:  \n        return (triton.cdiv(M, args[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, args[\"BLOCK_SIZE_N\"]), )  \n  \n    matmult_kernel = triton_multreduce_matmul_kernel  \n  \n    matmult_kernel[grid](  \n        # Data pointers  \n        a,  \n        b,  \n        c,  \n        bias,  \n        # Size of matrices  \n        M,  \n        N,  \n        K,  \n        # Strides  \n        a.stride(0),  \n        a.stride(1),  \n        b.stride(0),  \n        b.stride(1),  \n        c.stride(0),  \n        c.stride(1),  \n        bias.stride(0) if bias is not None else 0,  \n        # Other kernel parameters  \n        USE_BIAS=bias is not None,  \n    )  \n  \n    return c  \n\n\n\ndef matmul(provider: str, a: Tensor, b: Tensor, bias: Optional[Tensor]) -> Tensor:  \n    assert provider in [\"torch\", \"triton-multreduce\"]  \n  \n    assert a.is_cuda, \"Matrix A must be in GPU.\"  \n    assert a.is_contiguous(), \"Matrix A must be contiguous.\"  \n    assert b.is_cuda, \"Matrix B must be in GPU.\"  \n    assert a.device == b.device, \"Matrix A and matrix B must be in the same GPU.\"  \n    assert a.dtype == b.dtype, \"Matrix A and matrix B must have the same data type.\"  \n    assert a.dim() == b.dim() == 2, \"Matrix A and matrix B must be two-dimensional tensors.\"  \n    assert a.shape[1] == b.shape[0], \"Matrix A columns must be equal to matrix B rows.\"  \n  \n    if bias is not None:  \n        assert bias.is_cuda, \"Bias vector must be in GPU.\"  \n        assert bias.is_contiguous(), \"Bias vector must be continuous.\"  \n        assert bias.device == a.device, \"Matrix A and bias vector must be in the same GPU.\"  \n        assert bias.dtype == a.dtype, \"Matrix A and bias vector must have the same data type.\"  \n        assert bias.dim() == 1, \"Bias vector must be one-dimensional tensor.\"  \n        assert bias.shape == (a.shape[0], ), \"Bias vector length must be equal to matrix A rows.\"  \n  \n    if provider == \"torch\":  \n        return torch_matmul(a, b, bias)  \n  \n    return triton_matmul(provider, a, b, bias)  \n\n\n# Input generation:  \n# -----------------  \n  \n  \ndef gen_input(M: int, N: int, K: int, use_bias: bool, device: str = \"cuda\") -> tuple[Tensor, Tensor, Optional[Tensor]]:  \n    assert M > 0, \"M for input generation must be positive.\"  \n    assert M <= 8, \"M for input generation must be less or equal to 8.\"  \n    assert N > 0, \"N for input generation must be positive.\"  \n    assert K > 0, \"K for input generation must be positive.\"  \n  \n    set_seed()\n  \n    a: Tensor = torch.randn((M, K), dtype=torch.float16, device=device)  \n    b: Tensor = torch.randn((N, K), dtype=a.dtype, device=a.device).T  \n    bias: Optional[Tensor] = torch.randn(M, dtype=a.dtype, device=a.device) if use_bias else None  \n  \n    return a, b, bias  \n\n    \ndef get_target_shapes() -> list[tuple[int, int, int]]:  \n    # yapf: disable  \n    return [  \n        (1, 8192, 28672),   # Llama 70B  \n        (1, 6144, 6144),    # Grok  \n        (1, 4096, 4096),    # Generic GEMM  \n        (2, 16384, 16384),  # Generic GEMM  \n        (1, 4096, 3078),    # Uneven K  \n        (1, 23, 31),        # Very small shape, uneven K  \n        (1, 23, 128),       # Very small shape, even K  \n    ]  \n    # yapf: enable  \n  \n  \ndef allclose(x: Tensor, y: Tensor) -> bool:  \n    return torch.allclose(x, y, atol=1e-3, rtol=1e-2)  \n  \n  \n@pytest.mark.parametrize(\"use_bias\", [False, True])  \n@pytest.mark.parametrize(\"M, N, K\", get_target_shapes())  \ndef test_matmul(M: int, N: int, K: int, use_bias: bool, request) -> None:  \n    a: Tensor  \n    b: Tensor  \n    bias: Optional[Tensor]  \n    a, b, bias = gen_input(M, N, K, use_bias)  \n  \n    c_torch: Tensor = matmul(\"torch\", a, b, bias)  \n    c_triton_multreduce: Tensor = matmul(\"triton-multreduce\", a, b, bias)  \n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save c_triton_dot in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = c_triton_multreduce.clone().detach().cpu()\n    ###################################################################\n    \n    assert allclose(c_torch, c_triton_multreduce), \"PyTorch and Triton Multreduce results don't match.\"  \n  \n\ndef gen_input_for_perf(M: int, N: int, K: int, use_bias: bool, dtype_str: str = \"float16\", device: str = \"cuda\") -> tuple[Tensor, Tensor, Optional[Tensor]]:  \n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16 \n    set_seed(42) \n    a: Tensor = torch.randn((M, K), dtype=current_dtype, device=device)  \n    b: Tensor = torch.randn((K, N), dtype=current_dtype, device=device) \n    bias: Optional[Tensor] = torch.randn(M, dtype=current_dtype, device=device) if use_bias else None  \n    return a, b, bias  \n\ndef multreduce_matmul_triton_wrapper(a_tensor, b_tensor, c_buffer, bias_tensor,\n                                     M_dim, N_dim, K_dim, \n                                     block_m_const, block_n_const, block_k_const_tile, # block_k_const_tile is K-tile for kernel\n                                     use_bias_flag, num_warps_launch, num_stages_launch): # num_stages for launch\n    grid = (triton.cdiv(M_dim, block_m_const) * triton.cdiv(N_dim, block_n_const), )\n    even_k_flag = (K_dim % block_k_const_tile == 0)\n\n    # Call the CORE kernel directly, not the autotuned one, to avoid meta-param conflict\n    triton_matmul_kernel[grid]( # Calling the non-autotuned version\n        a_tensor, b_tensor, c_buffer, bias_tensor,\n        M_dim, N_dim, K_dim,\n        a_tensor.stride(0), a_tensor.stride(1),\n        b_tensor.stride(0), b_tensor.stride(1),\n        c_buffer.stride(0), c_buffer.stride(1),\n        bias_tensor.stride(0) if use_bias_flag and bias_tensor is not None else 0,\n        BLOCK_SIZE_M=block_m_const, \n        BLOCK_SIZE_N=block_n_const, \n        BLOCK_SIZE_K=block_k_const_tile, # This is the K-tile size\n        USE_BIAS=use_bias_flag, \n        USE_DOT=False, # Explicitly set for \"multreduce\" behavior\n        EVEN_K=even_k_flag,\n        num_warps=num_warps_launch,\n        num_stages=num_stages_launch # Pass num_stages\n    )\n    return c_buffer\n\ndef calculate_gemm_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    flops = 2 * M * N * K \n    if params.get('use_bias', False): flops += M * N \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef get_torch_dtype_from_str(dtype_str: str, default_dtype=torch.float16) -> torch.dtype:\n    if dtype_str == 'fp32': return torch.float32\n    if dtype_str == 'bf16': return torch.bfloat16\n    if dtype_str == 'fp16': return torch.float16\n    return default_dtype\n\ndef calculate_gemm_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    use_bias = params.get('use_bias', False)\n    dtype_str = params.get('dtype_str', 'fp16') \n    current_dtype = get_torch_dtype_from_str(dtype_str)\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    bytes_a, bytes_b, bytes_c_write = [dim * element_size for dim in [M*K, K*N, M*N]]\n    total_bytes = bytes_a + bytes_b + bytes_c_write\n    if use_bias: total_bytes += M * element_size \n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"triton_multreduce_matmul_perf\"\n\nGEMM_PERF_SHAPES = [\n    (512, 512, 512), (1024, 1024, 512), (2048, 1024, 256), \n]\n\n# --- REVISED BLOCK CONFIGS focusing on BM * BN <= 4096 (for fp16 inputs, fp32 acc) ---\n# And also BM * BK_tile + BK_tile * BN for the tl.load part.\nGEMM_PERF_BLOCK_CONFIGS = []\n# Target BM * BN <= 4096 (for fp32 acc, if 4 temp buffers of this size are needed)\n# Target BM * BN <= 2048 (for fp32 acc, if 2 temp buffers of this size, plus A and B inputs)\n\n# Small BM * BN products\nfor bk_tile in [32, 64]: # K-tile size\n    # BM * BN approx 1024\n    for bm, bn in [(32,32), (16,64), (64,16)]:\n        GEMM_PERF_BLOCK_CONFIGS.append((bm,bn,bk_tile))\n    # BM * BN approx 2048\n    for bm, bn in [(32,64), (64,32), (16,128), (128,16)]:\n        GEMM_PERF_BLOCK_CONFIGS.append((bm,bn,bk_tile))\n    # BM * BN approx 4096\n    for bm, bn in [(64,64), (32,128), (128,32)]:\n        GEMM_PERF_BLOCK_CONFIGS.append((bm,bn,bk_tile))\n\n# Try some original autotune-like configs for multreduce (BM=1)\nfor bk_tile in [128, 256, 512]:\n    GEMM_PERF_BLOCK_CONFIGS.append((1, 64, bk_tile))\n\n\n# Remove duplicates and ensure K_tile is not too large for shared mem of A, B load\nunique_block_configs = []\nseen_block_configs_str = set()\nfor bm, bn, bk_tile in GEMM_PERF_BLOCK_CONFIGS:\n    # Check initial load shared memory: (bm * bk_tile + bk_tile * bn) * elem_size_input\n    # Assume fp16 inputs (2 bytes) for this pre-filter\n    smem_load_fp16_elements = bm * bk_tile + bk_tile * bn\n    if smem_load_fp16_elements * 2 > 65536 * 0.8 : # Check against 80% of 64KB\n        continue\n\n    cfg_str = f\"bm{bm}bn{bn}bk{bk_tile}\"\n    if cfg_str not in seen_block_configs_str:\n        unique_block_configs.append((bm,bn,bk_tile))\n        seen_block_configs_str.add(cfg_str)\nGEMM_PERF_BLOCK_CONFIGS = unique_block_configs\nprint(f\"Generated {len(GEMM_PERF_BLOCK_CONFIGS)} unique block configurations for multreduce matmul.\")\n\n\nGEMM_PERF_DTYPES = ['fp16'] # Start with fp16, as fp32 inputs will double shared mem for A/B\nGEMM_PERF_BIAS = [False]    # Start simple\nGEMM_PERF_NUM_STAGES = [1]  # Start with num_stages=1 to minimize shared memory\nGEMM_PERF_NUM_WARPS = [4]   # Start with 4 warps\n\n@pytest.mark.parametrize(\"m_n_k_shape\", GEMM_PERF_SHAPES)\n@pytest.mark.parametrize(\"block_config\", GEMM_PERF_BLOCK_CONFIGS)\n@pytest.mark.parametrize(\"use_bias_flag\", GEMM_PERF_BIAS)\n@pytest.mark.parametrize(\"dtype_str\", GEMM_PERF_DTYPES)\n@pytest.mark.parametrize(\"num_stages_val\", GEMM_PERF_NUM_STAGES)\n@pytest.mark.parametrize(\"num_warps_val\", GEMM_PERF_NUM_WARPS)\ndef test_performance(m_n_k_shape, block_config, use_bias_flag, dtype_str, \n                                       num_stages_val, num_warps_val, request):\n    set_seed()\n    M, N, K = m_n_k_shape\n    BLOCK_M_const, BLOCK_N_const, BLOCK_K_tile_const = block_config\n\n    if K % BLOCK_K_tile_const != 0 : pytest.skip(f\"K={K} not multiple of BLOCK_K_tile={BLOCK_K_tile_const}\")\n    if M % BLOCK_M_const !=0 : pytest.skip(f\"M={M} not multiple of BLOCK_M={BLOCK_M_const}\")\n    \n    current_dtype = get_torch_dtype_from_str(dtype_str)\n    elem_size = torch.tensor([], dtype=current_dtype).element_size()\n    \n    # More refined shared memory check for the USE_DOT=False path\n    # Acc (BM, BN) is fp32 (4 bytes)\n    # A_casted (BM, BK_tile) is fp32\n    # B_casted (BK_tile, BN) is fp32\n    # If intermediate product P(BM, BK_tile, BN) is materialized, that's the main issue.\n    # Let's assume Triton tiles the sum over K_tile.\n    # The critical part might be holding one (BM,BN) slice of P in fp32, plus A and B.\n    # Required fp32 elements for this slice: BM*BN\n    # Required fp32 elements for A_casted tile: BM*BK_tile\n    # Required fp32 elements for B_casted tile: BK_tile*BN\n    # Total_fp32_elements_approx = BM*BN + BM*BK_tile + BK_tile*BN\n    # This must be an underestimate if the error is 131072 bytes for BM=64,BN=128,BK=32\n    # (64*128) + (64*32) + (32*128) = 8192 + 2048 + 4096 = 14336 fp32 elements.\n    # 14336 * 4 bytes = 57344 bytes. This *should* fit if num_stages=1.\n\n    # The error 131072 bytes = 32768 fp32 elements.\n    # If BM=64, BN=128, then BM*BN = 8192.\n    # 32768 / 8192 = 4. This implies 4 buffers of size (BM,BN) in fp32.\n    # (e.g. accumulator, one slice of P, and perhaps two more for some reason).\n    # So, the constraint is roughly: 4 * BLOCK_M * BLOCK_N * sizeof(fp32) <= 65536\n    # BLOCK_M * BLOCK_N <= 65536 / 16 = 4096\n    if BLOCK_M_const * BLOCK_N_const > 4096 :\n         pytest.skip(f\"Skipping BM={BLOCK_M_const}, BN={BLOCK_N_const} as BM*BN > 4096, likely OOM for USE_DOT=False path.\")\n\n\n    a, b, bias = gen_input_for_perf(M, N, K, use_bias_flag, dtype_str=dtype_str)\n    # Output C is always fp16 or fp32 (based on input), acc is fp32. Kernel casts final acc.\n    # The kernel itself casts output to c_ptr.type.element_ty.\n    # Let's match input dtype for c_buffer for simplicity, kernel handles final cast.\n    c_buffer = torch.empty((M, N), device='cuda', dtype=current_dtype)\n\n\n    op_lambda = lambda: multreduce_matmul_triton_wrapper(\n        a, b, c_buffer, bias, M, N, K,\n        BLOCK_M_const, BLOCK_N_const, BLOCK_K_tile_const,\n        use_bias_flag, num_warps_val, num_stages_val\n    )\n\n    bench_config = do_bench_config(warm_up=10, repetition=50)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K,\n        \"BLOCK_M\": BLOCK_M_const, \"BLOCK_N\": BLOCK_N_const, \"BLOCK_K_tile\": BLOCK_K_tile_const,\n        \"use_bias\": use_bias_flag, \"dtype_str\": dtype_str,\n        \"num_stages\": num_stages_val, \"num_warps\": num_warps_val,\n        \"USE_DOT\": False \n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_gemm_gbps,\n                                            tflops_calculator=calculate_gemm_tflops)\n\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all c_triton_multreduce results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} c_triton_multreduce tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ######################################## \n\n\n# Benchmark Triton GEMM, comparing it to PyTorch GEMM reference implementation:  \n# -----------------------------------------------------------------------------  \n  \n  \n# Convert milliseconds to GiB/s.  \ndef ms_to_gibps(M: int, N: int, K: int, milliseconds: float) -> float:  \n    read_elems: int = M * K + K * N  \n    write_elems: int = M * N  \n    transf_elems: int = read_elems + write_elems  \n    transf_bytes: int = 2 * transf_elems  # times 2 due to fp16  \n    transf_gibibytes: float = 2**-30 * transf_bytes  \n    seconds: float = 1e-3 * milliseconds  \n    return round(transf_gibibytes / seconds, 2)  \n  \n  \ndef run_benchmark(use_bias: bool) -> None:  \n    perf_unit: str = \"GiB/s\"  \n    line_vals: list[str] = [\"torch\", \"triton-multreduce\"]  \n    line_names: list[str] = [f\"{x.replace('-', ' ').title()} ({perf_unit})\" for x in line_vals]  \n  \n    # Triton benchmark:  \n    @triton.testing.perf_report(  \n        triton.testing.Benchmark(  \n            x_names=[\"M\", \"N\", \"K\"],  \n            x_vals=get_target_shapes(),  \n            line_arg=\"provider\",  \n            line_vals=line_vals,  \n            line_names=line_names,  \n            ylabel=perf_unit,  \n            args={},  \n            plot_name=f\"fp16_{os.path.splitext(os.path.basename(__file__))[0]}\",  \n        ))  \n    def benchmark(M: int, N: int, K: int, provider: str) -> tuple[float, float, float]:  \n  \n        def perf(milliseconds: float) -> float:  \n            return ms_to_gibps(M, N, K, milliseconds)  \n  \n        a: Tensor  \n        b: Tensor  \n        bias: Optional[Tensor]  \n        a, b, bias = gen_input(M, N, K, use_bias)  \n  \n        p20_ms: float  \n        p50_ms: float  \n        p80_ms: float  \n        p20_ms, p50_ms, p80_ms = triton.testing.do_bench(lambda: matmul(provider, a, b, bias),  \n                                                         quantiles=[0.2, 0.5, 0.8])  \n  \n        p20_gibps: float = perf(p80_ms)  \n        p50_gibps: float = perf(p50_ms)  \n        p80_gibps: float = perf(p20_ms)  \n  \n        print(\", \".join([  \n            f\"(M, N, K) = {(M, N, K)}\",  \n            f\"provider = {provider}\",  \n            f\"p20 = {p20_gibps} {perf_unit}\",  \n            f\"p50 = {p50_gibps} {perf_unit}\",  \n            f\"p80 = {p80_gibps} {perf_unit}\",  \n        ]))  \n  \n        if provider == \"triton-multreduce\":  \n            print(f\"Triton Multreduce kernel best config = {triton_multreduce_matmul_kernel.best_config}\")  \n  \n        return p50_gibps, p20_gibps, p80_gibps  \n  \n    print(f\"Running benchmark (use_bias = {use_bias})...\")  \n    benchmark.run(show_plots=False, print_data=True)  \n    print(\"Done.\")  \n  \n  \n# Script entry point:  \n# -------------------  \n  \n  \ndef positive_int(value: str) -> int:  \n    try:  \n        int_value = int(value)  \n    except ValueError:  \n        raise argparse.ArgumentTypeError(f\"{value} is not an integer.\")  \n    if int_value <= 0:  \n        raise argparse.ArgumentTypeError(f\"{value} is not a positive integer.\")  \n    return int_value  \n  \n  \ndef parse_args() -> argparse.Namespace:  \n    parser = argparse.ArgumentParser(  \n        description=\"C = A * B + BIAS matrix multiplication kernel for small matrices (M \u2264 8)\",  \n        formatter_class=argparse.RawTextHelpFormatter)  \n    parser.add_argument(  \n        \"mode\", choices=[\"bench\"], help=\"mode of operation:\\n\"  \n        \"  run: run Triton kernel for a given (M, N, K) shape\\n\"  \n        \"  bench: benchmark performance for target shapes\\n\")  \n    shape_group = parser.add_argument_group(\"kernel shape arguments\")  \n    shape_group.add_argument(\"-M\", type=positive_int, help=\"rows of matrix A (must be less or equal to 8)\")  \n    shape_group.add_argument(\"-N\", type=positive_int, help=\"columns of matrix A / rows of matrix B\")  \n    shape_group.add_argument(\"-K\", type=positive_int, help=\"columns of matrix B\")  \n    shape_group.add_argument(\"--use-bias\", default=False, action=\"store_true\", help=\"use BIAS vector\")  \n    shape_group.add_argument(\"--use-dot\", default=False, action=\"store_true\", help=\"use tl.dot for dot product\")  \n    args = parser.parse_args()  \n    if args.mode == \"run\":  \n        try:  \n            sizes: tuple[Optional[int], ...] = tuple(size for size in (args.M, args.N, args.K))  \n            if any(size is None for size in sizes):  \n                raise ValueError(f\"(M, N, K) = {sizes}, all sizes must be specified together.\")  \n            if args.M > 8:  \n                raise ValueError(f\"M = {args.M} is too big, this kernel was designed for M \u2264 8.\")  \n        except ValueError as arg_error:  \n            print(arg_error)  \n            sys.exit(1)  \n    return args  \n  \n  \ndef main() -> int:  \n    args: argparse.Namespace = parse_args()  \n    status: int = 0  \n    try:  \n        match args.mode:    \n            case \"bench\":  \n                run_benchmark(args.use_bias)  \n    except KeyboardInterrupt:  \n        print(\"\\nInterrupted.\")  \n    except Exception as error:  \n        print(f\"\\nUnexpected error: {error}\")  \n        status = 1  \n    return status  \n  \n  \nif __name__ == \"__main__\":  \n    sys.exit(main())  \n"
    },
    {
        "file": "gemm.py",
        "target_kernel_name": "matmul_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `matmul_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list ,\n\nThis kernel, `matmul_kernel`,  is designed to perform matrix multiplication C = A x B using a tiled approach.\n\n**Your objective is to implement the body of `matmul_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\n\n\nThe full definition for `matmul_kernel` and relevant helper utilities are provided in the context below. You only need to wrcomplete the the code for `matmul_kernel` whilst keeping other things intact.\n\n\nimport torch\nimport triton\nimport triton.language as tl\nimport sys\nimport argparse\nimport pytest\nimport re\n\n# This is a Triton kernel for matrix multiplication (GEMM) with support for various data types and scaling modes.\n\n#################### Helper utils functions ####################\n# Activation function.  \n@triton.jit  \ndef leaky_relu(x):  \n    x = x + 1  \n    return tl.where(x >= 0, x, 0.01 * x)  \n#################### Helper utils functions ####################\n\n\n\n@triton.autotune(  \n    configs=[  \n        triton.Config(  \n            {  \n                'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 4, 'waves_per_eu': 2,  \n                'kpack': 2, 'matrix_instr_nonkdim': 16  \n            }, num_warps=4, num_stages=2),  \n        triton.Config(  \n            {  \n                'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 4, 'waves_per_eu': 2,  \n                'kpack': 2, 'matrix_instr_nonkdim': 16  \n            }, num_warps=8, num_stages=2),  \n        triton.Config(  \n            {'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 4, 'waves_per_eu': 0},  \n            num_warps=8, num_stages=2),  \n        triton.Config(  \n            {  \n                'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 4, 'waves_per_eu': 2,  \n                'kpack': 1, 'matrix_instr_nonkdim': 16  \n            }, num_warps=8, num_stages=2),  \n        triton.Config(  \n            {  \n                'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 0,  \n                'kpack': 1  \n            }, num_warps=8, num_stages=2),  \n        triton.Config(  \n            {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 4, 'waves_per_eu': 0},  \n            num_warps=8, num_stages=2),  \n        triton.Config(  \n            {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2},  \n            num_warps=8, num_stages=2),  \n    ],  \n    key=['M', 'N', 'K'],  \n    use_cuda_graph=True,  \n)  \n@triton.heuristics({  \n    'EVEN_K':  \n    lambda args: args['K'] % args['BLOCK_SIZE_K'] == 0, 'GRID_MN':  \n    lambda args: triton.cdiv(args['M'], args['BLOCK_SIZE_M']) * triton.cdiv(args['N'], args['BLOCK_SIZE_N'])  \n})  \n@triton.jit  \ndef matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    a_scale_ptr,\n    b_scale_ptr,\n    stride_ascale_m,\n    stride_ascale_k,\n    stride_bscale_k,\n    stride_bscale_n,\n    # Meta-parameters\n    GROUP_K: tl.constexpr,\n    GROUP_N: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    APPLY_SCALE: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    GRID_MN: tl.constexpr,\n):\n    \"\"\"\n    Computes the matrix multiplication C = A x B using a tiled approach.\n\n    This kernel is designed for efficient matrix multiplication on GPUs,\n    incorporating features like block-wise computation, optional per-tensor or\n    per-block scaling, optional activation functions, and program ID (PID)\n    remapping for improved L2 cache utilization, particularly on multi-XCD\n    (Cross-Chip Die) hardware.\n\n    Parameters:\n        a_ptr: Pointer to the input matrix A.\n        b_ptr: Pointer to the input matrix B.\n        c_ptr: Pointer to the output matrix C.\n        M: The number of rows in matrix A and matrix C.\n        N: The number of columns in matrix B and matrix C.\n        K: The number of columns in matrix A and rows in matrix B (the reduction dimension).\n        stride_am: Stride for matrix A along the M dimension (row stride).\n        stride_ak: Stride for matrix A along the K dimension (column stride).\n        stride_bk: Stride for matrix B along the K dimension (row stride).\n        stride_bn: Stride for matrix B along the N dimension (column stride).\n        stride_cm: Stride for matrix C along the M dimension (row stride).\n        stride_cn: Stride for matrix C along the N dimension (column stride).\n        a_scale_ptr: Pointer to scale factors for matrix A. Used if `APPLY_SCALE` is 'tensor' or 'block'.\n                     If `APPLY_SCALE` is 'tensor', this points to a single scalar.\n                     If `APPLY_SCALE` is 'block', this points to a tensor of scales.\n        b_scale_ptr: Pointer to scale factors for matrix B. Used if `APPLY_SCALE` is 'tensor' or 'block'.\n                     If `APPLY_SCALE` is 'tensor', this points to a single scalar.\n                     If `APPLY_SCALE` is 'block', this points to a tensor of scales.\n        stride_ascale_m: Stride for A's scale tensor along its M-dimension (if `APPLY_SCALE` is 'block' and A scales are per-row-block).\n        stride_ascale_k: Stride for A's scale tensor along its K-dimension (if `APPLY_SCALE` is 'block' and A scales are per-K-group).\n        stride_bscale_k: Stride for B's scale tensor along its K-dimension (if `APPLY_SCALE` is 'block' and B scales are per-K-group).\n        stride_bscale_n: Stride for B's scale tensor along its N-dimension (if `APPLY_SCALE` is 'block' and B scales are per-N-group).\n        GROUP_K (tl.constexpr): Grouping factor for the K dimension when `APPLY_SCALE` is 'block'.\n                                Scales for A and B are loaded based on K-groups of this size.\n        GROUP_N (tl.constexpr): Grouping factor for the N dimension when `APPLY_SCALE` is 'block' for matrix B.\n                                Scales for B are loaded based on N-groups of this size.\n        BLOCK_SIZE_M (tl.constexpr): The tile size for the M dimension processed by each kernel instance.\n        BLOCK_SIZE_N (tl.constexpr): The tile size for the N dimension processed by each kernel instance.\n        BLOCK_SIZE_K (tl.constexpr): The tile size for the K dimension (reduction dimension) processed in each inner loop.\n        EVEN_K (tl.constexpr): Boolean flag. If True, K is assumed to be perfectly divisible by `BLOCK_SIZE_K`.\n                               If False, boundary checks (masking) are applied when loading data along the K dimension.\n        GROUP_SIZE_M (tl.constexpr): Number of M-dimension blocks to group together for program ID mapping.\n                                     This influences L2 data reuse.\n        APPLY_SCALE (tl.constexpr): Specifies how scaling is applied.\n                                    - `None`: No scaling.\n                                    - `'tensor'`: A single scale factor is applied to matrix A and another to matrix B.\n                                    - `'block'`: Scale factors are loaded and applied per block of A and/or B.\n        ACTIVATION (tl.constexpr): Specifies the activation function to apply after the accumulation and scaling.\n                                   Example: \"leaky_relu\". If `None`, no activation is applied.\n        GRID_MN (tl.constexpr): The total number of program instances (PIDs) launched for the M and N dimensions.\n                                This is typically `tl.cdiv(M, BLOCK_SIZE_M) * tl.cdiv(N, BLOCK_SIZE_N)`.\n                                Used for PID remapping across XCDs.\n    \"\"\"\n    # Your code here.\n\n\n",
        "label": "# Usage Instruction: python3 -m pytest gemm.py\n\nimport torch\nimport triton\nimport triton.language as tl\nimport sys\nimport argparse\nimport pytest\nimport re\n\n# This is a Triton kernel for matrix multiplication (GEMM) with support for various data types and scaling modes.\n\n#################### Helper utils functions ####################\n# Activation function.  \n@triton.jit  \ndef leaky_relu(x):  \n    x = x + 1  \n    return tl.where(x >= 0, x, 0.01 * x)  \n#################### Helper utils functions ####################\n\n\n\n@triton.autotune(  \n    configs=[  \n        triton.Config(  \n            {  \n                'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 4, 'waves_per_eu': 2,  \n                'kpack': 2, 'matrix_instr_nonkdim': 16  \n            }, num_warps=4, num_stages=2),  \n        triton.Config(  \n            {  \n                'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 4, 'waves_per_eu': 2,  \n                'kpack': 2, 'matrix_instr_nonkdim': 16  \n            }, num_warps=8, num_stages=2),  \n        triton.Config(  \n            {'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 4, 'waves_per_eu': 0},  \n            num_warps=8, num_stages=2),  \n        triton.Config(  \n            {  \n                'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 4, 'waves_per_eu': 2,  \n                'kpack': 1, 'matrix_instr_nonkdim': 16  \n            }, num_warps=8, num_stages=2),  \n        triton.Config(  \n            {  \n                'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 0,  \n                'kpack': 1  \n            }, num_warps=8, num_stages=2),  \n        triton.Config(  \n            {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 4, 'waves_per_eu': 0},  \n            num_warps=8, num_stages=2),  \n        triton.Config(  \n            {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2},  \n            num_warps=8, num_stages=2),  \n    ],  \n    key=['M', 'N', 'K'],  \n    use_cuda_graph=True,  \n)  \n@triton.heuristics({  \n    'EVEN_K':  \n    lambda args: args['K'] % args['BLOCK_SIZE_K'] == 0, 'GRID_MN':  \n    lambda args: triton.cdiv(args['M'], args['BLOCK_SIZE_M']) * triton.cdiv(args['N'], args['BLOCK_SIZE_N'])  \n})  \n@triton.jit  \ndef matmul_kernel(  \n    a_ptr,  \n    b_ptr,  \n    c_ptr,  \n    M,  \n    N,  \n    K,  \n    stride_am,  \n    stride_ak,  \n    stride_bk,  \n    stride_bn,  \n    stride_cm,  \n    stride_cn,  \n    a_scale_ptr,  \n    b_scale_ptr,  \n    stride_ascale_m,  \n    stride_ascale_k,  \n    stride_bscale_k,  \n    stride_bscale_n,  \n    # Meta-parameters  \n    GROUP_K: tl.constexpr,  \n    GROUP_N: tl.constexpr,  \n    BLOCK_SIZE_M: tl.constexpr,  \n    BLOCK_SIZE_N: tl.constexpr,  \n    BLOCK_SIZE_K: tl.constexpr,  \n    EVEN_K: tl.constexpr,  \n    GROUP_SIZE_M: tl.constexpr,  \n    APPLY_SCALE: tl.constexpr,  \n    ACTIVATION: tl.constexpr,  \n    GRID_MN: tl.constexpr,  \n):  \n    \"\"\"Kernel for computing the matmul C = A x B.  \n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)  \n    \"\"\"  \n  \n    NUM_XCDS: tl.constexpr = 8  \n  \n    tl.static_assert(((APPLY_SCALE is None) or (APPLY_SCALE == 'tensor')) or (APPLY_SCALE == 'block'),  \n                     f\"Scaling mode {APPLY_SCALE} is not supported!!!\")  \n  \n    tl.assume(stride_am > 0)  \n    tl.assume(stride_ak > 0)  \n    tl.assume(stride_bk > 0)  \n    tl.assume(stride_bn > 0)  \n    tl.assume(stride_cm > 0)  \n    tl.assume(stride_cn > 0)  \n    tl.assume(stride_ascale_m > 0)  \n    tl.assume(stride_ascale_k > 0)  \n    tl.assume(stride_bscale_k > 0)  \n    tl.assume(stride_bscale_n > 0)  \n  \n    # -----------------------------------------------------------  \n    # Map program ids `pid` to the block of C it should compute.  \n    # This is done in a grouped ordering to promote L2 data reuse.  \n    # TODO(vgokhale): Add XCD remapping.  \n    pid = tl.program_id(axis=0)  \n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)  \n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)  \n  \n    ## pid remapping on xcds  \n    # Number of pids per XCD in the new arrangement  \n    pids_per_xcd = (GRID_MN + NUM_XCDS - 1) // NUM_XCDS  \n    # When GRID_MN cannot divide NUM_XCDS, some xcds will have  \n    # pids_per_xcd pids, the other will have pids_per_xcd - 1 pids.  \n    # We calculate the number of xcds that have pids_per_xcd pids as  \n    # tall_xcds  \n    tall_xcds = GRID_MN % NUM_XCDS  \n    tall_xcds = NUM_XCDS if tall_xcds == 0 else tall_xcds  \n    # Compute current XCD and local pid within the XCD  \n    xcd = pid % NUM_XCDS  \n    local_pid = pid // NUM_XCDS  \n    # Calculate new pid based on the new grouping  \n    # Note that we need to consider the following two cases:  \n    # 1. the current pid is on a tall xcd  \n    # 2. the current pid is on a short xcd  \n    if xcd < tall_xcds:  \n        pid = xcd * pids_per_xcd + local_pid  \n    else:  \n        pid = tall_xcds * pids_per_xcd + (xcd - tall_xcds) * (pids_per_xcd - 1) + local_pid  \n  \n    if GROUP_SIZE_M == 1:  \n        pid_m = pid // num_pid_n  \n        pid_n = pid % num_pid_n  \n    else:  \n        num_pid_in_group = GROUP_SIZE_M * num_pid_n  \n        group_id = pid // num_pid_in_group  \n        first_pid_m = group_id * GROUP_SIZE_M  \n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)  \n        pid_m = first_pid_m + (pid % group_size_m)  \n        pid_n = (pid % num_pid_in_group) // group_size_m  \n  \n    tl.assume(pid_m > 0)  \n    tl.assume(pid_n > 0)  \n  \n    # Create pointers for first block of A and B input matrices  \n    offs_k = tl.arange(0, BLOCK_SIZE_K)  \n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M  \n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N  \n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  \n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)  \n    if APPLY_SCALE == 'tensor':  \n        a_scale = tl.load(a_scale_ptr) if a_scale_ptr else 1.0  \n        b_scale = tl.load(b_scale_ptr)  \n    elif APPLY_SCALE == 'block':  \n        k_start = 0  \n        offs_ks = k_start // GROUP_K  \n        a_scale_ptrs = None if a_scale_ptr is None else (a_scale_ptr + offs_am * stride_ascale_m +  \n                                                         offs_ks * stride_ascale_k)  \n        offs_bsn = offs_bn // GROUP_N  \n        b_scale_ptrs = b_scale_ptr + offs_bsn * stride_bscale_n + offs_ks * stride_bscale_k  \n  \n    acc_dtype = tl.float32 if c_ptr.type.element_ty != tl.int8 else tl.int32  \n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=acc_dtype)  \n  \n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):  \n        # Load the next block of A and B, generate a mask by checking the K dimension.  \n        # If it is out of bounds, set it to 0.  \n        if EVEN_K:  \n            a = tl.load(a_ptrs)  \n            b = tl.load(b_ptrs)  \n        else:  \n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)  \n            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)  \n  \n        if APPLY_SCALE == 'block':  \n            b_scale = tl.load(b_scale_ptrs)  \n            if a_scale_ptrs is not None:  \n                a_scale = tl.load(a_scale_ptrs)  \n  \n        # Type conversion to support mixed precision GEMMs where b is lower precision than a  \n        b = b.to(a_ptr.type.element_ty)  \n  \n        if APPLY_SCALE == 'block':  \n            if a_scale_ptrs is not None:  \n                accumulator += tl.dot(a, b, input_precision=\"ieee\") * a_scale[:, None] * b_scale[None, :]  \n            else:  \n                accumulator += tl.dot(a, b, input_precision=\"ieee\") * b_scale[None, :]  \n        else:  \n            accumulator += tl.dot(a, b, input_precision=\"ieee\")  \n  \n        # Advance the ptrs to the next K block.  \n        a_ptrs += BLOCK_SIZE_K * stride_ak  \n        b_ptrs += BLOCK_SIZE_K * stride_bk  \n  \n        if APPLY_SCALE == 'block':  \n            k_cur = k * BLOCK_SIZE_K // GROUP_K  \n            k_nxt = (k + 1) * BLOCK_SIZE_K // GROUP_K  \n            offs_ks = k_nxt - k_cur  \n            b_scale_ptrs += offs_ks * stride_bscale_k  \n            if a_scale_ptrs is not None:  \n                a_scale_ptrs += offs_ks * stride_ascale_k  \n  \n    # Apply scale to recover dynamic range reduced due to lower precision inputs.  \n    if APPLY_SCALE == 'tensor':  \n        accumulator = accumulator * a_scale * b_scale  \n    # Apply activation function, if specified.  \n    # TODO(vgokhale): Add different types of activations.  \n    if ACTIVATION == \"leaky_relu\":  \n        accumulator = leaky_relu(accumulator)  \n    c = accumulator.to(c_ptr.type.element_ty)  \n  \n    # Write back the block of the output matrix C with masks.  \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)  \n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)  \n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]  \n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)  \n    tl.store(c_ptrs, c, mask=c_mask)  \n  \n  \n\n\n\n##################################################################################################################################################\n\n# This is a Triton kernel for matrix multiplication (GEMM) with support for various data types and scaling modes.\n# Usage Instruction: python3 -m pytest gemm.py  \n  \nimport torch  \nimport triton  \nimport triton.language as tl  \nimport sys  \nimport argparse  \nimport pytest  \nimport re  \nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict  \n######################################## HELPERS for Eval ######################################## \nimport numpy as np\nimport random\nimport torch \nimport os\n\nresult_gold = {}\nCONFIG = {\n  \"llama3\": {\n    \"8B\": {\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"vocab_size\": 128256\n    },\n    \"70B\": {\n      \"num_attention_heads\": 64,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 8192,\n      \"intermediate_size\": 28672,\n      \"vocab_size\": 128256\n    },\n    \"405B\": {\n      \"num_attention_heads\": 128,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 16384,\n      \"intermediate_size\": 53248,\n      \"vocab_size\": 128256\n    }\n  },\n  \"mistral\": {\n    \"7B\": {\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    },\n    \"22B\": {\n      \"hidden_size\": 6144,\n      \"intermediate_size\": 16384,\n      \"num_attention_heads\": 48,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    }\n\n  }\n}\n\n  \ndef get_model_configs(config_path='model_configs.json', model_families=[\"llama3\"], model=\"all\"):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_path (str): User-provided path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        dict: A dictionary of available models and their configurations for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    # Extract models and their configurations for the specified families  \n    filtered_configs = {}  \n  \n    for family in model_families:  \n        if family in configs:  \n            # Check if model filtering is required  \n            if model == \"all\":  \n                # Include all models in the family  \n                for model_size, model_configs in configs[family].items():  \n                    filtered_configs[f\"{family}-{model_size}\"] = model_configs  \n            else:  \n                # Parse the model string (e.g., llama3_8B or llama3-8B)  \n                delimiter = \"_\" if \"_\" in model else \"-\"  \n                model_parts = model.split(delimiter)  \n  \n                # Check if the family and size match  \n                if len(model_parts) == 2 and model_parts[0] == family:  \n                    model_size = model_parts[1]  \n                    if model_size in configs[family]:  \n                        filtered_configs[f\"{family}-{model_size}\"] = configs[family][model_size]  \n  \n    if not filtered_configs:  \n        print(f\"Warning: No models selected for families: {model_families} with filter: '{model}'\")  \n  \n    return filtered_configs  \n  \n  \ndef get_available_models(config_file='model_configs.json', model_families=[\"llama3\"]):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_file (str): Path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        list: A list of available models for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    models = [f\"{family}-{model}\" for family in model_families if family in configs for model in configs[family]]  \n  \n    return models  \n\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n# --- Define TFLOPS and GB/s calculators for GEMM ---\ndef calculate_gemm_tflops(params: dict, ms: float) -> float:\n    M = params['M']\n    N = params['N']\n    K = params['K']\n    # For GEMM: 2 * M * N * K FLOPs\n    flops = 2 * M * N * K\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_gemm_gbps(params: dict, ms: float) -> float:\n    M = params['M']\n    N = params['N']\n    K = params['K']\n    # Dtypes are needed for element size\n    # Assuming params will contain 'in_dtype_a_str' and 'in_dtype_b_str'\n    # or we can infer from the created tensors if passed differently.\n    # For simplicity, let's assume fp16/bf16 for now if not specified.\n    # A more robust way is to pass tensor objects or precise dtype info.\n    try:\n        dtype_a = name_to_torch_types[params['in_dtype_a_str']]\n        dtype_b = name_to_torch_types[params['in_dtype_b_str']]\n        # dtype_c = name_to_torch_types[params['out_dtype_str']] # If C is different\n    except KeyError:\n        print(\"Warning: Dtype strings not found in params for GB/s calc, assuming float16.\")\n        dtype_a = torch.float16\n        dtype_b = torch.float16\n        # dtype_c = torch.float16\n\n    bytes_a = M * K * torch.tensor([], dtype=dtype_a).element_size()\n    bytes_b = K * N * torch.tensor([], dtype=dtype_b).element_size()\n    bytes_c = M * N * torch.tensor([], dtype=dtype_a).element_size() # Assuming C is same type as A for this calc\n\n    # Read A, Read B, Write C\n    total_bytes = bytes_a + bytes_b + bytes_c\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n######################################## HELPERS for Eval ######################################## \n   \nSCALE_BLOCK_SIZE = 128  \n  \n  \n# Wrapper for gemm kernel.  \ndef matmul(a, b, c, a_scale, b_scale, scale_a8_b8=None, activation=\"\"):  \n    # Check constraints.  \n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions!!!\"  \n    assert (a.element_size()  \n            >= b.element_size()), \"Mixed dtype GEMMs are only supported when data type of a is bigger than b!!!\"  \n    assert (a.is_floating_point() == b.is_floating_point()  \n            ), \"GEMMs between float and integer type tensors are not supported!!!\"  \n    assert (scale_a8_b8 in [None, 'tensor', 'block']), f\"Scaling mode {scale_a8_b8} is not supported!!!\"  \n    M, K = a.shape  \n    K, N = b.shape  \n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )  \n    matmul_kernel[grid](  \n        a,  \n        b,  \n        c,  \n        M,  \n        N,  \n        K,  \n        a.stride(0),  \n        a.stride(1),  \n        b.stride(0),  \n        b.stride(1),  \n        c.stride(0),  \n        c.stride(1),  \n        a_scale,  \n        b_scale,  \n        a_scale.stride(0) if (a_scale is not None) and a_scale.ndim else 0,  \n        a_scale.stride(1) if (a_scale is not None) and a_scale.ndim else 0,  \n        b_scale.stride(0) if (b_scale is not None) and b_scale.ndim else 0,  \n        b_scale.stride(1) if (b_scale is not None) and b_scale.ndim else 0,  \n        GROUP_K=SCALE_BLOCK_SIZE,  \n        GROUP_N=SCALE_BLOCK_SIZE,  \n        APPLY_SCALE=scale_a8_b8,  \n        ACTIVATION=activation,  \n    )  \n  \n  \ndef is_cdna4():  \n    return triton.runtime.driver.active.get_current_target().arch == 'gfx950'  \n  \n  \ne5m2_type = torch.float8_e5m2 if is_cdna4() else torch.float8_e5m2fnuz  \ne4m3_type = torch.float8_e4m3fn if is_cdna4() else torch.float8_e4m3fnuz  \n  \nname_to_torch_types = {  \n    'int8': torch.int8,  \n    'int32': torch.int32,  \n    'fp16': torch.float16,  \n    'fp32': torch.float32,  \n    'bf16': torch.bfloat16,  \n    'fp8e5': e5m2_type,  \n    'fp8e4': e4m3_type,  \n}  \n  \ndtype_max = {  \n    dtype: (torch.finfo(dtype) if dtype.is_floating_point else torch.iinfo(dtype)).max  \n    for dtype in [  \n        e5m2_type,  \n        e4m3_type,  \n        torch.int8,  \n    ]  \n}  \n  \n  \ndef dtype_is_8_bit(dtype):\n    return (\n        dtype is e5m2_type\n        or dtype is e4m3_type\n        or dtype is torch.int8\n    )\n  \n  \ndef gen_input(M, N, dtype, needTrans, seed=0, fp8_scaling_mode='tensor', device='cuda'):  \n    set_seed()  \n  \n    if needTrans:  \n        raw_data = torch.randn((N, M), dtype=torch.float32, device='cuda').T  \n    else:  \n        raw_data = torch.randn((M, N), dtype=torch.float32, device='cuda')  \n    scale = None  \n    if dtype_is_8_bit(dtype):  \n        if fp8_scaling_mode == 'token':  \n            assert raw_data.size(1) % SCALE_BLOCK_SIZE == 0  \n            raw_data = raw_data.view(M, -1, SCALE_BLOCK_SIZE)  \n            max_val = raw_data.abs().float().amax(dim=2).view(M, -1).clamp(1e-4)  \n            scale = max_val.unsqueeze(2) / dtype_max[dtype]  \n            raw_data = (raw_data / scale).view(M, N)  \n            scale = scale.view(M, -1)  \n            scale = scale.T.contiguous().T  \n        elif fp8_scaling_mode == 'block':  \n            x_padded = torch.zeros((triton.cdiv(M, SCALE_BLOCK_SIZE) * SCALE_BLOCK_SIZE,  \n                                    triton.cdiv(N, SCALE_BLOCK_SIZE) * SCALE_BLOCK_SIZE), dtype=raw_data.dtype,  \n                                   device=raw_data.device)  \n            x_padded[:M, :N] = raw_data  \n            x_view = x_padded.view(-1, SCALE_BLOCK_SIZE, x_padded.size(1) // SCALE_BLOCK_SIZE, SCALE_BLOCK_SIZE)  \n            x_amax = x_view.abs().float().amax(dim=(1, 3), keepdim=True).clamp(1e-4)  \n            x_scaled = x_view * (dtype_max[dtype] / x_amax)  \n            raw_data = x_scaled.view_as(x_padded)[:M, :N].T.contiguous().T  \n            scale = (x_amax / dtype_max[dtype]).view(x_view.size(0), x_view.size(2))  \n        elif fp8_scaling_mode == 'tensor':  \n            max_val = torch.max(torch.abs(raw_data))  \n            scale = max_val / dtype_max[dtype]  \n            raw_data = raw_data / scale  \n  \n    input = raw_data.to(dtype)  \n    input_f32 = input.to(torch.float32)  \n  \n    return input, input_f32, scale  \n  \n  \ndef get_x_vals():  \n    x_vals = [(1024 * v, 1024 * v, 1024 * v) for v in range(1, 9)]  \n  \n    x_vals += [(4864, 4096, 8192), (9728, 8192, 65536), (4864, 8192, 4160)]  \n  \n    return x_vals  \n  \n  \n# Unit tests  \n#TODO(vgokhale): Test activation.  \n# yapf: disable  \n@pytest.mark.parametrize(  \n    \"M, N, K, in_dtype_a, in_dtype_b, out_dtype, col_a, col_b\",  \n    [(*shape, in_dtype_a, in_dtype_b, out_dtype, col_a, col_b)  \n     for shape in get_x_vals()  \n     for in_dtype_a, in_dtype_b, out_dtype in [  \n        ('fp16', 'fp16', 'fp16'),   ('bf16', 'bf16', 'bf16'),   ('fp32', 'fp32', 'fp32'),  \n        ('fp8e4', 'fp8e4', 'fp16'), ('fp8e5', 'fp8e5', 'fp16'), ('fp16', 'fp8e4', 'fp16'),  \n        ('fp16', 'fp8e5', 'fp16'),  ('bf16', 'fp8e4', 'bf16'),  ('bf16', 'fp8e5', 'bf16'),  \n        ('int8', 'int8', 'int8'),   ('int8', 'int8', 'int32')]  \n     # Defines if a matrix is row or column major.  \n     for col_a in [True, False]  \n     for col_b in [True, False]])  \n# yapf: enable  \ndef test_correctness(M, N, K, col_a, col_b, in_dtype_a, in_dtype_b, out_dtype, request):  \n    set_seed()\n    torch_in_dtype_a = name_to_torch_types[in_dtype_a]  \n    torch_in_dtype_b = name_to_torch_types[in_dtype_b]  \n    a, a_fp32, a_scale = gen_input(M, K, torch_in_dtype_a, col_a, seed=1, device='cuda')  \n    b, b_fp32, b_scale = gen_input(K, N, torch_in_dtype_b, col_b, seed=2, device='cuda')  \n    torch_out_dtype = name_to_torch_types[out_dtype]  \n    c = torch.empty((M, N), device=a.device, dtype=torch_out_dtype)  \n    # For 8-bit, we have scaled to the dynamic range of the data type.  \n    # This requires us to compute in fp32 because for e5m2, the range is same as fp16 (e5m10).  \n    # If we use fp16 it is possible to return infs from the torch.matmul call.  \n    if dtype_is_8_bit(torch_in_dtype_a) or dtype_is_8_bit(torch_in_dtype_b):  \n        matmul(a, b, c, a_scale, b_scale, scale_a8_b8='tensor', activation=\"\")  \n        torch_output = torch.matmul(a_fp32, b_fp32)  \n        # Set a_scale to 1.0 if it is not set  \n        torch_output = torch_output * (a_scale or 1.0) * b_scale  \n    # For other dtypes, use the same torch matmul as the dtype.  \n    else:  \n        matmul(a, b, c, a_scale=None, b_scale=None, scale_a8_b8=None, activation=\"\")  \n        torch_output = torch.matmul(a.to(torch_in_dtype_a), b.to(torch_in_dtype_b))  \n    \n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n\n    if out_dtype == 'int8':  \n        ################### save c in result_gold ###################\n        test_case_name = request.node.name\n        sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n        result_gold[sanitized_key_name] = c.to(torch.float32).clone().detach().cpu()\n        ###################################################################\n        # torch.testing.assert_close(c.to(torch.float32),  \n        #                            torch_output.to(torch.int8).to(torch.float32), atol=1e-3, rtol=1e-2)  \n    else:  \n        ################### save c in result_gold ###################\n        test_case_name = request.node.name\n        sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n        result_gold[sanitized_key_name] = c.clone().detach().cpu()\n        ###################################################################\n        # torch.testing.assert_close(c, torch_output.to(torch_out_dtype), atol=5e-3, rtol=1e-2)  \n  \nOP_NAME_FOR_BENCHMARK = \"gemm_triton_perf\"\n\n@pytest.mark.parametrize(\n    \"M, N, K, in_dtype_a_str, in_dtype_b_str, out_dtype_str, col_a, col_b\",\n    [(*shape, ida, idb, od, ca, cb)\n     for shape in get_x_vals() # Uses the reduced get_x_vals for faster testing\n     for ida, idb, od in [\n        ('fp16', 'fp16', 'fp16'), ('bf16', 'bf16', 'bf16'), ('fp32', 'fp32', 'fp32'),\n        # ('fp8e4', 'fp8e4', 'fp16'), ('fp8e5', 'fp8e5', 'fp16'), # FP8 needs careful scale handling\n        # ('int8', 'int8', 'int32') # Int8 also needs care\n        ]\n     for ca in [False] # Simplified: only row-major A for now\n     for cb in [False]] # Simplified: only row-major B for now\n)\ndef test_performance(M, N, K, col_a, col_b, in_dtype_a_str, in_dtype_b_str, out_dtype_str, request):\n    set_seed() # Consistent seed for input data generation\n    torch_in_dtype_a = name_to_torch_types[in_dtype_a_str]\n    torch_in_dtype_b = name_to_torch_types[in_dtype_b_str]\n    torch_out_dtype = name_to_torch_types[out_dtype_str]\n\n    # --- Input Generation (from original test_correctness) ---\n    # Determine fp8_scaling_mode based on dtypes, or make it a parameter\n    fp8_mode_a = 'tensor' if dtype_is_8_bit(torch_in_dtype_a) else None\n    fp8_mode_b = 'tensor' if dtype_is_8_bit(torch_in_dtype_b) else None\n\n    a, a_fp32_ref, a_scale = gen_input(M, K, torch_in_dtype_a, col_a, seed=1, fp8_scaling_mode=fp8_mode_a or 'tensor', device='cuda')\n    b, b_fp32_ref, b_scale = gen_input(K, N, torch_in_dtype_b, col_b, seed=2, fp8_scaling_mode=fp8_mode_b or 'tensor', device='cuda')\n    c = torch.empty((M, N), device=a.device, dtype=torch_out_dtype)\n\n    # Determine scale_a8_b8 and activation for matmul wrapper\n    current_scale_a8_b8 = None\n    current_activation = \"\" # No activation for pure GEMM perf\n    if dtype_is_8_bit(torch_in_dtype_a) or dtype_is_8_bit(torch_in_dtype_b):\n        current_scale_a8_b8 = 'tensor' # Or make this part of parametrize if testing block/token scaling perf\n\n    # --- Create op_lambda for benchmarking ---\n    # This lambda will call the matmul wrapper\n    op_lambda = lambda: matmul(a, b, c, a_scale, b_scale,\n                               scale_a8_b8=current_scale_a8_b8,\n                               activation=current_activation)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=10, repetition=100) # Adjust for GEMM complexity\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K,\n        \"in_dtype_a_str\": in_dtype_a_str, \"in_dtype_b_str\": in_dtype_b_str, \"out_dtype_str\": out_dtype_str,\n        \"col_a\": col_a, \"col_b\": col_b,\n        \"fp8_scaling_a\": fp8_mode_a, \"fp8_scaling_b\": fp8_mode_b,\n        \"triton_scale_mode\": current_scale_a8_b8, \"activation\": current_activation\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_gemm_gbps,\n                              tflops_calculator=calculate_gemm_tflops)\n  \ndef get_type(provider):  \n    res = re.findall(r'\\(.*?\\)', provider)  \n    return res[0][1:-1].split('/', 1)  \n  \n  \n@triton.testing.perf_report(  \n    triton.testing.Benchmark(  \n        x_names=['M', 'N', 'K'],  \n        x_vals=get_x_vals(),  \n        line_arg='provider',  \n        line_vals=[  \n            'hipblaslt(fp16/fp16)', 'hipblaslt(bf16/bf16)', 'triton(fp16/fp16)', 'triton(bf16/bf16)',  \n            'triton(int8/int8)', 'triton(fp8e4/fp8e4)', 'triton(fp8e5/fp8e5)', 'triton(fp16/fp8e4)',  \n            'triton(fp16/fp8e5)'  \n        ],  \n        line_names=[  \n            \"rocBLAS.Fp16\", \"rocBLAS.Bf16\", \"Triton.Fp16\", \"Triton.Bf16\", \"Triton.Int8\", \"Triton.Fp8E4\", \"Triton.Fp8E5\",  \n            \"Triton.Fp16.Fp8E4\", \"Triton.Fp16.Fp8E5\"  \n        ],  \n        ylabel=\"TFLOPS\",  \n        plot_name=\"matmul-performance\",  \n        args={},  \n    ))  \ndef benchmark(M, N, K, provider, model=None, args=None):  \n    in_dtype_a, in_dtype_b = [name_to_torch_types[x] for x in get_type(provider)]  \n    out_dtype = in_dtype_a  \n  \n    quantiles = [0.5, 0.2, 0.8]  \n    layout_tn = args.layout == 'tn'  \n  \n    if args.fp8_scaling_mode == 'tensor' or in_dtype_b == torch.int8:  \n        a, _, a_scale = gen_input(M, K, in_dtype_a, False, seed=1, device='cuda')  \n        b, _, b_scale = gen_input(K, N, in_dtype_b, layout_tn, seed=2, device='cuda')  \n    else:  \n        a, _, a_scale = gen_input(M, K, in_dtype_a, False, seed=1, fp8_scaling_mode='token', device='cuda')  \n        b, _, b_scale = gen_input(K, N, in_dtype_b, layout_tn, seed=2, fp8_scaling_mode='block', device='cuda')  \n  \n    if 'hipblaslt' in provider:  \n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)  \n    else:  # triton, different data types  \n        assert \"triton\" in provider  \n        # Allocates output.  \n        c = torch.empty((M, N), device=a.device, dtype=out_dtype)  \n  \n        # If data type is 8 bit  \n        #   Default to tensor scaling if scaling mode is tensor or dtype is int8  \n        #   Use block scaling otherwise  \n        scale_a8_b8 = None  \n        if dtype_is_8_bit(in_dtype_a) or dtype_is_8_bit(in_dtype_b):  \n            scale_a8_b8 = 'tensor' if in_dtype_b == torch.int8 else args.fp8_scaling_mode  \n  \n        ms, min_ms, max_ms = triton.testing.do_bench(  \n            lambda: matmul(a, b, c, a_scale, b_scale, scale_a8_b8=scale_a8_b8, activation=\"\"), quantiles=quantiles)  \n        if args.v:  \n            print(f'Best tuning config for M={M}, N={N}, K={K}, '  \n                  f'dtype={in_dtype_a} / {in_dtype_b} / {out_dtype}: \\n({matmul_kernel.best_config})\\n')  \n    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)  \n    return perf(ms), perf(max_ms), perf(min_ms)  \n  \n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all c_triton_dot results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} c_triton_dot tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n######################################## HELPERS for Eval ######################################## \n  \ndef parse_args():  \n    parser = argparse.ArgumentParser(  \n        prog=\"AMD Triton GEMM kernel\",  \n        allow_abbrev=False,  \n    )  \n  \n    parser.add_argument('-model_configs', type=str, default=\"model_configs.json\", help=\"Model config json file.\")  \n  \n    available_models = get_available_models(model_families=[\"llama3\"])  # Dynamically load model names  \n    model_help = (  \n        \"Model name to benchmark. Select from: [\" + \", \".join(available_models) +  \n        \"]. Use 'all' to benchmark all models. Not providing runs the default benchmark script with custom configs.\")  \n    parser.add_argument('-model', type=str, default=None, help=model_help)  \n    parser.add_argument(\"-v\", action='store_true', default=False, help=\"Print out the best tuning config\")  \n    parser.add_argument(\"-M\", type=int, default=0)  \n    parser.add_argument(\"-N\", type=int, default=0)  \n    parser.add_argument(\"-K\", type=int, default=0)  \n    parser.add_argument(\"-layout\", type=str, default='tn')  \n    parser.add_argument(\"-dtype\", type=str, default=None, help=\"Data type of inputs and outputs\")  \n    parser.add_argument(\"-b_dtype\", type=str, default=None,  \n                        help=\"Data type of B operand, if specified (else same as dtype)\")  \n    parser.add_argument(\"-fp8_scaling_mode\", type=str, default='tensor', choices=['tensor', 'block'],  \n                        help=\"Type of scaling to apply when either or both inputs are fp8\")  \n  \n    args = parser.parse_args()  \n  \n    return args  \n  \n  \ndef get_line_vals_names(a_dtype=None, b_dtype=None):  \n    line_vals = [  \n        'hipblaslt(fp16/fp16)', 'hipblaslt(bf16/bf16)', 'triton(fp16/fp16)', 'triton(bf16/bf16)', 'triton(int8/int8)',  \n        'triton(fp8e4/fp8e4)', 'triton(fp8e5/fp8e5)', 'triton(fp16/fp8e4)', 'triton(fp16/fp8e5)'  \n    ]  \n    line_names = [  \n        \"rocBLAS.Fp16\", \"rocBLAS.Bf16\", \"Triton.Fp16\", \"Triton.Bf16\", \"Triton.Int8\", \"Triton.Fp8E4\", \"Triton.Fp8E5\",  \n        \"Triton.Fp16.Fp8E4\", \"Triton.Fp16.Fp8E5\"  \n    ]  \n    assert not ((a_dtype is None) ^ (b_dtype is None))  \n    if a_dtype is not None:  \n        line_vals_suffix_str = '(' + a_dtype + '/' + b_dtype + ')'  \n        line_names_suffix_str = '.' + a_dtype + '.' + b_dtype  \n        line_vals = ['triton' + line_vals_suffix_str]  \n        line_names = ['Triton' + line_names_suffix_str]  \n        if (not dtype_is_8_bit(name_to_torch_types[a_dtype])\n            and not dtype_is_8_bit(name_to_torch_types[b_dtype])):\n            line_vals += ['hipblaslt' + line_vals_suffix_str]\n            line_names += ['hipblaslt' + line_names_suffix_str]\n  \n    return line_vals, line_names  \n  \n  \ndef main():  \n    args = parse_args()  \n  \n    if args.model:  \n        config_file = args.model_configs  \n        configs = get_model_configs(config_path=config_file, model_families=[\"llama3\"], model=args.model)  \n        mnk_list = []  \n  \n        for model_name, config in configs.items():  \n            M, N, K = args.M or 8192, config[\"hidden_size\"], config[\"intermediate_size\"]  \n            mnk_list.append((model_name, M, N, K))  \n  \n        benchmark.benchmarks.x_names = ['model', 'M', 'N', 'K']  \n        benchmark.benchmarks.x_vals = mnk_list  \n  \n    a_dtype = args.dtype  \n    b_dtype = args.b_dtype or args.dtype  \n    assert a_dtype is None or a_dtype in name_to_torch_types, f\"Unsupported dtype {a_dtype}\"  \n    assert b_dtype is None or b_dtype in name_to_torch_types, f\"Unsupported dtype {b_dtype}\"  \n    benchmark.benchmarks.line_vals, benchmark.benchmarks.line_names = get_line_vals_names(a_dtype, b_dtype)  \n    if args.N or args.K:  \n        assert args.model is None, \"Providing both -model and N/K is not compatible! -model already fixes N/K.\"  \n  \n    if args.M and args.N and args.K:  \n        x_vals = [(args.M, args.N, args.K)]  \n        benchmark.benchmarks.x_vals = x_vals  \n  \n    benchmark.run(show_plots=True, print_data=True, args=args)  \n  \n  \nif __name__ == '__main__':  \n    sys.exit(main())  \n"
    },
    {
        "file": "layernorm.py",
        "target_kernel_name": "layernorm_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `layernorm_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list ,\n\nThis kernel, `layernorm_kernel`,  is designed to perform layer normalization on the input tensor.\n\n**Your objective is to implement the body of `layernorm_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `layernorm_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\n\n\nThe full definition for `layernorm_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `layernorm_kernel` whilst keeping other things intact.\n\n\nimport argparse  \nimport sys  \nimport pytest  \n\nimport torch  \nimport triton  \nimport triton.language as tl  \nimport os  \nimport json  \nimport math  \nfrom itertools import product  \n\n######################################## HELPERS utils ######################################## \ndef is_cuda():  \n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"  \n\n\ndef is_hip():  \n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"  \n\n\ndef get_cuda_autotune_config():  \n    return [  \n        triton.Config({}, num_warps=4, num_stages=1),  \n        triton.Config({}, num_warps=8, num_stages=1),  \n        triton.Config({}, num_warps=16, num_stages=1),  \n    ]  \n\n\ndef get_hip_autotune_config():  \n    return [  \n        triton.Config({'waves_per_eu': we}, num_warps=wa, num_stages=1) for we, wa in product([1, 2, 4], [4, 8, 16])  \n    ]  \n\n\ndef get_autotune_config():  \n    if is_cuda():  \n        return get_cuda_autotune_config()  \n    else:  \n        return get_hip_autotune_config()  \n\n######################################## HELPERS utils ######################################## \n@triton.autotune(configs=get_autotune_config(), key=['n_rows', 'n_cols'], use_cuda_graph=True)  \n@triton.jit  \ndef layernorm_kernel(x_ptr, y_ptr, w_ptr, b_ptr, mean_ptr, rstd_ptr, x_row_stride, y_row_stride, n_rows, n_cols, eps,  \n                     BLOCK_SIZE: tl.constexpr):  \n  \"\"\"  \n  Performs Layer Normalization on an input tensor.  \n\n  This kernel normalizes each row of the input tensor `x` independently.  \n  For each row, it calculates the mean and variance across its columns (features).  \n  It then normalizes the row using these statistics, applies a learnable affine  \n  transformation (scale `w` and bias `b`), and stores the result in `y`.  \n  The per-row mean and reciprocal standard deviation (rstd) are also stored.  \n\n  Args:  \n      x_ptr (triton.language.tensor): Pointer to the input tensor of shape (n_rows, n_cols).  \n      y_ptr (triton.language.tensor): Pointer to the output tensor of shape (n_rows, n_cols),  \n                                      where the normalized and transformed data will be stored.  \n      w_ptr (triton.language.tensor): Pointer to the weight tensor (gamma) of shape (n_cols).  \n                                      Used for scaling the normalized input.  \n      b_ptr (triton.language.tensor): Pointer to the bias tensor (beta) of shape (n_cols).  \n                                      Used for shifting the normalized input.  \n      mean_ptr (triton.language.tensor): Pointer to a tensor of shape (n_rows) where the  \n                                         calculated mean for each row will be stored.  \n      rstd_ptr (triton.language.tensor): Pointer to a tensor of shape (n_rows) where the  \n                                         calculated reciprocal standard deviation  \n                                         (1/sqrt(variance + eps)) for each row will be stored.  \n      x_row_stride (int): The stride (number of elements) to move from one row  \n                          to the next in the `x_ptr` tensor.  \n      y_row_stride (int): The stride (number of elements) to move from one row  \n                          to the next in the `y_ptr` tensor.  \n      n_rows (int): The number of rows in the input tensor `x`. Each row is  \n                    processed independently by a separate program instance.  \n      n_cols (int): The number of columns (features) in the input tensor `x`.  \n                    Normalization is performed across these columns for each row.  \n      eps (float): A small constant added to the variance for numerical stability  \n                   before calculating the reciprocal square root.  \n      BLOCK_SIZE (tl.constexpr): A compile-time constant defining the size of blocks  \n                                 used to process columns. This influences how data is  \n                                 loaded and processed in parallel within a row.  \n  \"\"\"  \n    # Your code here.\n\n\n",
        "label": "import argparse  \nimport sys  \nimport pytest  \n  \nimport torch  \nimport triton  \nimport triton.language as tl  \nimport os  \nimport json  \nimport math  \nfrom itertools import product  \n  \n######################################## HELPERS utils ######################################## \ndef is_cuda():  \n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"  \n  \n  \ndef is_hip():  \n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"  \n  \n  \ndef get_cuda_autotune_config():  \n    return [  \n        triton.Config({}, num_warps=4, num_stages=1),  \n        triton.Config({}, num_warps=8, num_stages=1),  \n        triton.Config({}, num_warps=16, num_stages=1),  \n    ]  \n  \n  \ndef get_hip_autotune_config():  \n    return [  \n        triton.Config({'waves_per_eu': we}, num_warps=wa, num_stages=1) for we, wa in product([1, 2, 4], [4, 8, 16])  \n    ]  \n  \n  \ndef get_autotune_config():  \n    if is_cuda():  \n        return get_cuda_autotune_config()  \n    else:  \n        return get_hip_autotune_config()  \n  \n######################################## HELPERS utils ######################################## \n\n\n@triton.autotune(configs=get_autotune_config(), key=['n_rows', 'n_cols'], use_cuda_graph=True)  \n@triton.jit  \ndef layernorm_kernel(x_ptr, y_ptr, w_ptr, b_ptr, mean_ptr, rstd_ptr, x_row_stride, y_row_stride, n_rows, n_cols, eps,  \n                     BLOCK_SIZE: tl.constexpr):  \n  \n    #program id  \n    row = tl.program_id(0)  \n    x_ptr_start = x_ptr + (row * x_row_stride)  \n    y_ptr_start = y_ptr + (row * y_row_stride)  \n  \n    loop_num = tl.cdiv(n_cols, BLOCK_SIZE) - 1  \n  \n    #calculate mean  \n    mean = 0  \n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)  \n    loop_num_l = loop_num  \n    for b in range(0, loop_num_l):  \n        col_offsets = b * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)  \n        x_block = tl.load(x_ptr_start + col_offsets).to(tl.float32)  #Unmasked loads  \n        _mean += x_block  \n  \n    #For last iteration, do masked load  \n    col_offsets = loop_num_l * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)  \n    x_block = tl.load(x_ptr_start + col_offsets, mask=col_offsets < n_cols, other=0.).to(tl.float32)  \n    _mean += x_block  \n    mean = tl.sum(_mean, axis=0) / n_cols  \n  \n    #variance  \n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)  \n    loop_num_l = loop_num  \n    for b in range(0, loop_num_l):  \n        col_offsets = b * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)  \n        x_block = tl.load(x_ptr_start + col_offsets).to(tl.float32)  #Unmasked loads  \n        x_block = x_block - mean  \n        _var += x_block * x_block  \n  \n    #For last iteration, do masked load  \n    col_offsets = loop_num_l * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)  \n    x_block = tl.load(x_ptr_start + col_offsets, mask=col_offsets < n_cols, other=0.).to(tl.float32)  \n    x_block = tl.where(col_offsets < n_cols, x_block - mean, 0.)  \n    _var += x_block * x_block  \n  \n    var = tl.sum(_var, axis=0) / n_cols  \n    rstd = tl.rsqrt(var + eps)  \n  \n    # Write mean / rstd  \n    tl.store(mean_ptr + row, mean)  \n    tl.store(rstd_ptr + row, rstd)  \n  \n    #Normalize and store  \n    loop_num_l = loop_num  \n    for b in range(0, loop_num_l):  \n        col_offsets = b * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)  \n        w_block = tl.load(w_ptr + col_offsets)  \n        b_block = tl.load(b_ptr + col_offsets)  \n        x_block = tl.load(x_ptr_start + col_offsets).to(tl.float32)  \n        y_block = (x_block - mean) * rstd  \n        y_block = y_block * w_block + b_block  \n        tl.store(y_ptr_start + col_offsets, y_block)  \n  \n    #For last iteration, do masked load and store  \n    col_offsets = loop_num_l * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)  \n    mask = col_offsets < n_cols  \n    w_block = tl.load(w_ptr + col_offsets, mask=mask, other=0.0)  \n    b_block = tl.load(b_ptr + col_offsets, mask=mask, other=0.0)  \n    x_block = tl.load(x_ptr_start + col_offsets, mask=mask, other=0.0).to(tl.float32)  \n    y_block = (x_block - mean) * rstd  \n    y_block = y_block * w_block + b_block  \n    tl.store(y_ptr_start + col_offsets, y_block, mask=mask)  \n  \n  \n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n  \n\n@triton.jit  \ndef _layer_norm_bwd_dx_fused(DX,  # pointer to the input gradient  \n                             DY,  # pointer to the output gradient  \n                             DW,  # pointer to the partial sum of weights gradient  \n                             DB,  # pointer to the partial sum of biases gradient  \n                             X,  # pointer to the input  \n                             W,  # pointer to the weights  \n                             Mean,  # pointer to the mean  \n                             Rstd,  # pointer to the 1/std  \n                             stride,  # how much to increase the pointer when moving by 1 row  \n                             N,  # number of columns in X  \n                             NUM_ROWS: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):  \n    # Map the program id to the elements of X, DX, and DY it should compute.  \n    pid = tl.program_id(0)  \n    pid_n = tl.program_id(1)  \n    tile_num = tl.num_programs(0)  \n    rows_per_tile = NUM_ROWS // tile_num  \n    if pid < NUM_ROWS % tile_num:  \n        rows_per_tile += 1  \n  \n    cols = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)  \n    mask = cols < N  \n    row = pid  \n    for _ in range(0, rows_per_tile):  \n        x_ptrs = X + row * stride  \n        dy_ptrs = DY + row * stride  \n        dx_ptrs = DX + row * stride  \n        dw_ptrs = DW + pid * N + cols  \n        db_ptrs = DB + pid * N + cols  \n        # Load data to SRAM  \n        x = tl.load(x_ptrs + cols, mask=mask, other=0).to(tl.float32)  \n        dy = tl.load(dy_ptrs + cols, mask=mask, other=0).to(tl.float32)  \n        w = tl.load(W + cols, mask=mask).to(tl.float32)  \n        mean = tl.load(Mean + row)  \n        rstd = tl.load(Rstd + row)  \n        # Compute dx  \n        xhat = (x - mean) * rstd  \n        wdy = w * dy  \n        xhat = tl.where(mask, xhat, 0.)  \n        wdy = tl.where(mask, wdy, 0.)  \n        c1 = tl.sum(xhat * wdy, axis=0) / N  \n        c2 = tl.sum(wdy, axis=0) / N  \n        dx = (wdy - (xhat * c1 + c2)) * rstd  \n        # Write dx  \n        tl.store(dx_ptrs + cols, dx, mask=mask)  \n        # Accumulate partial sums for dw/db  \n        partial_dw = (dy * xhat).to(w.dtype)  \n        partial_db = (dy).to(w.dtype)  \n        partial_dw += tl.load(dw_ptrs, mask=mask)  \n        partial_db += tl.load(db_ptrs, mask=mask)  \n        tl.store(dw_ptrs, partial_dw, mask=mask)  \n        tl.store(db_ptrs, partial_db, mask=mask)  \n        row += tile_num  \n  \n  \n@triton.jit  \ndef _layer_norm_bwd_dwdb(DW,  # pointer to the partial sum of weights gradient  \n                         DB,  # pointer to the partial sum of biases gradient  \n                         FINAL_DW,  # pointer to the weights gradient  \n                         FINAL_DB,  # pointer to the biases gradient  \n                         M,  # GROUP_SIZE_M  \n                         N,  # number of columns  \n                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):  \n    # Map the program id to the elements of DW and DB it should compute.  \n    pid = tl.program_id(0)  \n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)  \n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)  \n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)  \n    # Iterate through the rows of DW and DB to sum the partial sums.  \n    for i in range(0, M, BLOCK_SIZE_M):  \n        rows = i + tl.arange(0, BLOCK_SIZE_M)  \n        mask = (rows[:, None] < M) & (cols[None, :] < N)  \n        offs = rows[:, None] * N + cols[None, :]  \n        dw += tl.load(DW + offs, mask=mask, other=0.)  \n        db += tl.load(DB + offs, mask=mask, other=0.)  \n    # Write the final sum to the output.  \n    sum_dw = tl.sum(dw, axis=0)  \n    sum_db = tl.sum(db, axis=0)  \n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)  \n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)  \n\n\n\nclass LayerNorm(torch.autograd.Function):  \n  \n    @staticmethod  \n    def forward(ctx, x, normalized_shape, weight, bias, eps=1e-5):  \n        y = torch.empty_like(x)  \n        M, N = x.shape  \n        mean = torch.empty((M, ), dtype=torch.float32, device=x.device)  \n        rstd = torch.empty((M, ), dtype=torch.float32, device=x.device)  \n        # Less than 64KB per feature: enqueue fused kernel  \n        MAX_FUSED_SIZE = 65536 // x.element_size()  \n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))  \n        # heuristics for number of warps  \n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)  \n        layernorm_kernel[(M, )](x, y, weight, bias, mean, rstd, x.stride(0), y.stride(0), M, N, eps, BLOCK_SIZE)  \n        ctx.save_for_backward(x, weight, bias, mean, rstd)  \n        ctx.BLOCK_SIZE = BLOCK_SIZE  \n        ctx.num_warps = num_warps  \n        ctx.eps = eps  \n  \n        return y  \n  \n    @staticmethod  \n    def backward(ctx, dy):  \n        x, w, b, m, v = ctx.saved_tensors  \n        N = w.shape[0]  \n        x_arg = x.reshape(-1, x.shape[-1])  \n        M = x_arg.shape[0]  \n        tile_num = max(min(256, M // 4), 1)  \n        # allocate output  \n        _dw = torch.zeros((tile_num, N), dtype=x.dtype, device=w.device)  \n        _db = torch.zeros((tile_num, N), dtype=x.dtype, device=w.device)  \n        dw = torch.empty((N, ), dtype=w.dtype, device=w.device)  \n        db = torch.empty((N, ), dtype=w.dtype, device=w.device)  \n        dx = torch.empty_like(dy)  \n  \n        # enqueue kernel using forward pass heuristics  \n        # also compute partial sums for DW and DB  \n        M, N = x_arg.shape  \n        grid_bwd = lambda meta: (tile_num, triton.cdiv(N, meta['BLOCK_SIZE_N']))  \n        _layer_norm_bwd_dx_fused[grid_bwd](  #  \n            dx, dy, _dw, _db, x, w, m, v,  #  \n            x_arg.stride(0), N,  #  \n            NUM_ROWS=M,  #  \n            BLOCK_SIZE_N=ctx.BLOCK_SIZE,  #  \n            num_warps=ctx.num_warps)  \n        grid_reduce = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]  \n        # accumulate partial sums in separate kernel  \n        _layer_norm_bwd_dwdb[grid_reduce](  \n            _dw, _db, dw, db, min(tile_num, M), N,  #  \n            BLOCK_SIZE_M=32,  #  \n            BLOCK_SIZE_N=128)  \n  \n        return dx, None, dw, db, None  \n  \n  \nlayernorm = LayerNorm.apply  \n  \n  \ndef torch_layernorm(x, w_shape, w, b):  \n    M, N = x.shape  \n    y_torch = torch.nn.functional.layer_norm(x, w_shape, w, b, eps=1e-5)  \n    return y_torch  \n  \n  \ndef run_layernorm(M, N):  \n    print(f\"Running Layernorm on shape ({M},{N})\")  \n    set_seed()  \n    x = torch.randn(M, N, device='cuda')  \n    w_shape = (N, )  \n    w = torch.rand(w_shape, device='cuda')  \n    b = torch.rand(w_shape, device='cuda')  \n    y_triton = layernorm(x, w_shape, w, b)  \n  \n    return y_triton  \n  \n  \n#pytest  \n@pytest.mark.parametrize('M, N', [(1823, 781), (2, 128), (1, 4), (128, 2), (1, 128), (8192, 8192), (4096, 8192),  \n                                  (359, 1), (1, 359), (1, 131072), (1, 89999)])  \ndef test_layernorm(M, N, request, eps=1e-5):  \n    set_seed()  \n    x = torch.randn(M, N, device='cuda')  \n    w_shape = (N, )  \n    w = torch.rand(w_shape, device='cuda', requires_grad=True)  \n    b = torch.rand(w_shape, device='cuda', requires_grad=True)  \n  \n    dy = 0.1 * torch.randn_like(x)  \n    x.requires_grad_(True)  \n  \n    # forward pass  \n    y_triton = layernorm(x, w_shape, w, b, eps)  \n    y_ref = torch.nn.functional.layer_norm(x, w_shape, w, b, eps)  \n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\") + \"_fwd\"\n    result_gold[sanitized_key_name] = y_triton.clone().detach().cpu()\n    ###################################################################\n  \n    # backward pass (triton)  \n    y_triton.backward(dy, retain_graph=True)  \n    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, w, b]]  \n    x.grad, w.grad, b.grad = None, None, None  \n  \n    #backward pass (torch)  \n    y_ref.backward(dy, retain_graph=True)\n\n\n# --- Define TFLOPS and GB/s calculators for LayerNorm Forward ---\ndef calculate_layernorm_gbps(params: dict, ms: float) -> float:\n    M = params['M']\n    N = params['N']\n    # Assuming params contains dtype_str for x, w, b or we infer.\n    # For layernorm:\n    # Read x (M*N), w (N), b (N)\n    # Write y (M*N), mean (M), rstd (M)\n    try:\n        # Assuming x, y, w, b, mean, rstd are of similar precision for byte counting\n        # A more precise calculation would use actual dtypes if they vary significantly.\n        dtype = torch.float16 # Default assumption if not in params\n        if 'dtype_str' in params:\n            if params['dtype_str'] == 'fp32': dtype = torch.float32\n            elif params['dtype_str'] == 'bf16': dtype = torch.bfloat16\n        element_size = torch.tensor([], dtype=dtype).element_size()\n    except KeyError:\n        element_size = 2 # Default to 2 bytes (fp16)\n\n    bytes_read_x = M * N * element_size\n    bytes_read_w = N * element_size\n    bytes_read_b = N * element_size\n    bytes_write_y = M * N * element_size\n    bytes_write_mean = M * 4 # Mean/rstd often float32\n    bytes_write_rstd = M * 4\n\n    total_bytes = bytes_read_x + bytes_read_w + bytes_read_b + \\\n                  bytes_write_y + bytes_write_mean + bytes_write_rstd\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\ndef calculate_layernorm_tflops(params: dict, ms: float) -> float:\n    M = params['M']\n    N = params['N']\n    # FLOPs for LayerNorm forward:\n    # 1. Mean: N additions + 1 division per row  (N ops)\n    # 2. Variance: N subtractions, N squares, N additions, 1 division per row (3N+1 ops, approx 3N)\n    # 3. rsqrt: (approx ~5-10 ops, let's say 5)\n    # 4. Normalize: N subtractions, N multiplications per row (2N ops)\n    # 5. Scale/shift: N multiplications, N additions per row (2N ops)\n    # Total per row: N + 3N + 5 + 2N + 2N = 8N + 5 ops\n    # Total FLOPs: M * (8*N + 5)\n    flops = M * (8 * N + 5) # Simplified, actual can vary by rsqrt implementation etc.\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"layernorm_triton_fwd_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Using the same parameters as the original test_layernorm\nLAYERNORM_SHAPES_FOR_PERF = [\n    (1823, 781), (2048, 2048), (8192, 8192), # Some medium to large\n    (4096, 10240), # LLM typical\n    (1, 131072), (1, 89999), # Long sequence, batch 1\n    (128, 2048), (512, 4096)\n]\n# Dtypes to test for performance\nDTYPES_FOR_PERF = ['fp16', 'bf16', 'fp32']\n\n\n@pytest.mark.parametrize('M, N', LAYERNORM_SHAPES_FOR_PERF)\n@pytest.mark.parametrize('dtype_str', DTYPES_FOR_PERF)\ndef test_performance(M, N, dtype_str, request):\n    set_seed()\n    eps = 1e-5\n\n    # Determine torch dtype\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16 # Default to fp16\n\n    # Input tensors\n    x = torch.randn(M, N, device='cuda', dtype=current_dtype)\n    normalized_shape_arg = (N,) # For torch.nn.functional.layer_norm and our LayerNorm.apply\n    w = torch.rand(N, device='cuda', dtype=current_dtype)\n    b = torch.rand(N, device='cuda', dtype=current_dtype)\n\n    # --- Create op_lambda for benchmarking the forward pass ---\n    op_lambda = lambda: layernorm(x, normalized_shape_arg, w, b, eps)\n\n    # --- Benchmarking ---\n    # Autotuned kernels might benefit from fewer reps if tuning takes time,\n    # but do_bench needs enough reps for stable measurement AFTER tuning.\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"eps\": eps, \"dtype_str\": dtype_str\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_layernorm_gbps,\n                              tflops_calculator=calculate_layernorm_tflops)\n    \n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n######################################## HELPERS for Eval ######################################## "
    },
    {
        "file": "softmax.py",
        "target_kernel_name": "softmax_kernel_online",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `softmax_kernel_online`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `softmax_kernel_online`,  is designed to perform softmax function on the input tensor in an online, numerically stable manner.\n\n**Your objective is to implement the body of `softmax_kernel_online`.**\n\nYou must ensure that:\n1.  All arguments received by `softmax_kernel_online` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\n\n\nThe full definition for `softmax_kernel_online` and relevant helper utilities are provided in the context below. You only need to complete the code for `softmax_kernel_online` whilst keeping other things intact.\n\n\n#Imports \nimport argparse\nimport torch\nimport sys\nimport pytest\n\nimport triton\nimport triton.language as tl\n\n######################################## HELPERS utils ######################################## \ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef is_hip():\n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n\n\ndef is_cdna():\n    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n                                                                                   'gfx90a', 'gfx908')\n\n\ndef get_cuda_autotune_config():\n    return [\n        triton.Config({}, num_warps=4, num_stages=1),\n        triton.Config({}, num_warps=8, num_stages=1),\n        triton.Config({}, num_warps=16, num_stages=1),\n    ]\n\n\ndef get_hip_autotune_config():\n    return [\n        triton.Config({'waves_per_eu': 1}, num_warps=4, num_stages=1),\n        triton.Config({'waves_per_eu': 1}, num_warps=8, num_stages=1),\n        triton.Config({'waves_per_eu': 1}, num_warps=16, num_stages=1),\n        triton.Config({'waves_per_eu': 2}, num_warps=4, num_stages=1),\n        triton.Config({'waves_per_eu': 2}, num_warps=8, num_stages=1),\n        triton.Config({'waves_per_eu': 2}, num_warps=16, num_stages=1),\n        triton.Config({'waves_per_eu': 4}, num_warps=4, num_stages=1),\n        triton.Config({'waves_per_eu': 4}, num_warps=8, num_stages=1),\n        triton.Config({'waves_per_eu': 4}, num_warps=16, num_stages=1),\n    ]\n\n\ndef get_autotune_config():\n    if is_cuda():\n        return get_cuda_autotune_config()\n    else:\n        return get_hip_autotune_config()\n\n######################################## HELPERS utils ######################################## \n\n\n@triton.autotune(configs=get_autotune_config(), key=['n_rows', 'n_cols'], use_cuda_graph=True)\n@triton.jit\ndef softmax_kernel_online(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols,\n                          BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Computes the softmax function for each row of the input tensor in an online, numerically stable manner.\n\n    This Triton kernel processes each row of the input tensor independently. For each row,\n    it iterates through the columns in blocks. In the first pass over the blocks,\n    it computes the row-wise maximum value and the sum of exponentials (scaled by the\n    running maximum) in an online fashion. This online update of the sum involves\n    rescaling previous sums if a new, larger maximum is found.\n    In the second pass, it subtracts the final row-wise maximum, exponentiates,\n    divides by the final sum of exponentials, and stores the result.\n    This approach is numerically stable, especially for inputs with large variations in magnitude.\n    Each program instance (kernel launch) is responsible for processing a single row.\n\n    Parameters:\n    -----------\n    output_ptr : tl.pointer_type\n        Pointer to the output tensor where the softmax results will be stored.\n        Expected to be of a floating-point type (e.g., float32).\n    input_ptr : tl.pointer_type\n        Pointer to the input tensor.\n        Expected to be of a floating-point type (e.g., float32).\n    input_row_stride : int\n        The stride (in number of elements) to move from one row to the next in the input tensor.\n    output_row_stride : int\n        The stride (in number of elements) to move from one row to the next in the output tensor.\n    n_rows : int\n        The total number of rows in the input (and output) tensor. The kernel is typically\n        launched with `n_rows` program instances in the first dimension.\n    n_cols : int\n        The total number of columns (features) in each row of the input tensor.\n    BLOCK_SIZE : tl.constexpr\n        The size of the blocks into which each row is divided for processing during the\n        online computation. This is a compile-time constant and should ideally be a\n        power of 2 for efficiency (e.g., 1024, 2048).\n    \"\"\"\n    # Your code here.\n\n\n",
        "label": "#Imports \nimport argparse\nimport torch\nimport sys\nimport pytest\n\nimport triton\nimport triton.language as tl\n\n######################################## HELPERS utils ######################################## \ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef is_hip():\n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n\n\ndef is_cdna():\n    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n                                                                                   'gfx90a', 'gfx908')\n\n\ndef get_cuda_autotune_config():\n    return [\n        triton.Config({}, num_warps=4, num_stages=1),\n        triton.Config({}, num_warps=8, num_stages=1),\n        triton.Config({}, num_warps=16, num_stages=1),\n    ]\n\n\ndef get_hip_autotune_config():\n    return [\n        triton.Config({'waves_per_eu': 1}, num_warps=4, num_stages=1),\n        triton.Config({'waves_per_eu': 1}, num_warps=8, num_stages=1),\n        triton.Config({'waves_per_eu': 1}, num_warps=16, num_stages=1),\n        triton.Config({'waves_per_eu': 2}, num_warps=4, num_stages=1),\n        triton.Config({'waves_per_eu': 2}, num_warps=8, num_stages=1),\n        triton.Config({'waves_per_eu': 2}, num_warps=16, num_stages=1),\n        triton.Config({'waves_per_eu': 4}, num_warps=4, num_stages=1),\n        triton.Config({'waves_per_eu': 4}, num_warps=8, num_stages=1),\n        triton.Config({'waves_per_eu': 4}, num_warps=16, num_stages=1),\n    ]\n\n\ndef get_autotune_config():\n    if is_cuda():\n        return get_cuda_autotune_config()\n    else:\n        return get_hip_autotune_config()\n\n######################################## HELPERS utils ######################################## \n\n\n@triton.autotune(configs=get_autotune_config(), key=['n_rows', 'n_cols'], use_cuda_graph=True)\n@triton.jit\ndef softmax_kernel_online(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols,\n                          BLOCK_SIZE: tl.constexpr):\n\n    row_start = tl.program_id(0)\n    row_idx = row_start\n\n    #loop 1, find max and sum\n    m = -float('inf')  #Initial value of max\n    row_sum = 0.0\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    for b in tl.range(0, n_cols, BLOCK_SIZE):\n        col_offsets = b + tl.arange(0, BLOCK_SIZE)\n        input_ptrs = row_start_ptr + col_offsets\n        mask = col_offsets < n_cols\n        row_block = tl.load(input_ptrs, mask=mask, other=-float('inf'), cache_modifier=\".cg\")  #load block\n        m_p = tl.max(row_block, axis=0)  #find block max\n        m_p = tl.maximum(m, m_p)  #Find new max across all blocks so far\n        row_sum = row_sum * tl.exp(m - m_p)  #Adjust previous sum\n        row_sum += tl.sum(tl.exp(row_block - m_p))  #Add to exponentiated sum of this block\n        m = m_p  #save max\n\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    #Loop 2\n    for b in tl.range(0, n_cols, BLOCK_SIZE):\n        col_offsets = b + tl.arange(0, BLOCK_SIZE)\n        input_ptrs = row_start_ptr + col_offsets\n        mask = col_offsets < n_cols\n        row_block = tl.load(input_ptrs, mask=mask, other=-float('inf'), cache_modifier=\".cg\")  #load block\n        #subtract, exponentiate and divide by sum\n        softmax_output = tl.exp(row_block - m) / row_sum\n        #store\n        output_ptrs = output_row_start_ptr + col_offsets\n        tl.store(output_ptrs, softmax_output, mask=mask)\n\n##################################################################################################################################################  \n\n\nimport numpy as np\nimport random\nimport torch \nimport os\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \nCONFIG = {\n  \"llama3\": {\n    \"8B\": {\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"vocab_size\": 128256\n    },\n    \"70B\": {\n      \"num_attention_heads\": 64,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 8192,\n      \"intermediate_size\": 28672,\n      \"vocab_size\": 128256\n    },\n    \"405B\": {\n      \"num_attention_heads\": 128,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 16384,\n      \"intermediate_size\": 53248,\n      \"vocab_size\": 128256\n    }\n  },\n  \"mistral\": {\n    \"7B\": {\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    },\n    \"22B\": {\n      \"hidden_size\": 6144,\n      \"intermediate_size\": 16384,\n      \"num_attention_heads\": 48,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    }\n\n  }\n}\n\n  \ndef get_model_configs(config_path='model_configs.json', model_families=[\"llama3\"], model=\"all\"):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_path (str): User-provided path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        dict: A dictionary of available models and their configurations for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    # Extract models and their configurations for the specified families  \n    filtered_configs = {}  \n  \n    for family in model_families:  \n        if family in configs:  \n            # Check if model filtering is required  \n            if model == \"all\":  \n                # Include all models in the family  \n                for model_size, model_configs in configs[family].items():  \n                    filtered_configs[f\"{family}-{model_size}\"] = model_configs  \n            else:  \n                # Parse the model string (e.g., llama3_8B or llama3-8B)  \n                delimiter = \"_\" if \"_\" in model else \"-\"  \n                model_parts = model.split(delimiter)  \n  \n                # Check if the family and size match  \n                if len(model_parts) == 2 and model_parts[0] == family:  \n                    model_size = model_parts[1]  \n                    if model_size in configs[family]:  \n                        filtered_configs[f\"{family}-{model_size}\"] = configs[family][model_size]  \n  \n    if not filtered_configs:  \n        print(f\"Warning: No models selected for families: {model_families} with filter: '{model}'\")  \n  \n    return filtered_configs  \n  \n  \ndef get_available_models(config_file='model_configs.json', model_families=[\"llama3\"]):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_file (str): Path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        list: A list of available models for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    models = [f\"{family}-{model}\" for family in model_families if family in configs for model in configs[family]]  \n  \n    return models  \n\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n  \n\ndef softmax(x):\n    n_rows, n_cols = x.shape\n\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))\n    y = torch.empty_like(x)\n\n    num_programs = n_rows\n\n    grid = lambda meta: (num_programs, )\n    softmax_kernel_online[grid](\n        y,\n        x,\n        x.stride(0),\n        y.stride(0),\n        n_rows,\n        n_cols,\n        BLOCK_SIZE,\n    )\n\n    return y\n\n\ndef run_softmax(M, N):\n    print(f\"Running Softmax on shape ({M},{N})\")\n    set_seed()\n    x = torch.randn(M, N, device='cuda')\n    y_triton = softmax(x)\n\n    return y_triton\n\n\n#pytest\n@pytest.mark.parametrize('M, N', [(1823, 781), (1, 1), (128, 1), (1, 128), (8192, 8192), (4096, 8192), (359, 1),\n                                  (1, 359), (1, 131072), (1, 89999)])\ndef test_softmax(M, N, request):\n    set_seed()\n    x = torch.randn(M, N, device='cuda')\n    y_triton = softmax(x)\n    y_torch = torch.softmax(x, axis=1)\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = y_triton.clone().detach().cpu()\n    ###################################################################\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n\n\n#Benchmark\narg_to_torch_dtype = {'fp16': torch.float16, 'bf16': torch.bfloat16, 'fp32': torch.float32}\n\n# --- Define TFLOPS and GB/s calculators for Softmax Forward ---\ndef calculate_softmax_fwd_gbps(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    dtype_str = params.get('dtype_str', 'fp16')\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    \n    # Read x (M,N), Write y (M,N)\n    # Intermediate m (M) and row_sum (M) are usually kept in registers/SRAM per row,\n    # not necessarily global memory traffic unless spilled, which is hard to model simply.\n    # For online softmax, data is read twice effectively (once for max/sum, once for normalization).\n    bytes_read_x_pass1 = M * N * element_size\n    bytes_read_x_pass2 = M * N * element_size # Or just once if fully fused and data stays in cache\n    bytes_write_y = M * N * element_size\n\n    # A common simplification for bandwidth: 2*M*N (one read, one write)\n    # More accurate for online: read_pass1 + read_pass2 + write\n    # Let's use 2 reads, 1 write for online softmax\n    total_bytes = bytes_read_x_pass1 + bytes_read_x_pass2 + bytes_write_y\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\ndef calculate_softmax_fwd_tflops(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    # FLOPs for Softmax forward (per row):\n    # 1. Find max: N-1 comparisons (approx N ops)\n    # 2. Subtract max: N subtractions (N ops)\n    # 3. Exp: N exponentials (N * ~5-10 ops, say N*5)\n    # 4. Sum exps: N-1 additions (approx N ops)\n    # 5. Divide by sum: N divisions (N ops)\n    # Total per row approx: N + N + 5N + N + N = 9N ops\n    flops_per_row = 9 * N \n    total_flops = M * flops_per_row\n    tflops = total_flops / (ms / 1000) / 1e12\n    return tflops\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"softmax_triton_perf\"\n\n# --- Pytest test_softmax function MODIFIED for performance benchmarking ---\n# Original parametrization is kept.\nSOFTMAX_SHAPES_FOR_PERF = [\n    (2048, 2048), (4096, 4096), (8192, 8192), # Square\n    (1, 32000), (1, 131072),                 # Typical vocab sizes (batch 1)\n    (1024, 8192), (512, 32000),              # Batch > 1\n    # (1,4), (1823, 781) # Smaller/odd shapes\n]\nSOFTMAX_DTYPES_FOR_PERF = ['fp16', 'bf16', 'fp32']\n\n\n@pytest.mark.parametrize('M, N', SOFTMAX_SHAPES_FOR_PERF)\n@pytest.mark.parametrize('dtype_str', SOFTMAX_DTYPES_FOR_PERF)\ndef test_performance(M, N, dtype_str, request): # Renamed from test_softmax\n    set_seed()\n    \n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    x = torch.randn(M, N, device='cuda', dtype=current_dtype)\n\n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: softmax(x)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    # Determine BLOCK_SIZE as it's done in the softmax_triton_wrapper for logging\n    # This is for logging consistency, the actual kernel uses autotuned block size.\n    # The BLOCK_SIZE passed to kernel is a key for autotuning.\n    MAX_FUSED_SIZE_log = 65536 // x.element_size()\n    BLOCK_SIZE_log = min(MAX_FUSED_SIZE_log, triton.next_power_of_2(N if N > 0 else 1))\n    if BLOCK_SIZE_log == 0: BLOCK_SIZE_log = 1\n\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"dtype_str\": dtype_str,\n        \"LOGGED_BLOCK_SIZE_heuristic\": BLOCK_SIZE_log # Log the heuristic block size\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_softmax_fwd_gbps,\n                              tflops_calculator=calculate_softmax_fwd_tflops)\n\n\ndef model_benchmark_configs(args):\n    config_file = args.model_configs\n    configs = get_model_configs(config_path=config_file, model_families=[\"llama3\"], model=args.model)\n\n    x_vals_list = []\n    batch_size = args.b if args.b else 1\n\n    for model_name, config in configs.items():\n        seq_len = args.sq if args.sq else 4096\n        x_vals_list.append((model_name, batch_size * seq_len, config[\"vocab_size\"]))\n\n    return x_vals_list\n\n\ndef run_benchmark(args):\n    config = []\n    if (args.M_benchmark):\n        val = args.M_start\n        x_vals_list = []\n        while val <= args.M_end:\n            x_vals_list.append(val)\n            val *= args.M_step\n        mn_args = {'N': args.N_start}\n        plot_name = str(\"softmax-performance_\" + args.dtype + \"_N\" + str(args.N_start) + \"_M\" + str(args.M_start) +\n                        \"-\" + str(args.M_end) + \"-\" + str(args.M_step))\n        x_names = ['M']\n    else:\n        x_vals_list = [i for i in range(args.N_start, args.N_end, args.N_step)]\n        mn_args = {'M': args.M_start}\n        plot_name = str(\"softmax-performance_\" + args.dtype + \"_M\" + str(args.M_start) + \"_N\" + str(args.N_start) +\n                        \"-\" + str(args.N_end) + \"-\" + str(args.N_step))\n        x_names = ['N']\n\n    if args.model:\n        assert not args.M_benchmark, \\\n            \"Trying to provide both -model benchmark and M_benchmark is not supported!\"\n        x_names = ['model', 'M', 'N']\n        mn_args = {}\n        plot_name = str(\"softmax-performance_\" + args.dtype)\n        x_vals_list = model_benchmark_configs(args)\n\n    dtype = arg_to_torch_dtype[args.dtype]\n\n    print(plot_name)\n    config.append(\n        triton.testing.Benchmark(\n            x_names=x_names,\n            x_vals=x_vals_list,\n            line_arg='provider',\n            line_vals=['triton', 'torch'],\n            line_names=[\n                \"Triton\",\n                \"Torch\",\n            ],\n            styles=[('blue', '-'), ('green', '-')],\n            ylabel=\"GB/s\",\n            plot_name=plot_name,\n            args=mn_args,\n        ))\n\n    @triton.testing.perf_report(config)\n    def benchmark(M, N, provider, model=None):\n        x = torch.randn(M, N, device='cuda', dtype=dtype)\n        stream = torch.cuda.Stream()\n        torch.cuda.set_stream(stream)\n        if provider == 'torch':\n            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n        if provider == 'triton':\n            ms = triton.testing.do_bench(lambda: softmax(x))\n        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n        return gbps(ms)\n\n    benchmark.run(save_path=\".\", show_plots=True, print_data=True)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ######################################## \n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        prog=\"Benchmark Softmax\",\n        allow_abbrev=False,\n    )\n    parser.add_argument('-model_configs', type=str, default=\"model_configs.json\", help=\"Model config json file.\")\n\n    available_models = get_available_models(model_families=[\"llama3\"])  # Dynamically load model names\n    model_help = (\n        \"Model name to benchmark. Select from: [\" + \", \".join(available_models) +\n        \"]. Use 'all' to benchmark all models. Not providing runs the default benchmark script with custom configs.\")\n    parser.add_argument('-model', type=str, default=None, help=model_help)\n    parser.add_argument('-b', type=int, default=0, help=\"Batch size used together with model.\")\n    parser.add_argument('-sq', type=int, default=0, help=\"Sequence length used together with model.\")\n    parser.add_argument('-M', \"--M_start\", default=\"1\", type=int)\n    parser.add_argument('-Ms', \"--M_step\", default=\"2\", type=int)\n    parser.add_argument('-Me', \"--M_end\", default=\"512\", type=int)\n    parser.add_argument('-Mb', \"--M_benchmark\", default=False, type=bool)\n\n    parser.add_argument('-N', \"--N_start\", default=\"1024\", type=int)\n    parser.add_argument('-Ns', \"--N_step\", default=\"2048\", type=int)\n    parser.add_argument('-Ne', \"--N_end\", default=\"65536\", type=int)\n\n    parser.add_argument('-d', \"--dtype\", default=\"fp16\")\n    parser.add_argument('-nb', \"--no_benchmark\", default=False, type=bool)\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    if args.no_benchmark:\n        run_softmax(args.M_start, args.N_start)\n    else:\n        run_benchmark(args)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"
    },
    {
        "file": "test_chained_dot_fp8.py",
        "target_kernel_name": "_chained_dot",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `_chained_dot`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `_chained_dot`,  is designed to perform \"chained dot product\" operation.\n\n**Your objective is to implement the body of `_chained_dot`.**\n\nYou must ensure that:\n1.  All arguments received by `_chained_dot` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\n\n\nThe full definition for `_chained_dot` and relevant helper utilities are provided in the context below. You only need to complete the code for `_chained_dot` whilst keeping other things intact.\n\n\"\"\"\nTesting the (FP8) case of a dot op that consumes the output (MFMA) of\nanother dot op as an input.\n\n\"\"\"\n#Imports\n\nimport math\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n\n########################## HELPER utils ##########################\nTORCH_HAS_FP8E4 = hasattr(torch, 'float8_e4m3fnuz')\nfloat8: tl.constexpr = None if not TORCH_HAS_FP8E4 else torch.float8_e4m3fnuz\n########################## HELPER utils ##########################\n\n@triton.jit\ndef _chained_dot(\n    Q,\n    K,\n    V,\n    Out,\n    q_desc,\n    k_desc,\n    v_desc,\n    s_sc,\n    s_desc,\n    o_sc,\n    stride_qz,\n    stride_qm,\n    stride_qd,\n    stride_kz,\n    stride_kn,\n    stride_kd,\n    stride_vz,\n    stride_vd,\n    stride_vn,\n    stride_oz,\n    stride_om,\n    stride_od,\n    Z,\n    M,\n    N,\n    BLOCK_D: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    USE_FP8: tl.constexpr,\n):\n    \"\"\"\n    This Triton kernel computes a \"chained dot product\" operation,\n    effectively performing (Q @ K.T) @ V in a tiled manner.\n    This is a core component of attention mechanisms.\n    The kernel is parallelized across the M dimension of Q (target sequence length)\n    and the Z dimension (batch size * number of heads).\n    It iteratively loads blocks of K and V to compute partial results for the output.\n    FP8 support allows for reduced precision computation with scaling factors.\n\n    Parameters:\n    -----------\n    Q : tl.tensor\n        Pointer to the Q (query) tensor. Expected shape (Z, M, D).\n    K : tl.tensor\n        Pointer to the K (key) tensor. Expected shape (Z, N, D).\n    V : tl.tensor\n        Pointer to the V (value) tensor. Expected shape (Z, N, D).\n    Out : tl.tensor\n        Pointer to the O (output) tensor. Expected shape (Z, M, D).\n    q_desc : float\n        Dequantization scale for the Q tensor (used if USE_FP8 is True).\n    k_desc : float\n        Dequantization scale for the K tensor (used if USE_FP8 is True).\n    v_desc : float\n        Dequantization scale for the V tensor (used if USE_FP8 is True).\n    s_sc : float\n        Scaling factor applied to the intermediate S (QK^T) tensor before dot product with V (used if USE_FP8 is True).\n        This can be thought of as a quantization scale if S were to be stored in FP8.\n    s_desc : float\n        Dequantization scale for the intermediate S (QK^T) tensor when it's used in S@V (used if USE_FP8 is True).\n    o_sc : float\n        Quantization scale for the O (output) tensor (used if USE_FP8 is True).\n    stride_qz : int\n        Stride of the Q tensor along the Z (batch/head) dimension.\n    stride_qm : int\n        Stride of the Q tensor along the M (sequence length of Q / rows) dimension.\n    stride_qd : int\n        Stride of the Q tensor along the D (feature/embedding) dimension.\n    stride_kz : int\n        Stride of the K tensor along the Z (batch/head) dimension.\n    stride_kn : int\n        Stride of the K tensor along the N (sequence length of K / rows) dimension.\n    stride_kd : int\n        Stride of the K tensor along the D (feature/embedding) dimension.\n    stride_vz : int\n        Stride of the V tensor along the Z (batch/head) dimension.\n    stride_vd : int\n        Stride of the V tensor along the D (feature/embedding) dimension.\n    stride_vn : int\n        Stride of the V tensor along the N (sequence length of V / rows) dimension.\n    stride_oz : int\n        Stride of the Out tensor along the Z (batch/head) dimension.\n    stride_om : int\n        Stride of the Out tensor along the M (sequence length of Out / rows) dimension.\n    stride_od : int\n        Stride of the Out tensor along the D (feature/embedding) dimension.\n    Z : int\n        Size of the Z dimension (e.g., batch_size * num_heads).\n    M : int\n        Size of the M dimension (e.g., sequence length of Q, number of rows in Q).\n    N : int\n        Size of the N dimension (e.g., sequence length of K and V, number of columns in K.T / rows in V).\n    BLOCK_D : tl.constexpr\n        Tile size for the D dimension (feature/embedding dimension). Compile-time constant.\n    BLOCK_M : tl.constexpr\n        Tile size for the M dimension (rows of Q). Compile-time constant.\n    BLOCK_N : tl.constexpr\n        Tile size for the N dimension (columns of K.T / rows of V). Compile-time constant.\n    USE_FP8 : tl.constexpr\n        Boolean flag indicating whether to use FP8 E4M3 precision and apply scaling. Compile-time constant.\n    \"\"\"\n    # Your code here\n\n\n",
        "label": "\"\"\"\nTesting the (FP8) case of a dot op that consumes the output (MFMA) of\nanother dot op as an input.\n\n\"\"\"\n######################################## Imports#######################################\nimport math\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n######################################## Imports#######################################\n\n########################## HELPER utils ##########################\nTORCH_HAS_FP8E4 = hasattr(torch, 'float8_e4m3fnuz')\nfloat8: tl.constexpr = None if not TORCH_HAS_FP8E4 else torch.float8_e4m3fnuz\n########################## HELPER utils ##########################\n\n\n@triton.jit\ndef _chained_dot(\n    Q,\n    K,\n    V,\n    Out,\n    q_desc,\n    k_desc,\n    v_desc,\n    s_sc,\n    s_desc,\n    o_sc,\n    stride_qz,\n    stride_qm,\n    stride_qd,\n    stride_kz,\n    stride_kn,\n    stride_kd,\n    stride_vz,\n    stride_vd,\n    stride_vn,\n    stride_oz,\n    stride_om,\n    stride_od,\n    Z,\n    M,\n    N,\n    BLOCK_D: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    USE_FP8: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_z = tl.program_id(1)\n    qkv_offset = off_z * stride_qz\n    Q_block_ptr = tl.make_block_ptr(base=Q + qkv_offset, shape=(N, BLOCK_D), strides=(stride_qm, stride_qd),\n                                    offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_D), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qkv_offset, shape=(BLOCK_D, N), strides=(stride_kd, stride_kn),\n                                    offsets=(0, 0), block_shape=(BLOCK_D, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + qkv_offset, shape=(N, BLOCK_D), strides=(stride_vn, stride_vd),\n                                    offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_D), order=(0, 1))\n\n    s_scale = q_desc * k_desc * s_sc\n    acc_scale = s_desc * v_desc * o_sc\n\n    q = tl.load(Q_block_ptr)\n\n    acc = tl.zeros([BLOCK_M, BLOCK_D], dtype=tl.float32)\n    lo, hi = 0, N\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n\n        k = tl.load(K_block_ptr)\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, k)\n\n        if USE_FP8:\n            s *= s_scale\n\n        v = tl.load(V_block_ptr)\n        acc += tl.dot(s.to(v.dtype), v)\n\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    if USE_FP8:\n        acc *= acc_scale\n\n    O_block_ptr = tl.make_block_ptr(base=Out + qkv_offset, shape=(N, BLOCK_D), strides=(stride_om, stride_od),\n                                    offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_D), order=(1, 0))\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty))\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n########################## HELPER utils ##########################\nTORCH_HAS_FP8E4 = hasattr(torch, 'float8_e4m3fnuz')\nfloat8: tl.constexpr = None if not TORCH_HAS_FP8E4 else torch.float8_e4m3fnuz\n########################## HELPER utils ##########################\n\n\nresult_gold = {}\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \nclass chained_dot_fn(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, msize=32, q_desc=1.0, k_desc=1.0, v_desc=1.0, s_sc=1.0, s_desc=1.0, o_sc=1.0):\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-2]\n        assert Lq == Lk and Lk == Lv\n        assert Lk in {16, 32, 64, 128}\n        assert msize in {16, 32}\n        o = torch.empty_like(q, dtype=v.dtype)\n\n        BLOCK_M = 128 if q.dtype == float8 else 256\n        if BLOCK_M > q.shape[1]:\n            BLOCK_M = int(math.pow(2, math.floor(math.log2(q.shape[1]))))\n        BLOCK_N = 32\n        if BLOCK_N > k.shape[1]:\n            BLOCK_N = int(math.pow(2, math.floor(math.log2(k.shape[1]))))\n        waves_per_eu = 2\n        num_warps = 4 if q.dtype == float8 else 8\n        num_stages = 1\n\n        grid = (triton.cdiv(q.shape[1], BLOCK_M), q.shape[0], 1)\n\n        _chained_dot[grid](q, k, v, o, q_desc,\n                           k_desc, v_desc, s_sc, s_desc, o_sc, q.stride(0), q.stride(1), q.stride(2), k.stride(0),\n                           k.stride(1), k.stride(2), v.stride(0), v.stride(1), v.stride(2), o.stride(0), o.stride(1),\n                           o.stride(2), Z=q.shape[0], M=q.shape[1], N=k.shape[1], BLOCK_D=Lk, BLOCK_M=BLOCK_M,\n                           BLOCK_N=BLOCK_N, USE_FP8=(q.dtype == float8), waves_per_eu=waves_per_eu, num_warps=num_warps,\n                           num_stages=num_stages, matrix_instr_nonkdim=msize)\n\n        return o\n\n\nchained_dot = chained_dot_fn.apply\n\n\ndef to_float8(x, dtype=float8, margin: float = 1.0):\n    finfo = torch.finfo(dtype)\n    scale = finfo.max / x.abs().max().clamp(min=1e-12)\n    scale = math.pow(2, math.floor(math.log2(scale.float().item())) - margin)\n    x_scaled = (x.float() * scale).clamp(min=finfo.min, max=finfo.max)\n    return x_scaled.to(dtype), scale, 1.0 / scale\n\n\n@pytest.mark.parametrize('M, N, D, dtype, msize', [(*shape, dtype, msize)\n                                                   for shape in [(128, 64, 32), (256, 128, 128)]\n                                                   for dtype in ['fp8']\n                                                   for msize in [16, 32]])\ndef test_chained_dot(M, N, D, dtype, msize,request):\n    set_seed()\n    if dtype == 'fp8':\n        assert float8 is not None\n\n    BATCH = 1\n    q = torch.empty((BATCH, M, D), dtype=torch.float16, device=\"cuda\").normal_(mean=0., std=0.5)\n    k = torch.empty((BATCH, N, D), dtype=torch.float16, device=\"cuda\").normal_(mean=0., std=0.5)\n    v = torch.empty((BATCH, D, N), dtype=torch.float16, device=\"cuda\").normal_(mean=0., std=0.5)\n\n    if dtype == 'fp8':\n        q_f8, _, q_desc = to_float8(q)\n        k_f8, _, k_desc = to_float8(k)\n        v_f8, _, v_desc = to_float8(v)\n\n        s = torch._scaled_mm(q_f8[0], k_f8[0].transpose(0, 1), out_dtype=torch.float32,\n                             scale_a=torch.tensor(q_desc, dtype=torch.float32, device=\"cuda\"),\n                             scale_b=torch.tensor(k_desc, dtype=torch.float32, device=\"cuda\"))\n        s_f8, s_sc, s_desc = to_float8(s)\n        ref = torch._scaled_mm(s_f8, v_f8[0].transpose(0, 1), out_dtype=torch.float32,\n                               scale_a=torch.tensor(s_desc, dtype=torch.float32, device=\"cuda\"),\n                               scale_b=torch.tensor(v_desc, dtype=torch.float32, device=\"cuda\"))\n        ref_f8, ref_sc, _ = to_float8(ref)\n\n        tri_out = chained_dot(q_f8, k_f8, v_f8, msize, q_desc, k_desc, v_desc, s_sc, s_desc, ref_sc)\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n        ################### save tri_out in result_gold ###################\n        test_case_name = request.node.name\n        sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\") + \"_fwd\"\n        result_gold[sanitized_key_name] = tri_out[0].float().clone().detach().cpu()\n        ###################################################################\n\n\n        assert tri_out.isnan().sum() == 0\n        torch.testing.assert_close(tri_out[0].float(), ref_f8.float(), atol=1e-2, rtol=0)\n\n    else:\n        s = torch.matmul(q, k.transpose(1, 2))\n        ref = torch.matmul(s, v.transpose(1, 2))\n\n        tri_out = chained_dot(q, k, v, msize)\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n        ################### save tri_out in result_gold ###################\n        test_case_name = request.node.name\n        sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n        result_gold[sanitized_key_name] = tri_out.clone().detach().cpu()\n        ###################################################################\n\n        torch.testing.assert_close(tri_out, ref, atol=1e-2, rtol=0)\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_chained_dot_tflops(params: dict, ms: float) -> float:\n    BATCH, M, N_kv, D = params['BATCH'], params['M_seqlen'], params['N_kv_seqlen'], params['D_head']\n    # S = Q(M,D) @ K.T(D,N_kv) -> 2*M*N_kv*D\n    # O = S(M,N_kv) @ V.T(N_kv,D) -> 2*M*N_kv*D (assuming V value dim is D)\n    flops = BATCH * ( (2 * M * N_kv * D) + (2 * M * N_kv * D) )\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_chained_dot_gbps(params: dict, ms: float) -> float:\n    BATCH, M, N_kv, D = params['BATCH'], params['M_seqlen'], params['N_kv_seqlen'], params['D_head']\n    dtype_str = params.get('dtype_str', 'fp16')\n    \n    element_size = 1 if dtype_str == 'fp8' and TORCH_HAS_FP8E4 and float8 is not None else \\\n                   (4 if dtype_str == 'fp32' else 2) # Default to 2 bytes for fp16/bf16\n\n    bytes_q = BATCH * M * D * element_size\n    bytes_k = BATCH * N_kv * D * element_size\n    # V is passed as (B,D,N_kv) to kernel, effectively same number of elements as K\n    bytes_v = BATCH * D * N_kv * element_size \n    bytes_o = BATCH * M * D * element_size # Output O is (B,M,D)\n    total_bytes = bytes_q + bytes_k + bytes_v + bytes_o\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"chained_dot_fp8_perf\"\n\n# --- Pytest parametrize for performance testing ---\nCHAINED_DOT_PERF_CONFIGS = []\n# M, N_kv, D_head\nbase_shapes_perf = [(128, 64, 32), (256, 128, 64), (512, 128, 128), (1024, 256, 128), (2048, 512, 64)]\ndtypes_perf_list = ['fp16']\nif TORCH_HAS_FP8E4 and float8 is not None: # Check if the global float8 (torch.dtype or None) is set\n    dtypes_perf_list.append('fp8')\nmsize_args_perf_list = [16, 32] # msize from original test\n\nfor shape_cfg in base_shapes_perf:\n    m_val, n_kv_val, d_val = shape_cfg\n    for dtype_s_val in dtypes_perf_list:\n        for msize_val_val in msize_args_perf_list:\n            CHAINED_DOT_PERF_CONFIGS.append({\n                'M':m_val, 'N_kv':n_kv_val, 'D':d_val, \n                'dtype_str':dtype_s_val, 'msize':msize_val_val\n            })\n\n@pytest.mark.parametrize(\"test_config\", CHAINED_DOT_PERF_CONFIGS)\ndef test_performance(test_config, request):\n    set_seed()\n    M_seqlen = test_config['M']\n    N_kv_seqlen = test_config['N_kv']\n    D_head = test_config['D']\n    dtype_str = test_config['dtype_str']\n    msize_arg = test_config['msize']\n\n    BATCH = 2 # Example batch size for performance test\n\n    # Prepare inputs (always start with fp16 for data generation simplicity before potential cast)\n    q_host = torch.empty((BATCH, M_seqlen, D_head), dtype=torch.float16, device=\"cuda\").normal_(mean=0., std=0.5)\n    k_host = torch.empty((BATCH, N_kv_seqlen, D_head), dtype=torch.float16, device=\"cuda\").normal_(mean=0., std=0.5)\n    # V is passed as (B, D, N_kv) to chained_dot call in original test\n    v_host_for_call = torch.empty((BATCH, D_head, N_kv_seqlen), dtype=torch.float16, device=\"cuda\").normal_(mean=0., std=0.5)\n\n    q_for_kernel, k_for_kernel, v_for_kernel_call = q_host, k_host, v_host_for_call\n    # Python floats for scales, matching chained_dot_fn.forward signature\n    q_desc_py, k_desc_py, v_desc_py = 1.0, 1.0, 1.0\n    s_sc_py, s_desc_py, o_sc_py = 1.0, 1.0, 1.0\n\n    if dtype_str == 'fp8':\n        if not (TORCH_HAS_FP8E4 and float8 is not None): # Check global float8\n            pytest.skip(\"FP8 (e4m3fnuz) not available or global float8 is None.\")\n        \n        # to_float8's default dtype is the global `float8` (torch.dtype or None)\n        q_f8, _, q_desc_float = to_float8(q_host) # Returns Python float for scales\n        k_f8, _, k_desc_float = to_float8(k_host)\n        v_f8, _, v_desc_float = to_float8(v_host_for_call)\n        \n        q_for_kernel, k_for_kernel, v_for_kernel_call = q_f8, k_f8, v_f8\n        q_desc_py, k_desc_py, v_desc_py = q_desc_float, k_desc_float, v_desc_float\n\n        # Derive s_sc_py, s_desc_py, o_sc_py using the reference path from original test\n        # Use BATCH=0 for these calculations as original test did\n        s_ref_fp32 = torch._scaled_mm(\n            q_f8[0], k_f8[0].transpose(0, 1), \n            out_dtype=torch.float32,\n            scale_a=torch.tensor(q_desc_py, dtype=torch.float32, device=\"cuda\"), # _scaled_mm needs tensor scales\n            scale_b=torch.tensor(k_desc_py, dtype=torch.float32, device=\"cuda\")\n        )\n        _s_f8_ref, s_sc_float, s_desc_float = to_float8(s_ref_fp32)\n        \n        # v_f8[0] is (D, N_kv). Transpose to (N_kv, D) for S(M,N_kv) @ V.T(N_kv,D)\n        o_ref_fp32 = torch._scaled_mm(\n            _s_f8_ref, v_f8[0].transpose(0, 1), \n            out_dtype=torch.float32, \n            scale_a=torch.tensor(s_desc_float, dtype=torch.float32, device=\"cuda\"), \n            scale_b=torch.tensor(v_desc_float, dtype=torch.float32, device=\"cuda\")\n        )\n        _o_f8_ref, o_sc_float, _ = to_float8(o_ref_fp32)\n\n        s_sc_py, s_desc_py, o_sc_py = s_sc_float, s_desc_float, o_sc_float\n    \n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: chained_dot( # This is chained_dot_fn.apply\n        q_for_kernel, k_for_kernel, v_for_kernel_call, msize_arg,\n        q_desc_py, k_desc_py, v_desc_py,\n        s_sc_py, s_desc_py, o_sc_py\n    )\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=10, repetition=50) # Adjust reps as needed\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"BATCH\": BATCH, \"M_seqlen\": M_seqlen, \"N_kv_seqlen\": N_kv_seqlen, \"D_head\": D_head,\n        \"dtype_str\": dtype_str, \"msize_arg\": msize_arg\n    }\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_chained_dot_gbps,\n                              tflops_calculator=calculate_chained_dot_tflops)\n    \n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\") \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ######################################## "
    },
    {
        "file": "test_cast_matmul.py",
        "target_kernel_name": "matmul_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `matmul_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `matmul_kernel`,  is designed to perform a matrix multiplication (C = A @ B) using a tiled approach.\n\n**Your objective is to implement the body of `matmul_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `matmul_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `matmul_kernel` whilst keeping other things intact.\n\n######################################## Imports#######################################\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n######################################## Imports#######################################\n\n######################################## HELPERS utils ########################################\ninput_dtypes = [\"float16\", \"float32\", \"float64\"]\nout_dtypes = [\"float16\", \"float32\"]\n######################################## HELPERS utils ########################################\n\n\n@triton.jit\ndef matmul_kernel(\n    A,  # Pointer to the first input matrix (M x K).\n    B,  # Pointer to the second input matrix (K x N).\n    C,  # Pointer to the output matrix (M x N). The element type of C also dictates the type to which input tiles `a` and `b` are cast before `tl.dot`.\n    M,  # Number of rows in matrix A and C.\n    N,  # Number of columns in matrix B and C.\n    K,  # Number of columns in matrix A and rows in matrix B (the shared dimension).\n    stride_am,  # Stride for matrix A along the M dimension (row stride).\n    stride_ak,  # Stride for matrix A along the K dimension (column stride).\n    stride_bk,  # Stride for matrix B along the K dimension (row stride).\n    stride_bn,  # Stride for matrix B along the N dimension (column stride).\n    stride_cm,  # Stride for matrix C along the M dimension (row stride).\n    stride_cn,  # Stride for matrix C along the N dimension (column stride).\n    dot_out_dtype: tl.constexpr,  # The data type used for the accumulator in the `tl.dot` operation. This is a compile-time constant.\n    BLOCK_M: tl.constexpr,  # Tile size for the M dimension (rows per block). This is a compile-time constant.\n    BLOCK_N: tl.constexpr,  # Tile size for the N dimension (columns per block). This is a compile-time constant.\n    BLOCK_K: tl.constexpr,  # Tile size for the K dimension (inner dimension per block). This is a compile-time constant.\n    GROUP_M: tl.constexpr,  # Grouping factor for the M dimension to improve L2 cache performance. This is a compile-time constant.\n):\n    \"\"\"\n    Performs a matrix multiplication (C = A @ B) using a tiled approach.\n\n    This kernel is designed to test mixed precision capabilities, specifically focusing\n    on how `tl.dot` interacts with `tl.to` (cast) operations.\n    Input tiles from matrices A and B are loaded, then explicitly cast to the\n    element type of the output matrix C before the `tl.dot` operation.\n    The accumulation within `tl.dot` is performed using the specified `dot_out_dtype`.\n    Finally, the accumulated tile is cast to the element type of matrix C before\n    being stored.\n\n    The tiling strategy involves dividing the M and N dimensions into blocks of\n    size `BLOCK_M` and `BLOCK_N` respectively. The K dimension is processed in\n    chunks of `BLOCK_K`. Program IDs are re-ordered using `GROUP_M` to\n    potentially improve L2 cache locality.\n    \"\"\"\n    # Your code here\n",
        "label": "# Usage Instruction: python3 -m pytest test_cast_matmul.py\n\n\"\"\"\nMixed precision tests for matmul (tl.dot) with cast (tl.to)\n\nissue: https://github.com/triton-lang/triton/issues/2523\n\"\"\"\n\n\n\n######################################## Imports#######################################\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n######################################## Imports#######################################\n\n######################################## HELPERS utils ######################################## \ninput_dtypes = [\"float16\", \"float32\", \"float64\"]\nout_dtypes = [\"float16\", \"float32\"]\n######################################## HELPERS utils ######################################## \n\n\n@triton.jit\ndef matmul_kernel(A, B, C, M, N, K,  #\n                  stride_am, stride_ak,  #\n                  stride_bk, stride_bn,  #\n                  stride_cm, stride_cn,  #\n                  dot_out_dtype: tl.constexpr,  #\n                  BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,  #\n                  BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr):\n    # matrix multiplication\n    pid = tl.program_id(0)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    # re-order program ID for better L2 performance\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // (group_size)\n    # do matrix multiplication\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    # pointers\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=dot_out_dtype)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n        a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n        b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n        a = a.to(C.dtype.element_ty)\n        b = b.to(C.dtype.element_ty)\n        acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    acc = acc.to(C.dtype.element_ty)\n    # rematerialize rm and rn to save registers\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc, mask=mask)\n\n##################################################################################################################################################  \nimport numpy as np\nimport random\nimport torch \nimport os\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n######################################## HELPERS for Eval ########################################\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ########################################\n\n\n@pytest.mark.parametrize(\"M, K, N, w_dtype, x_dtype, out_dtype\",\n                         [(M, K, N, w, x, o)  #\n                          for (M, K, N) in [(128, 128, 128), (1280, 768, 1024)]  #\n                          for w in input_dtypes\n                          for x in input_dtypes  #\n                          for o in out_dtypes])\ndef test_cast_matmul(M, K, N, w_dtype, x_dtype, out_dtype, request):\n    set_seed()\n\n    if x_dtype == w_dtype:\n        pytest.skip(\"skip the same input dtype\")\n    device = torch.cuda.current_device()\n    x_dtype = getattr(torch, x_dtype)\n    w_dtype = getattr(torch, w_dtype)\n    a = torch.randn((M, K), device=device, dtype=x_dtype)\n    b = torch.randn((K, N), device=device, dtype=w_dtype)\n    torch_dtype = getattr(torch, out_dtype)\n    triton_dtype = getattr(tl, out_dtype)  # <- here force dot_out_dtype\n    out_torch = torch.matmul(a.to(torch_dtype), b.to(torch_dtype))\n    out_triton = torch.empty((M, N), device=device, dtype=torch_dtype)\n\n    # launch kernel\n    BLOCK_M, BLOCK_N, BLOCK_K = 16, 16, 32\n    grid = ((triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N)), 1)\n\n    matmul_kernel[grid](\n        a, b, out_triton, M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        out_triton.stride(0), out_triton.stride(1), dot_out_dtype=triton_dtype,  #\n        GROUP_M=8,  #\n        BLOCK_M=BLOCK_M,  #\n        BLOCK_N=BLOCK_N,  #\n        BLOCK_K=BLOCK_K)\n\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = out_triton.clone().detach().cpu()\n    ###################################################################\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    torch.testing.assert_close(out_torch, out_triton, atol=0.3, rtol=0.01)\n\n# --- Python wrapper for the kernel ---\ndef cast_matmul_triton_wrapper(a_tensor, b_tensor, out_tensor, # out_tensor is C\n                                M, N, K, # runtime dims\n                                triton_dot_out_dtype, # tl.dtype for dot_out_dtype\n                                BLOCK_M_const, BLOCK_N_const, BLOCK_K_const, GROUP_M_const, # constexpr\n                                num_warps_arg # For launch, if kernel was autotuned for it\n                               ):\n    grid = (triton.cdiv(M, BLOCK_M_const) * triton.cdiv(N, BLOCK_N_const), 1) # As per original test\n    # The original test calculates grid based on M, N, BLOCK_M, BLOCK_N.\n    # The kernel itself uses GROUP_M for pid reordering.\n    # The grid should be (num_programs_total, 1, 1) or just (num_programs_total,).\n    # num_programs_total = group_count * group_width = cdiv(M, BLOCK_M*GROUP_M) * (GROUP_M * cdiv(N,BLOCK_N))\n    # This simplifies to cdiv(M,BLOCK_M) * cdiv(N,BLOCK_N) if group_id logic is correct.\n    # Let's stick to original test's grid calculation.\n    \n    matmul_kernel[grid](\n        a_tensor, b_tensor, out_tensor, M, N, K,\n        a_tensor.stride(0), a_tensor.stride(1),\n        b_tensor.stride(0), b_tensor.stride(1),\n        out_tensor.stride(0), out_tensor.stride(1),\n        dot_out_dtype=triton_dot_out_dtype, # Pass as constexpr\n        BLOCK_M=BLOCK_M_const, BLOCK_N=BLOCK_N_const,\n        BLOCK_K=BLOCK_K_const, GROUP_M=GROUP_M_const,\n        # num_warps=num_warps_arg # This kernel is not autotuned, so num_warps in launch is a hint\n                                 # to underlying compiler/scheduler but not a JIT param.\n    )\n    return out_tensor\n\n\n# --- Define TFLOPS and GB/s calculators for this specific GEMM ---\ndef calculate_cast_matmul_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    # Standard GEMM FLOPs: 2 * M * N * K\n    # Casting operations are usually not counted in high-level TFLOPS unless very significant.\n    flops = 2 * M * N * K\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef get_torch_dtype(dtype_str):\n    if dtype_str == \"float64\": return torch.float64\n    if dtype_str == \"float32\": return torch.float32\n    return torch.float16 # Default for \"float16\" or others\n\ndef calculate_cast_matmul_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    # Dtypes from params\n    a_dtype_str = params['a_dtype_str'] # x_dtype in original test\n    b_dtype_str = params['b_dtype_str'] # w_dtype in original test\n    c_dtype_str = params['c_dtype_str'] # out_dtype in original test\n\n    element_size_a = torch.tensor([], dtype=get_torch_dtype(a_dtype_str)).element_size()\n    element_size_b = torch.tensor([], dtype=get_torch_dtype(b_dtype_str)).element_size()\n    element_size_c = torch.tensor([], dtype=get_torch_dtype(c_dtype_str)).element_size()\n\n    bytes_a = M * K * element_size_a\n    bytes_b = K * N * element_size_b\n    bytes_c_write = M * N * element_size_c # Written in C's dtype\n\n    total_bytes = bytes_a + bytes_b + bytes_c_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"cast_matmul_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Shapes from original test\nCAST_MATMUL_SHAPES_FOR_PERF = [(128, 128, 128), (1024, 768, 1280), (1280, 1024, 768)] # Swapped K,N for one case\n                                                                                  # Original: (1280, 768, 1024) -> M,K,N\n\n# Dtypes for performance testing (subset of original to keep it manageable)\n# Focusing on cases where casting might impact performance.\n# a_dtype (x_dtype), b_dtype (w_dtype), c_dtype (out_dtype), dot_acc_dtype (dot_out_dtype)\nCAST_MATMUL_DTYPES_FOR_PERF = [\n    # (a_str, b_str, c_str, dot_acc_tl_dtype_str)\n    (\"float32\", \"float16\", \"float16\", \"float32\"), # A(f32), B(f16) -> C(f16), Acc(f32)\n    (\"float16\", \"float32\", \"float16\", \"float32\"), # A(f16), B(f32) -> C(f16), Acc(f32)\n    (\"float16\", \"float16\", \"float32\", \"float32\"), # A(f16), B(f16) -> C(f32), Acc(f32)\n    (\"float32\", \"float32\", \"float16\", \"float16\"), # A(f32), B(f32) -> C(f16), Acc(f16) <- check if acc in f16 is intended/good\n    (\"float16\", \"float16\", \"float16\", \"float16\"), # All f16\n    (\"float32\", \"float32\", \"float32\", \"float32\"), # All f32\n    # (\"bfloat16\", \"bfloat16\", \"bfloat16\", \"float32\"), # bf16 example\n]\n\n# Kernel block sizes are constexpr. Original test uses fixed BLOCK_M,N,K = 16,16,32 and GROUP_M=8.\n# For performance, these should ideally be tuned or part of parametrization.\n# For now, use the fixed ones from the original test.\nFIXED_BLOCK_M = 16\nFIXED_BLOCK_N = 16\nFIXED_BLOCK_K = 32\nFIXED_GROUP_M = 8\n\n\n@pytest.mark.parametrize(\"M, K, N\", CAST_MATMUL_SHAPES_FOR_PERF) # Note M,K,N order\n@pytest.mark.parametrize(\"a_dtype_str, b_dtype_str, c_dtype_str, dot_acc_tl_dtype_str\", CAST_MATMUL_DTYPES_FOR_PERF)\ndef test_performance(M, K, N, a_dtype_str, b_dtype_str, c_dtype_str, dot_acc_tl_dtype_str, request):\n    set_seed()\n    device = torch.cuda.current_device() # Use current device\n\n    # Skip same input dtypes as per original test logic for functional part\n    # For performance, we might want to test them, but let's follow original skip.\n    if a_dtype_str == b_dtype_str:\n        pytest.skip(\"Skipping same input dtypes for A and B for this performance test.\")\n\n    a_torch_dtype = get_torch_dtype(a_dtype_str)\n    b_torch_dtype = get_torch_dtype(b_dtype_str)\n    c_torch_dtype = get_torch_dtype(c_dtype_str)\n    \n    # Convert string representation of tl dtype to actual tl.dtype\n    if dot_acc_tl_dtype_str == \"float32\": dot_triton_dtype = tl.float32\n    elif dot_acc_tl_dtype_str == \"float16\": dot_triton_dtype = tl.float16\n    # Add more if other tl dtypes are used for dot_out_dtype, e.g. tl.int32\n    else: raise ValueError(f\"Unsupported dot_out_dtype_str: {dot_acc_tl_dtype_str}\")\n\n\n    a = torch.randn((M, K), device=device, dtype=a_torch_dtype)\n    b = torch.randn((K, N), device=device, dtype=b_torch_dtype) # B is (K,N) for A(M,K) @ B(K,N)\n    \n    # Output tensor for Triton kernel\n    out_triton = torch.empty((M, N), device=device, dtype=c_torch_dtype)\n\n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: cast_matmul_triton_wrapper(\n        a, b, out_triton, M, N, K,\n        triton_dot_out_dtype=dot_triton_dtype, # Pass actual tl.dtype\n        BLOCK_M_const=FIXED_BLOCK_M, BLOCK_N_const=FIXED_BLOCK_N,\n        BLOCK_K_const=FIXED_BLOCK_K, GROUP_M_const=FIXED_GROUP_M,\n        num_warps_arg=4 # Example, not used by JIT kernel params but by launch if autotuned for it\n    )\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K,\n        \"a_dtype_str\": a_dtype_str, \"b_dtype_str\": b_dtype_str, \"c_dtype_str\": c_dtype_str,\n        \"dot_acc_dtype_str\": dot_acc_tl_dtype_str, # Log the string version\n        \"BLOCK_M\": FIXED_BLOCK_M, \"BLOCK_N\": FIXED_BLOCK_N,\n        \"BLOCK_K\": FIXED_BLOCK_K, \"GROUP_M\": FIXED_GROUP_M\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_cast_matmul_gbps,\n                              tflops_calculator=calculate_cast_matmul_tflops)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_gemm_fusion.py",
        "target_kernel_name": "gemm_fusion_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `gemm_fusion_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `gemm_fusion_kernel`,  is designed to perform a fused matrix multiplication operation.\n\n**Your objective is to implement the body of `gemm_fusion_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `gemm_fusion_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `gemm_fusion_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `gemm_fusion_kernel` whilst keeping other things intact.\n\n\n######################################## Imports #######################################\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports #######################################\n\n\n\n@triton.jit\ndef gemm_fusion_kernel(A, B, C, E,  #\n                       M, N, K,  #\n                       stride_am, stride_ak, stride_bn, stride_bk, stride_cn, stride_ck, stride_em, stride_ek,  #\n                       BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n    \"\"\"\n    This Triton kernel is designed to perform a fused matrix multiplication operation.\n    It computes E = (A @ B^T) @ C, where A, B, C, and E are matrices.\n    The computation is performed in a tiled manner to optimize for memory access patterns\n    and leverage the parallelism of GPU architectures.\n\n    Each program instance (kernel launch) processes a tile of the output matrix E,\n    corresponding to a `BLOCK_M` strip of rows from matrix A. It iterates over\n    tiles of B and C along their N dimension to compute the intermediate product\n    (A_tile @ B_tile^T) and then accumulates the result with C_tile into E_tile.\n\n    Args:\n        A: Pointer to the input matrix A. Expected shape (M, K_common).\n        B: Pointer to the input matrix B. Expected shape (N, K_common).\n        C: Pointer to the input matrix C. Expected shape (N, K_out).\n        E: Pointer to the output matrix E, where the result E = (A @ B^T) @ C is stored. Expected shape (M, K_out).\n        M: The number of rows in matrix A and matrix E.\n        N: The number of rows in matrix B and matrix C. This is also the dimension\n           over which the product (A @ B^T) and C are contracted.\n        K: This parameter represents two potentially different dimensions depending on context,\n           but given the block shapes, it is used as K_common for A and B, and K_out for C and E.\n           Specifically:\n           - For A and B, it's the common dimension (K_common) for A @ B^T.\n           - For C and E, it's the output column dimension (K_out).\n           The kernel structure (BLOCK_K used for all) implies K_common == K_out.\n        stride_am: The stride (in number of elements) for matrix A along the M dimension (row stride).\n        stride_ak: The stride (in number of elements) for matrix A along the K dimension (column stride).\n        stride_bn: The stride (in number of elements) for matrix B along the N dimension (row stride).\n        stride_bk: The stride (in number of elements) for matrix B along the K dimension (column stride).\n        stride_cn: The stride (in number of elements) for matrix C along the N dimension (row stride).\n        stride_ck: The stride (in number of elements) for matrix C along the K dimension (column stride).\n        stride_em: The stride (in number of elements) for matrix E along the M dimension (row stride).\n        stride_ek: The stride (in number of elements) for matrix E along the K dimension (column stride).\n        BLOCK_M: tl.constexpr, the tile size for the M dimension. Each kernel instance\n                   processes a block of `BLOCK_M` rows from A and E.\n        BLOCK_N: tl.constexpr, the tile size for the N dimension. The kernel iterates\n                   over B and C in blocks of `BLOCK_N` along their N dimension.\n        BLOCK_K: tl.constexpr, the tile size for the K dimension. This is the block size\n                   for the common dimension in A@B^T and the output column dimension\n                   for C and E.\n    \"\"\"\n    # Your code here\n\n\n",
        "label": "# Usage Instruction: python3 -m pytest test_gemm_fusion.py\n\n# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n######################################## Imports#######################################\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports#######################################\n\n\n\n@triton.jit\ndef gemm_fusion_kernel(A, B, C, E,  #\n                       M, N, K,  #\n                       stride_am, stride_ak, stride_bn, stride_bk, stride_cn, stride_ck, stride_em, stride_ek,  #\n                       BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n    pid = tl.program_id(0)\n\n    a_tile_ptr = tl.make_block_ptr(base=A, shape=(M, K), strides=(stride_am, stride_ak), offsets=(pid * BLOCK_M, 0),\n                                   block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n    b_tile_ptr = tl.make_block_ptr(base=B, shape=(N, K), strides=(stride_bn, stride_bk), offsets=(0, 0),\n                                   block_shape=(BLOCK_N, BLOCK_K), order=(1, 0))\n    c_tile_ptr = tl.make_block_ptr(base=C, shape=(N, K), strides=(stride_cn, stride_ck), offsets=(0, 0),\n                                   block_shape=(BLOCK_N, BLOCK_K), order=(1, 0))\n    e_tile_ptr = tl.make_block_ptr(base=E, shape=(M, K), strides=(stride_em, stride_ek), offsets=(pid * BLOCK_M, 0),\n                                   block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n\n    acc_e = tl.zeros((BLOCK_M, BLOCK_K), dtype=tl.float32)\n    a = tl.load(a_tile_ptr)\n    for i in range(0, N, BLOCK_N):\n        b = tl.load(b_tile_ptr)\n        o_ab = tl.dot(a, tl.trans(b))\n        c = tl.load(c_tile_ptr)\n        o_ab = o_ab.to(tl.float16)\n        acc_e += tl.dot(o_ab, c)\n        b_tile_ptr = tl.advance(b_tile_ptr, [BLOCK_N, 0])\n        c_tile_ptr = tl.advance(c_tile_ptr, [BLOCK_N, 0])\n\n    acc_e = acc_e.to(tl.float16)\n    tl.store(e_tile_ptr, acc_e)\n\n##################################################################################################################################################  \nimport numpy as np\nimport random\nimport torch \nimport os\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n######################################## HELPERS for Eval ########################################\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ########################################\n\n\n@pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"not passed on ampere\")\ndef test_gemm_fusion(request):\n    set_seed()\n\n\n    M, N, K = 4096, 4096, 64\n    BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 64\n    A = torch.empty((M, K), dtype=torch.float16, device='cuda').normal_(mean=0.1, std=0.2)\n    B = torch.empty((N, K), dtype=torch.float16, device='cuda').normal_(mean=0.1, std=0.2)\n    C = torch.empty((N, K), dtype=torch.float16, device='cuda').normal_(mean=0.1, std=0.2)\n    E = torch.empty((M, K), dtype=torch.float16, device='cuda')\n    ref_out = torch.matmul(torch.matmul(A, B.T), C)\n    num_warps = 4\n    grid = (triton.cdiv(M, BLOCK_M), 1)\n    gemm_fusion_kernel[grid](\n        A, B, C, E, M, N, K,  #\n        A.stride(0), A.stride(1),  #\n        B.stride(0), B.stride(1),  #\n        C.stride(0), C.stride(1),  #\n        E.stride(0), E.stride(1),  #\n        BLOCK_M, BLOCK_N, BLOCK_K,  #\n        num_warps=num_warps)\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\") \n    result_gold[sanitized_key_name] = E.clone().detach().cpu()\n    ###################################################################\n\n    torch.testing.assert_close(ref_out, E, atol=1e-2, rtol=1e-2)\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef gemm_fusion_triton_wrapper(A_in, B_in, C_in, E_buffer, M_dim, N_dim, K_dim, \n                               block_m_const, block_n_const, num_warps_launch):\n    # K_dim is also block_k_const for this kernel\n    block_k_const = K_dim\n    \n    grid = (triton.cdiv(M_dim, block_m_const), 1) # As per original test\n    \n    gemm_fusion_kernel[grid](\n        A_in, B_in, C_in, E_buffer, M_dim, N_dim, K_dim,\n        A_in.stride(0), A_in.stride(1),\n        B_in.stride(0), B_in.stride(1),\n        C_in.stride(0), C_in.stride(1),\n        E_buffer.stride(0), E_buffer.stride(1),\n        BLOCK_M=block_m_const, BLOCK_N=block_n_const, BLOCK_K=block_k_const,\n        num_warps=num_warps_launch\n    )\n    return E_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_gemm_fusion_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    # Op1: Intermediate(M,N) = A(M,K) @ B.T(K,N) -> 2 * M * N * K\n    # Op2: Out(M,K) = Intermediate(M,N) @ C(N,K) -> 2 * M * K * N \n    flops = 2 * M * N * K + 2 * M * K * N \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_gemm_fusion_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    dtype_str = params.get('dtype_str', 'fp16')\n    current_dtype = torch.float16\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    bytes_a = M * K * element_size\n    bytes_b = N * K * element_size # B is (N,K)\n    bytes_c_read = N * K * element_size # C is (N,K)\n    bytes_e_write = M * K * element_size # E is (M,K)\n    total_bytes = bytes_a + bytes_b + bytes_c_read + bytes_e_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"gemm_fusion_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# K must be small enough for block_k = K to fit in shared memory for BOTH dots.\n# Constraint approx (fp16): block_m*K + 2*K*block_n + block_m*block_n <= 32768\nGEMM_FUSION_PERF_CONFIGS = []\n# K = 32 (Very safe)\nk_val = 32\nfor M_val in [128, 256, 512, 1024, 2048, 4096]:\n    for N_val in [128, 256, 512, 1024, 2048, 4096]:\n        for bm_val in [32, 64, 128]: # block_m\n            if M_val % bm_val == 0:\n                for bn_val in [32, 64, 128]: # block_n\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        GEMM_FUSION_PERF_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n# K = 64\nk_val = 64\nfor M_val in [128, 256, 512, 1024, 2048]: # Reduced M for larger K\n    for N_val in [128, 256, 512, 1024]:   # Reduced N for larger K\n        for bm_val in [32, 64, 128]:\n            if M_val % bm_val == 0:\n                for bn_val in [32, 64]: # Smaller block_n for K=64\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        GEMM_FUSION_PERF_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n# K = 128 (Requires smaller block_m, block_n)\nk_val = 128\nfor M_val in [64, 128, 256, 512]:\n    for N_val in [64, 128, 256]:\n        for bm_val in [16, 32, 64]:\n            if M_val % bm_val == 0:\n                for bn_val in [16, 32]:\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        GEMM_FUSION_PERF_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\nunique_configs = []\nseen_configs_str = set()\nfor cfg in GEMM_FUSION_PERF_CONFIGS:\n    cfg_str = f\"M{cfg['M']}_N{cfg['N']}_K{cfg['K']}_bm{cfg['bm']}_bn{cfg['bn']}\"\n    if cfg_str not in seen_configs_str:\n        unique_configs.append(cfg)\n        seen_configs_str.add(cfg_str)\nGEMM_FUSION_PERF_CONFIGS = unique_configs\nprint(f\"Generated {len(GEMM_FUSION_PERF_CONFIGS)} unique performance test configurations for gemm_fusion.\")\n\nGEMM_FUSION_DTYPES_FOR_PERF = ['fp16'] # Original test uses fp16\n# NUM_WARPS_FOR_PERF = [4, 8] # Original test used 4, can parametrize if needed\n\n@pytest.mark.parametrize(\"test_params_dict\", GEMM_FUSION_PERF_CONFIGS)\n@pytest.mark.parametrize(\"dtype_str\", GEMM_FUSION_DTYPES_FOR_PERF)\n# @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"not passed on ampere\") # Original skip\ndef test_performance(test_params_dict, dtype_str, request):\n    # Apply capability skip if needed, e.g. for specific dtypes or features\n    # if torch.cuda.get_device_capability()[0] < 8 and dtype_str == 'bf16':\n    #     pytest.skip(\"bf16 requires Ampere+\")\n\n    set_seed()\n    M = test_params_dict['M']\n    N = test_params_dict['N']\n    K = test_params_dict['K']\n    block_m = test_params_dict['bm']\n    block_n = test_params_dict['bn']\n    \n    block_k_const = K # Kernel requires BLOCK_K == K\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    A = torch.randn((M, K), dtype=current_dtype, device='cuda')\n    B = torch.randn((N, K), dtype=current_dtype, device='cuda')\n    C_mat = torch.randn((N, K), dtype=current_dtype, device='cuda')\n    E_buffer = torch.empty((M, K), dtype=current_dtype, device='cuda')\n    \n    num_warps_launch = 4 # As in original test_gemm_fusion\n\n    op_lambda = lambda: gemm_fusion_triton_wrapper(\n        A, B, C_mat, E_buffer, M, N, K,\n        block_m, block_n, num_warps_launch\n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K,\n        \"block_m\": block_m, \"block_n\": block_n, \"block_k\": block_k_const,\n        \"dtype_str\": dtype_str, \"num_warps\": num_warps_launch\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_gemm_fusion_gbps,\n                                            tflops_calculator=calculate_gemm_fusion_tflops)\n\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n        \n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_chained_matmul.py",
        "target_kernel_name": "chained_matmul_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `chained_matmul_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `chained_matmul_kernel`,  is designed to perform chained matrix multiplication of the form `(A @ B.T) @ C` on a GPU.\n\n**Your objective is to implement the body of `chained_matmul_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `chained_matmul_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `chained_matmul_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `chained_matmul_kernel` whilst keeping other things intact.\n\n######################################## Imports ########################################\nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ########################################\n\n\n\n@triton.jit\ndef chained_matmul_kernel(A,  # Pointer to the first input tensor `A`. Expected shape: (m, k). This tensor provides the first operand in the (A @ B.T) operation.\n                            B,  # Pointer to the second input tensor `B`. Expected shape: (n, k). The transpose of this tensor (B.T, shape (k, n)) is used as the second operand in the (A @ B.T) operation.\n                            C,  # Pointer to the third input tensor `C`. Expected shape: (n, k). This tensor is the second operand in the ((A @ B.T) @ C) operation.\n                            out,  # Pointer to the output tensor `out`. Expected shape: (m, k). This tensor will store the result of the chained matrix multiplication (A @ B.T) @ C.\n                            m,    # Integer representing the 'm' dimension. This is the number of rows in matrix `A` and the output matrix `out`.\n                            n,    # Integer representing the 'n' dimension. This is the number of rows in matrices `B` and `C`. It also becomes the shared inner dimension after the A @ B.T operation (i.e., A @ B.T results in an m x n matrix). The kernel iterates over this dimension in `block_n` sized chunks.\n                            k: tl.constexpr,  # Compile-time constant integer for the 'k' dimension. This is the number of columns in matrices `A`, `B`, `C`, and `out`. It's also the shared inner dimension for B.T @ C if C were (k,p) and for A @ B.T if B.T were (k,n). In this kernel, it's the common feature dimension.\n                            block_m: tl.constexpr,  # Compile-time constant integer defining the tile size for the 'm' dimension. Each kernel instance (program) will process `block_m` rows of matrix `A` (and write `block_m` rows to `out`) at a time.\n                            block_n: tl.constexpr,  # Compile-time constant integer defining the tile size for the 'n' dimension. The kernel will iterate through the 'n' dimension in steps of `block_n` when processing matrices `B` and `C`.\n                            block_k: tl.constexpr  # Compile-time constant integer defining the tile size for the 'k' dimension. For this specific kernel, there's a constraint `block_k == k`, meaning the entire 'k' dimension is processed at once within the dot products, rather than being tiled itself for reduction.\n                           ):\n    \"\"\"\n    Brief description of the kernel:\n    This Triton JIT-compiled kernel, `chained_matmul_kernel`, is designed to efficiently compute\n    a chained matrix multiplication of the form `(A @ B.T) @ C` on a GPU.\n    It takes three input matrices: `A` of shape `(m, k)`, `B` of shape `(n, k)`, and `C` of shape `(n, k)`.\n    The transpose of `B` is used in the first multiplication, resulting in an intermediate\n    matrix of shape `(m, n)`. This intermediate result is then multiplied by `C` (after `C` is\n    effectively processed column-wise due to the dot product with the intermediate, or rather,\n    the accumulation logic results in an `(m,k)` output from `(m,n) @ (n,k)` where the second `(n,k)`\n    is `C`). The final output matrix `out` has the shape `(m, k)`.\n    The kernel utilizes a tiled approach for parallel processing. \n\n\n    The user should implement the logic for (A @ B.T) @ C using Triton programming constructs.\n    This typically involves:\n    - Calculating program IDs and offsets for the current block.\n    - Loading tiles of A, B, and C.\n    - Performing the dot products and accumulations in a loop over the 'n' dimension.\n    - Handling boundary conditions carefully with masks.\n    - Storing the resulting tile to the output tensor `out`\n\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ######################################## \n\n\n@triton.jit\ndef chained_matmul_kernel(A,  # shape: (m, k)\n                            B,  # shape: (n, k)\n                            C,  # shape: (n, k)\n                            out,  # shape: (m, k)\n                            m, n, k: tl.constexpr,  #\n                            block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr):\n\n    tl.static_assert(block_k == k, f\"expected block_k == k but got {block_k} != {k}\")\n\n    block_ix = tl.program_id(0)\n    a_tile = (block_ix * block_m + tl.arange(0, block_m))[:, None] * block_k \\\n        + tl.arange(0, block_k)[None, :]\n\n    a = tl.load(A + a_tile, mask=a_tile < m * k, other=0.0)\n\n    acc = tl.zeros([block_m, block_k], dtype=tl.float32)\n\n    for loop_block_start in range(0, n, block_n):\n        bc_tile = (loop_block_start + tl.arange(0, block_n))[:, None] * block_k \\\n            + tl.arange(0, block_k)[None, :]\n        b = tl.load(B + bc_tile, mask=bc_tile < n * k, other=0.0)\n\n        intermediate = tl.dot(a, tl.trans(b))\n        intermediate_mask = ((loop_block_start + tl.arange(0, block_n)) < n)[None, :] \\\n            * (tl.arange(0, block_m) < m)[:, None]\n\n        intermediate = tl.where(intermediate_mask, intermediate, 0.0)\n\n        c = tl.load(C + bc_tile, mask=bc_tile < n * k)\n\n        acc += tl.dot(intermediate.to(A.dtype.element_ty), c)\n\n    tl.store(out + a_tile, acc.to(A.dtype.element_ty), mask=a_tile < m * k)\n\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\ndef chained_matmul_reference(a, b, c):\n    intermediate = torch.einsum('MK,NK->MN', a, b)\n    return torch.einsum('MN,NK->MK', intermediate, c)\n\n\ndef test_chained_matmul(request, device='cuda'):\n    # Regression test for issue #1601\n    set_seed()\n\n\n    m, n, k = 32, 64, 128\n    block_m, block_n, block_k = 16, 32, k\n\n    grid = (triton.cdiv(m, block_m), )\n    a = torch.randint(low=0, high=2, size=(m, k), dtype=torch.float16, device=device)\n    b = torch.randint(low=0, high=2, size=(n, k), dtype=torch.float16, device=device)\n    c = torch.randint_like(b, low=0, high=2)\n    triton_result = torch.zeros_like(a)\n\n    torch_result = chained_matmul_reference(a, b, c)\n    chained_matmul_kernel[grid](\n        a, b, c, triton_result, m, n, k,  #\n        block_m=block_m, block_n=block_n, block_k=block_k)\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = triton_result.clone().detach().cpu()\n    ################################################################### \n    assert (torch_result == triton_result).all()\n\n# --- Python wrapper for the kernel ---\ndef chained_matmul_triton_wrapper(A_in, B_in, C_in, out_buffer, block_m, block_n):\n    m, k_a = A_in.shape\n    n, k_b = B_in.shape\n    _n_c, k_c = C_in.shape # C is also (n,k)\n    assert k_a == k_b and k_a == k_c, \"K dimensions must match for A, B, C\"\n    \n    # Kernel's k is a constexpr fixed to k_a (full K dimension)\n    # Kernel's block_k is also fixed to k_a\n    \n    grid = (triton.cdiv(m, block_m),) # Grid is 1D, iterating over M blocks\n    \n    chained_matmul_kernel[grid](\n        A_in, B_in, C_in, out_buffer,\n        m, n, k_a, # runtime m, n, k\n        block_m=block_m, block_n=block_n, block_k=k_a # constexpr block_m, block_n, block_k=k\n        # num_warps not in kernel signature, would be for autotune\n    )\n    return out_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_chained_matmul_tflops(params: dict, ms: float) -> float:\n    m, n, k = params['M'], params['N'], params['K']\n    # Op1: Intermediate(M,N) = A(M,K) @ B.T(K,N) -> 2 * M * N * K FLOPs\n    # Op2: Out(M,K) = Intermediate(M,N) @ C(N,K) -> 2 * M * K * N FLOPs (Note: C is (N,K))\n    # Total FLOPs = 4 * M * N * K\n    flops = 2 * m * n * k + 2 * m * k * n \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_chained_matmul_gbps(params: dict, ms: float) -> float:\n    m, n, k = params['M'], params['N'], params['K']\n    dtype_str = params.get('dtype_str', 'fp16')\n    current_dtype = torch.float16\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    bytes_a = m * k * element_size\n    bytes_b = n * k * element_size\n    bytes_c_read = n * k * element_size\n    bytes_out_write = m * k * element_size\n    total_bytes = bytes_a + bytes_b + bytes_c_read + bytes_out_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"chained_matmul_triton_perf\"\n\n# --- REVISED Pytest parametrize for performance testing ---\n# K must be small enough for block_k = K to fit in shared memory.\n# Constraint: K * (block_m + block_n) <= 32768 (for fp16, 64KB shared mem limit)\n\nCM_PERF_TEST_CONFIGS = []\n# Shared memory constraint (fp16): block_m*K + 2*K*block_n + block_m*block_n <= 32768\n\n# K = 64\nk_val = 64\nfor M_val in [64, 128, 256]:\n    for N_val in [64, 128, 256]: # N is the total dimension, looped over by block_n\n        for bm_val in [16, 32, 64]:\n            if M_val % bm_val == 0:\n                for bn_val in [16, 32]: # Keep block_n small\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n                # Try bn_val = 64 if bm_val is small\n                if bm_val <= 32:\n                    bn_val = 64\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\n\n# K = 128\nk_val = 128\nfor M_val in [64, 128, 256]:\n    for N_val in [32, 64, 128]: \n        for bm_val in [16, 32]: # Keep block_m smaller for larger K\n            if M_val % bm_val == 0:\n                for bn_val in [16, 32]: # Keep block_n small\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n        # Try bm_val = 64 if bn_val is very small\n        if M_val % 64 == 0:\n            bm_val = 64\n            bn_val = 16 \n            if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                 CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\n\n# K = 256 (Requires even smaller block_m, block_n)\nk_val = 256\nfor M_val in [32, 64, 128]: \n    for N_val in [16, 32, 64]:  \n        for bm_val in [16, 32]: \n            if M_val % bm_val == 0:\n                for bn_val in [16]: # block_n must be very small\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n                # Try bn_val = 32 if bm_val is 16\n                if bm_val == 16:\n                    bn_val = 32\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768: # 16*256 + 2*256*32 + 16*32 = 4096 + 16384 + 512 = 20992 (OK)\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\n\n# K = 512 (Extremely restrictive for this kernel design) - Likely only 16x16 blocks will work\nk_val = 512\nfor M_val in [16, 32, 64]: \n    for N_val in [16, 32]: \n        for bm_val in [16]: \n            if M_val % bm_val == 0:\n                for bn_val in [16]: \n                    # 16*512 + 2*512*16 + 16*16 = 8192 + 16384 + 256 = 24832 (OK)\n                    if (bm_val*k_val + 2*k_val*bn_val + bm_val*bn_val) <= 32768:\n                        CM_PERF_TEST_CONFIGS.append({'M': M_val, 'N': N_val, 'K': k_val, 'bm': bm_val, 'bn': bn_val})\n\n\n# Remove duplicates\nunique_configs = []\nseen_configs_str = set()\nfor cfg in CM_PERF_TEST_CONFIGS:\n    # Create a string representation for checking uniqueness easily\n    cfg_str = f\"M{cfg['M']}_N{cfg['N']}_K{cfg['K']}_bm{cfg['bm']}_bn{cfg['bn']}\"\n    if cfg_str not in seen_configs_str:\n        unique_configs.append(cfg)\n        seen_configs_str.add(cfg_str)\nCM_PERF_TEST_CONFIGS = unique_configs\n\nprint(f\"Generated {len(CM_PERF_TEST_CONFIGS)} unique performance test configurations for chained_matmul.\")\n\n# Only fp16 for now to reduce matrix and focus on shared mem issue\nCM_DTYPES_FOR_PERF = ['fp16']\n# CM_DTYPES_FOR_PERF = ['fp16', 'fp32'] # Add fp32 later if fp16 works\n\n\n@pytest.mark.parametrize(\"test_params_dict\", CM_PERF_TEST_CONFIGS)\n@pytest.mark.parametrize(\"dtype_str\", CM_DTYPES_FOR_PERF)\ndef test_performance(test_params_dict, dtype_str, request):\n    # ... (rest of the test_chained_matmul_performance function remains the same as in the previous response)\n    set_seed()\n    m = test_params_dict['M']\n    n = test_params_dict['N']\n    k = test_params_dict['K']\n    block_m = test_params_dict['bm']\n    block_n = test_params_dict['bn']\n    \n    block_k_const = k \n\n    # This skip logic might be redundant if CM_PERF_TEST_CONFIGS is well-filtered\n    # element_size_bytes = 4 if dtype_str == 'fp32' else 2\n    # shared_mem_limit_elements = 65536 // element_size_bytes\n    # estimated_elements_needed = block_m * k + 2 * k * block_n + block_m * block_n\n    # if estimated_elements_needed > shared_mem_limit_elements:\n    #     pytest.skip(f\"Skipping M{m}N{n}K{k} bm{block_m}bn{block_n} dtype {dtype_str} due to estimated shared memory: {estimated_elements_needed*element_size_bytes} vs {65536}\")\n\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    a = torch.randn((m, k), dtype=current_dtype, device='cuda')\n    b = torch.randn((n, k), dtype=current_dtype, device='cuda') \n    c_mat = torch.randn((n, k), dtype=current_dtype, device='cuda')\n    triton_result_buffer = torch.empty((m, k), dtype=current_dtype, device='cuda')\n\n    op_lambda = lambda: chained_matmul_triton_wrapper(\n        a, b, c_mat, triton_result_buffer,\n        block_m, block_n\n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": m, \"N\": n, \"K\": k,\n        \"block_m\": block_m, \"block_n\": block_n, \"block_k\": block_k_const,\n        \"dtype_str\": dtype_str\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_chained_matmul_gbps,\n                              tflops_calculator=calculate_chained_matmul_tflops)\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_batched_vecmat.py",
        "target_kernel_name": "batched_vecmat",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `batched_vecmat`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `batched_vecmat`,  is designed to perform batched element-wise multiplication and sum operation.\n\n**Your objective is to implement the body of `batched_vecmat`.**\n\nYou must ensure that:\n1.  All arguments received by `batched_vecmat` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `batched_vecmat` and relevant helper utilities are provided in the context below. You only need to complete the code for `batched_vecmat` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ########################################\n\n@triton.jit\ndef batched_vecmat(\n    A,\n    B,\n    dim_m, dim_n, dim_k,\n    output,\n    block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr\n):\n    \"\"\"\n    Performs a batched element-wise multiplication and sum operation.\n    Effectively, for each m in dim_m, it computes the element-wise product of\n    A[m, :] with each row of B[m, :, :], and sums the results.\n    This can be expressed as: output[m, n] = sum_k (A[m, k] * B[m, n, k]).\n    This is equivalent to `torch.sum(A.unsqueeze(1) * B, dim=2)`.\n\n    Parameters\n    ----------\n    A\n        Input tensor of shape [dim_m, dim_k].\n        Interpreted as a batch of `dim_m` vectors, each of size `dim_k`.\n    B\n        Input tensor of shape [dim_m, dim_n, dim_k].\n        Interpreted as a batch of `dim_m` groups of vectors. Each group `B[i, :, :]`\n        contains `dim_n` vectors, each of size `dim_k`.\n    dim_m\n        The size of the first dimension of A and B (batch dimension).\n    dim_n\n        The size of the second dimension of B and the output tensor.\n        Represents the number of vectors in each batch entry of B.\n    dim_k\n        The size of the last dimension of A and B (the dimension over which the sum is performed).\n    output\n        Output tensor of shape [dim_m, dim_n] where the results are stored.\n    block_m\n        tl.constexpr: Tiling size for the m dimension.\n    block_n\n        tl.constexpr: Tiling size for the n dimension.\n    block_k\n        tl.constexpr: Tiling size for the k dimension (reduction dimension).\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ######################################## \n\n@triton.jit\ndef batched_vecmat(\n        # inputs\n        A,  # shape: [dim_m, dim_k]\n        B,  # shape: [dim_m, dim_n, dim_k]\n        # dimensions\n    dim_m, dim_n, dim_k,\n        # outputs\n        output,\n        # block information\n        block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr):\n    m_index = tl.program_id(0)\n    n_index = tl.program_id(1)\n    # Output tile\n    output_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_n \\\n        + (n_index * block_n + tl.arange(0, block_n))[None, :]\n\n    vecmat = tl.zeros([block_m, block_n], dtype=A.dtype.element_ty)\n    k_blocks = dim_k // block_k\n    for k_index in range(k_blocks):\n        # Load A tile\n        a_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_k \\\n            + (k_index * block_k + tl.arange(0, block_k))[None, :]\n        a = tl.load(A + a_tile)\n\n        # Load B tile, transposed to [n, m, k] in order to broadcast A on a\n        # leading dimension.\n        b_tile = (m_index * block_m + tl.arange(0, block_m))[None, :, None] * dim_n * dim_k \\\n            + (n_index * block_n + tl.arange(0, block_n))[:, None, None] * dim_k \\\n            + (k_index * block_k + tl.arange(0, block_k))[None, None, :]\n        b = tl.load(B + b_tile)\n\n        expanded_a, _ = tl.broadcast(a, b)\n        vecmat += tl.trans(tl.sum(expanded_a * b, axis=2))\n\n    tl.store(output + output_tile, vecmat)\n\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\n\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n  \ndef test_vecmat(request, device='cuda'):  \n    \"\"\"  \n    Test the batched vector-matrix multiplication kernel.  \n  \n    Args:  \n        device: The device (e.g., 'cuda' or 'cpu') on which the test is executed.  \n        request: Pytest request object used to retrieve the test case name.  \n    \"\"\"  \n    set_seed()  \n  \n    M, N, K = 128, 128, 128  \n    block_m, block_n, block_k = 16, 32, 64  \n  \n    rs = RandomState(17)  \n    A_vec = rs.randint(0, 4, (M, K)).astype('float32')  \n    B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')  \n    A = A_vec  \n    B = B_vec  \n  \n    A_tri = torch.tensor(A, device=device)  \n    B_tri = torch.tensor(B, device=device)  \n    C_tri = torch.zeros((M, N), dtype=torch.float32, device=device)  \n  \n    grid = (M // block_m, N // block_n)  \n  \n    # This is where the actual kernel would run and populate C_tri  \n    # If using the MockKernel above, C_tri will remain zeros unless populated for testing.  \n    # For the purpose of demonstrating saving, we'll assume C_tri is populated by the kernel.  \n    # To make the assert_allclose pass without a real kernel, we can compute C_tri using torch:  \n    if isinstance(batched_vecmat, MockKernel): # If using the mock  \n        A_expanded_torch = A_tri[:, None, :] # (M, 1, K)  \n        # B_tri is (M, N, K)  \n        # Element-wise product and sum over K  \n        # This is what the kernel is supposed to compute  \n        C_tri_computed_by_torch = torch.sum(A_expanded_torch * B_tri, dim=2) # (M, N)  \n        C_tri.copy_(C_tri_computed_by_torch) # Populate C_tri as the kernel would  \n    else: # If using the real kernel  \n        batched_vecmat[grid](  \n            A_tri, B_tri, M, N, K, C_tri,  #  \n            block_m=block_m, block_n=block_n, block_k=block_k,  #  \n            num_warps=4, num_stages=1)  \n  \n  \n    A_expanded_np = A[:, np.newaxis, :] # (M, 1, K)  \n    A_broadcasted_np = np.broadcast_to(A_expanded_np, (M, N, K)) # (M, N, K)  \n    AB_np = A_broadcasted_np * B # B is (M, N, K)  \n    C_ref = np.sum(AB_np, axis=2) # (M, N)  \n  \n    ################### save C_ref and C_tri as torch tensors in result_gold ###################  \n    test_case_name = request.node.name  \n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")  \n  \n    # Convert C_ref (numpy array) to a torch tensor on CPU  \n    # C_ref is float32 because A_vec and B_vec are float32.  \n    C_ref_torch = torch.tensor(C_ref, dtype=torch.float32, device='cpu')  \n  \n    # Store C_ref_torch and C_tri (on CPU) in result_gold  \n    # C_tri is already torch.float32 as defined.  \n    result_gold[sanitized_key_name + \"_ref\"] = C_ref_torch  \n    result_gold[sanitized_key_name + \"_tri\"] = C_tri.cpu() # Ensure C_tri is on CPU for saving  \n    #########################################################################################  \n  \n    np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)  \n  \n\n\ndef test_vecmat(request, device='cuda'):\n    \"\"\"\n    Test the batched vector-matrix multiplication kernel.\n\n    Args:\n        device: The device (e.g., 'cuda' or 'cpu') on which the test is executed.\n        request: Pytest request object used to retrieve the test case name.\n    \"\"\"\n    set_seed()\n\n    M, N, K = 128, 128, 128\n    block_m, block_n, block_k = 16, 32, 64\n\n    rs = RandomState(17)\n    A_vec = rs.randint(0, 4, (M, K)).astype('float32')\n    B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')\n    A = A_vec\n    B = B_vec\n\n    A_tri = torch.tensor(A, device=device)\n    B_tri = torch.tensor(B, device=device)\n    C_tri = torch.zeros((M, N), dtype=torch.float32, device=device)\n\n    grid = (M // block_m, N // block_n)\n\n    batched_vecmat[grid](\n        A_tri, B_tri, M, N, K, C_tri,  #\n        block_m=block_m, block_n=block_n, block_k=block_k,  #\n        num_warps=4, num_stages=1)\n\n    A_expanded = A[:, np.newaxis, :]\n    A_broadcasted = np.broadcast_to(A_expanded, (M, N, K))\n    AB = A_broadcasted * B\n    C_ref = np.sum(AB, axis=2)\n\n    ################### save tri_out in result_gold ###################\n    # Convert C_ref (numpy array) to a torch tensor on CPU  \n    # C_ref is float32 because A_vec and B_vec are float32.  \n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ## triton_result is assumed to be torch tensor not numpy ndarray\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = C_tri.cpu()\n    ################################################################### \n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n\n# --- Define TFLOPS and GB/s calculators for Batched VecMat ---\ndef calculate_batched_vecmat_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    # Operation: C[m,n] = sum_k A[m,k] * B[m,n,k]\n    # For each (m,n) pair, K multiplications and K-1 additions. Approx 2*K FLOPs.\n    # There are M*N such pairs.\n    # Total FLOPs = M * N * (2 * K)\n    flops = 2 * M * N * K\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_batched_vecmat_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    dtype_str = params.get('dtype_str', 'fp32') # Original test uses float32\n\n    if dtype_str == 'fp16': current_dtype = torch.float16\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float32 # Default to float32 as per original test\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    # Read A (M,K)\n    # Read B (M,N,K)\n    # Write C (M,N)\n    bytes_a = M * K * element_size\n    bytes_b = M * N * K * element_size\n    bytes_c = M * N * element_size\n    total_bytes = bytes_a + bytes_b + bytes_c\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n# --- Python wrapper for the batched_vecmat kernel ---\ndef batched_vecmat_triton_wrapper(A_in, B_in, block_m, block_n, block_k):\n    dim_m, dim_k_a = A_in.shape\n    dim_m_b, dim_n, dim_k_b = B_in.shape\n    assert dim_m == dim_m_b, \"M dimension must match for A and B\"\n    assert dim_k_a == dim_k_b, \"K dimension must match for A and B\"\n    \n    output_tensor = torch.zeros((dim_m, dim_n), dtype=A_in.dtype, device=A_in.device)\n    \n    grid = (triton.cdiv(dim_m, block_m), triton.cdiv(dim_n, block_n))\n    \n    batched_vecmat[grid](\n        A_in, B_in,\n        dim_m, dim_n, dim_k_a, # Pass runtime dimensions\n        output_tensor,\n        # Strides can be passed if non-contiguous, e.g., A_in.stride(0), A_in.stride(1), ...\n        # For now, assume contiguous and Triton handles it.\n        block_m=block_m, block_n=block_n, block_k=block_k # Pass constexpr block sizes\n        # num_warps, num_stages are not in this kernel's signature directly (would be via @triton.autotune if used)\n    )\n    return output_tensor\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"batched_vecmat_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Define shapes and block sizes for performance testing\nBV_SHAPES_FOR_PERF = [\n    # M,  N,   K\n    (128, 128, 128),\n    (256, 256, 256),\n    (512, 64,  1024), # More K\n    (64,  512, 256),  # More N\n    (1024, 32, 512),  # Large M, smaller N\n]\n# Block sizes are constexpr. For a fair benchmark of *this specific kernel version*,\n# we should use fixed block sizes or create different test entries for different block configs.\n# The original test_vecmat uses fixed block_m=16, block_n=32, block_k=64.\n# Let's use these, and optionally add more block configs.\nBV_BLOCK_CONFIGS_FOR_PERF = [\n    # block_m, block_n, block_k\n    (16, 32, 64),\n    (32, 32, 128), # Example alternative block config\n    (16, 64, 64),\n]\nBV_DTYPES_FOR_PERF = ['fp32', 'fp16'] # Original uses fp32, add fp16 for variety\n\n@pytest.mark.parametrize(\"M, N, K\", BV_SHAPES_FOR_PERF)\n@pytest.mark.parametrize(\"block_m, block_n, block_k\", BV_BLOCK_CONFIGS_FOR_PERF)\n@pytest.mark.parametrize(\"dtype_str\", BV_DTYPES_FOR_PERF)\ndef test_performance(M, N, K, block_m, block_n, block_k, dtype_str, request):\n    set_seed()\n\n    # Ensure M is divisible by block_m, N by block_n for simpler grid/kernel.\n    # The kernel itself should handle arbitrary M,N with masking.\n    # For performance, often aligned sizes are tested, but let's assume kernel handles it.\n    if M % block_m != 0 or N % block_n != 0:\n        pytest.skip(f\"Skipping M={M},N={N} with block_m={block_m},block_n={block_n} due to non-divisibility for perf simplicity.\")\n\n    if dtype_str == 'fp16': current_dtype = torch.float16\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16 # Not in original, but for consistency\n    else: current_dtype = torch.float32\n\n    # Input tensors\n    # Original test used rs.randint(0,4,...).astype('float32')\n    # For perf, torch.randn is more common.\n    A_tri = torch.randn((M, K), dtype=current_dtype, device='cuda')\n    B_tri = torch.randn((M, N, K), dtype=current_dtype, device='cuda')\n    # C_tri (output) will be created by the wrapper\n\n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: batched_vecmat_triton_wrapper(A_tri, B_tri, block_m, block_n, block_k)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K,\n        \"block_m\": block_m, \"block_n\": block_n, \"block_k\": block_k,\n        \"dtype_str\": dtype_str\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_batched_vecmat_gbps,\n                              tflops_calculator=calculate_batched_vecmat_tflops)\n    \n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_iv_dependent_matmul.py",
        "target_kernel_name": "iv_dependent_matmul",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `iv_dependent_matmul`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `iv_dependent_matmul`,  is designed to perform  tiled matrix multiplication (C = A @ B).\n\n**Your objective is to implement the body of `iv_dependent_matmul`.**\n\nYou must ensure that:\n1.  All arguments received by `iv_dependent_matmul` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `iv_dependent_matmul` and relevant helper utilities are provided in the context below. You only need to complete the code for `iv_dependent_matmul` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n@triton.jit\ndef iv_dependent_matmul(a_ptr, b_ptr, c_ptr,\n                        M, N, K,\n                        stride_am, stride_ak,\n                        stride_bk, stride_bn,\n                        stride_cm, stride_cn,\n                        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n                        type: tl.constexpr):\n    \"\"\"\n    Performs a tiled matrix multiplication (C = A @ B) with various strategies\n    for managing and advancing pointers to input matrices A and B within the\n    inner K-loop. The specific strategy for pointer updates is determined by\n    the `type` parameter. This kernel is designed to explore different\n    instruction scheduling and memory access patterns related to pointer\n    arithmetic within the loop.\n\n    Parameters:\n    -----------\n    a_ptr : tl.pointer_type\n        Pointer to the first input matrix A.\n    b_ptr : tl.pointer_type\n        Pointer to the second input matrix B.\n    c_ptr : tl.pointer_type\n        Pointer to the output matrix C.\n    M : int\n        Number of rows in matrix A and C.\n    N : int\n        Number of columns in matrix B and C.\n    K : int\n        Number of columns in matrix A and rows in matrix B (the dimension being reduced).\n    stride_am : int\n        Stride for the M dimension (rows) of matrix A.\n    stride_ak : int\n        Stride for the K dimension (columns) of matrix A.\n    stride_bk : int\n        Stride for the K dimension (rows) of matrix B.\n    stride_bn : int\n        Stride for the N dimension (columns) of matrix B.\n    stride_cm : int\n        Stride for the M dimension (rows) of matrix C.\n    stride_cn : int\n        Stride for the N dimension (columns) of matrix C.\n    BLOCK_SIZE_M : tl.constexpr\n        The size of the block used for tiling along the M dimension.\n    BLOCK_SIZE_N : tl.constexpr\n        The size of the block used for tiling along the N dimension.\n    BLOCK_SIZE_K : tl.constexpr\n        The size of the block used for tiling along the K dimension (inner loop).\n    type : tl.constexpr (str)\n        A string literal controlling the pointer update strategy for `a_ptr` and `b_ptr`\n        within the K-loop. Affects how `a_ptrs` and `b_ptrs` are calculated in each\n        iteration of the K-loop.\n        Possible values:\n        - \"pre_load\": Pointers are calculated at the beginning of each K-loop iteration.\n        - \"post_load\": Pointers are calculated at the end of each K-loop iteration for the next iteration.\n        - \"post_pre_mixed\": `a_ptrs` is calculated at the beginning, `b_ptrs` at the end for the next iteration.\n        - \"post_load_two_iters\": Pointers are advanced to prefetch/prepare for two iterations ahead.\n        - \"post_load_three_iters\": Pointers are advanced to prefetch/prepare for three iterations ahead.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n@triton.jit\ndef iv_dependent_matmul(a_ptr, b_ptr, c_ptr,  #\n            M, N, K,  #\n            stride_am, stride_ak,  #\n            stride_bk, stride_bn,  #\n            stride_cm, stride_cn,  #\n            BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n            type: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptr = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptr = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    a_ptrs = a_ptr\n    b_ptrs = b_ptr\n    if type == \"post_load_two_iters\":\n        a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n        b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n    elif type == \"post_load_three_iters\":\n        a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n        b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n        a_ptrs_next_next = a_ptr + 2 * BLOCK_SIZE_K * stride_ak\n        b_ptrs_next_next = b_ptr + 2 * BLOCK_SIZE_K * stride_bk\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        if type == \"pre_load\":\n            a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n            b_ptrs = b_ptr + k * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_pre_mixed\":\n            a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        if type == \"post_load\":\n            a_ptrs = a_ptr + (k + 1) * BLOCK_SIZE_K * stride_ak\n            b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_pre_mixed\":\n            b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_load_two_iters\":\n            a_ptrs = a_ptrs_next\n            b_ptrs = b_ptrs_next\n            a_ptrs_next = a_ptr + (k + 2) * BLOCK_SIZE_K * stride_ak\n            b_ptrs_next = b_ptr + (k + 2) * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_load_three_iters\":\n            a_ptrs = a_ptrs_next\n            b_ptrs = b_ptrs_next\n            a_ptrs_next = a_ptrs_next_next\n            b_ptrs_next = b_ptrs_next_next\n            a_ptrs_next_next = a_ptr + (k + 3) * BLOCK_SIZE_K * stride_ak\n            b_ptrs_next_next = b_ptr + (k + 3) * BLOCK_SIZE_K * stride_bk\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n@pytest.mark.parametrize(\"type\",\n                         [\"pre_load\", \"post_load\", \"post_pre_mixed\", \"post_load_two_iters\", \"post_load_three_iters\"])\ndef test_iv_dependent_matmul(type, request, device='cuda'):\n\n    \n    set_seed()\n\n    M = 256\n    K = 256\n    N = 256\n    BLOCK_SIZE_K = 32\n    BLOCK_SIZE_N = 32\n    BLOCK_SIZE_M = 32\n\n    a = torch.rand((M, K), device=device)\n    b = torch.rand((K, N), device=device)\n\n    torch_output = torch.mm(a, b)\n    triton_output = torch.empty_like(torch_output, device=torch_output.device)\n\n    def grid(META):\n        return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n\n    num_stages = 4 if type == \"post_load_three_iters\" else 3\n    iv_dependent_matmul[grid](\n        a, b, triton_output, M, N, K,  #\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1),  #\n        triton_output.stride(0), triton_output.stride(1),  #\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K, type=type,  #\n        num_stages=num_stages)\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save triton_output in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = triton_output.clone().detach().cpu()\n    ################################################################### \n\n    torch.testing.assert_close(torch_output, triton_output, rtol=1e-2, atol=1e-2)\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef iv_dependent_matmul_triton_wrapper(a_tensor, b_tensor, c_buffer,\n                                       M_dim, N_dim, K_dim,\n                                       block_m_const, block_n_const, block_k_const,\n                                       kernel_type_str, num_stages_launch, num_warps_launch):\n    grid = (triton.cdiv(M_dim, block_m_const) * triton.cdiv(N_dim, block_n_const), )\n    \n    iv_dependent_matmul[grid](\n        a_tensor, b_tensor, c_buffer, M_dim, N_dim, K_dim,\n        a_tensor.stride(0), a_tensor.stride(1),\n        b_tensor.stride(0), b_tensor.stride(1),\n        c_buffer.stride(0), c_buffer.stride(1),\n        BLOCK_SIZE_M=block_m_const, BLOCK_SIZE_N=block_n_const,\n        BLOCK_SIZE_K=block_k_const, type=kernel_type_str,\n        num_stages=num_stages_launch,\n        num_warps=num_warps_launch\n    )\n    return c_buffer\n\n# --- Define TFLOPS and GB/s calculators for GEMM ---\ndef calculate_gemm_tflops(params: dict, ms: float) -> float: # Standard GEMM\n    M, N, K = params['M'], params['N'], params['K']\n    flops = 2 * M * N * K \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_gemm_gbps(params: dict, ms: float) -> float: # Standard GEMM\n    M, N, K = params['M'], params['N'], params['K']\n    dtype_str = params.get('input_dtype_str', 'fp32') # Inputs are fp32 in this test\n    \n    current_dtype = torch.float32\n    if dtype_str == 'fp16': current_dtype = torch.float16\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    # Output is fp16\n    out_element_size = torch.tensor([], dtype=torch.float16).element_size()\n\n    bytes_a = M * K * element_size\n    bytes_b = K * N * element_size\n    bytes_c_write = M * N * out_element_size # Output is fp16\n    total_bytes = bytes_a + bytes_b + bytes_c_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"iv_dependent_matmul_perf\"\n\n# --- Pytest parametrize for performance testing ---\nIV_MATMUL_SHAPES_FOR_PERF = [\n    # M,   N,   K\n    (256, 256, 256), (512, 512, 128), (1024, 1024, 64),\n    (2048, 512, 128), (512, 2048, 64)\n]\nIV_MATMUL_BLOCK_CONFIGS_FOR_PERF = [\n    # BM, BN, BK\n    (32, 32, 32), (64, 64, 32), (64, 32, 64), (32, 64, 64),\n    (128, 128, 32), (128, 64, 64), (64, 128, 64) \n    # Add (128,128,64) if K can be larger for BLOCK_K\n]\nIV_MATMUL_TYPES_FOR_PERF = [\"pre_load\", \"post_load\", \"post_pre_mixed\", \"post_load_two_iters\", \"post_load_three_iters\"]\nIV_MATMUL_DTYPES_FOR_PERF = ['fp32', 'fp16'] # Input dtypes for A and B\n# num_stages and num_warps are launch hints\nNUM_STAGES_FOR_PERF = [2, 3, 4]\nNUM_WARPS_FOR_PERF = [4, 8]\n\n\n@pytest.mark.parametrize(\"m_n_k_shape\", IV_MATMUL_SHAPES_FOR_PERF)\n@pytest.mark.parametrize(\"block_config\", IV_MATMUL_BLOCK_CONFIGS_FOR_PERF)\n@pytest.mark.parametrize(\"kernel_type_str\", IV_MATMUL_TYPES_FOR_PERF)\n@pytest.mark.parametrize(\"input_dtype_str\", IV_MATMUL_DTYPES_FOR_PERF)\n@pytest.mark.parametrize(\"num_stages_launch\", NUM_STAGES_FOR_PERF)\n@pytest.mark.parametrize(\"num_warps_launch\", NUM_WARPS_FOR_PERF)\ndef test_performance(m_n_k_shape, block_config, kernel_type_str, input_dtype_str, \n                                         num_stages_launch, num_warps_launch, request):\n    set_seed()\n    M, N, K = m_n_k_shape\n    BLOCK_M, BLOCK_N, BLOCK_K = block_config\n\n    # Skip if BLOCK_K is too large for K (kernel has K-loop)\n    if BLOCK_K > K :\n        pytest.skip(f\"BLOCK_K ({BLOCK_K}) > K ({K}) not meaningful for tiled K-loop.\")\n    \n    # Shared memory check (approx for one dot A(BM,BK) @ B(BK,BN) -> C(BM,BN))\n    # Max shared mem usage is for A and B blocks in one dot.\n    # Smem elements = BM*BK + BK*BN\n    # This kernel has a K-loop, so BK is a tile size, not full K.\n    if input_dtype_str == 'fp32': current_in_dtype = torch.float32; elem_size = 4\n    elif input_dtype_str == 'bf16': current_in_dtype = torch.bfloat16; elem_size = 2\n    else: current_in_dtype = torch.float16; elem_size = 2\n    \n    # Output is always fp16 by the kernel\n    output_dtype = torch.float16\n\n    smem_elements_needed = (BLOCK_M * BLOCK_K) + (BLOCK_K * BLOCK_N)\n    # num_stages can increase shared memory usage for pipelining\n    # A rough factor could be num_stages, or num_stages/2 + 1 etc.\n    # Let's use a factor of num_stages for a conservative estimate.\n    if smem_elements_needed * elem_size * (num_stages_launch if num_stages_launch > 1 else 1) > 65536:\n        pytest.skip(f\"Skipping M{M}N{N}K{K} Blocks({BLOCK_M},{BLOCK_N},{BLOCK_K}) \"\n                    f\"dtype {input_dtype_str} stages {num_stages_launch} \"\n                    f\"due to estimated shared memory.\")\n\n    a = torch.randn((M, K), device='cuda', dtype=current_in_dtype)\n    b = torch.randn((K, N), device='cuda', dtype=current_in_dtype)\n    # Kernel casts output to tl.float16\n    triton_output_buffer = torch.empty((M, N), device='cuda', dtype=output_dtype)\n\n    op_lambda = lambda: iv_dependent_matmul_triton_wrapper(\n        a, b, triton_output_buffer, M, N, K,\n        BLOCK_M, BLOCK_N, BLOCK_K,\n        kernel_type_str, num_stages_launch, num_warps_launch\n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K,\n        \"BLOCK_M\": BLOCK_M, \"BLOCK_N\": BLOCK_N, \"BLOCK_K\": BLOCK_K,\n        \"type\": kernel_type_str, \"input_dtype_str\": input_dtype_str, \"output_dtype_str\": \"fp16\",\n        \"num_stages\": num_stages_launch, \"num_warps\": num_warps_launch\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_gemm_gbps,\n                                            tflops_calculator=calculate_gemm_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_reverse_range.py",
        "target_kernel_name": "reverse_range",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `reverse_range`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `reverse_range`,  is designed to reverse a specific 512-element segment from an input tensor and stores it into an output tensor.\n\n**Your objective is to implement the body of `reverse_range`.**\n\nYou must ensure that:\n1.  All arguments received by `reverse_range` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `reverse_range` and relevant helper utilities are provided in the context below. You only need to complete the code for `reverse_range` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n@triton.jit\ndef reverse_range(in_ptr, out_ptr):\n    \"\"\"\n    Reverses a specific 512-element segment from an input tensor and stores it\n    into an output tensor.\n\n    This kernel operates on fixed-size blocks of 512 elements.\n    It performs the following operation for each element `i` in the range `[0, 511]`:\n    `out_ptr[i] = in_ptr[512 - i]`\n\n    This means the kernel reads elements from `in_ptr + 1` up to `in_ptr + 512`\n    (inclusive) and writes them in reversed order to `out_ptr + 0` up to\n    `out_ptr + 511` (inclusive).\n\n    Parameters\n    ----------\n    in_ptr : tl.pointer_type\n        A pointer to the input tensor. The kernel reads a block of 512 elements\n        from this tensor. Specifically, it reads from memory locations\n        `in_ptr + 512` down to `in_ptr + 1`. For example, the value at\n        `in_ptr + 512` is read first (for `x0=0` in the original implementation)\n        and stored at `out_ptr + 0`.\n    out_ptr : tl.pointer_type\n        A pointer to the output tensor. The kernel writes the 512 reversed\n        elements to this tensor. Specifically, it writes to memory locations\n        `out_ptr + 0` through `out_ptr + 511`. For example, `out_ptr + 0`\n        receives the value from `in_ptr + 512`.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\nfrom numpy.random import RandomState\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n@triton.jit\ndef reverse_range(in_ptr, out_ptr):\n    x0 = tl.arange(0, 512)\n    tmp0 = tl.load(in_ptr + (512 - x0))\n    tl.store(out_ptr + x0, tmp0)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\ndef test_reverse_range(request, device='cuda'):\n    set_seed()\n    \n\n    data = torch.randn((516, ), dtype=torch.float32, device=device)\n    res = torch.empty((512, ), dtype=torch.float32, device=device)\n    reverse_range[(1, )](data, res)\n    ref = torch.flip(data[1:513], [0])\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = res.clone().detach().cpu()\n    ################################################################### \n\n\n    assert (res == ref).all()\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef reverse_range_triton_wrapper(in_tensor_for_kernel, out_buffer, num_warps_launch):\n    # The kernel is hardcoded to process 512 elements.\n    # in_tensor_for_kernel should be the base pointer from which kernel reads data[1]..data[512]\n    # out_buffer is where results are written.\n    grid = (1,)\n    reverse_range[grid](\n        in_tensor_for_kernel, \n        out_buffer,\n        num_warps=num_warps_launch # Launch hint\n    )\n    return out_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\nKERNEL_FIXED_SIZE = 512\n\ndef calculate_reverse_range_tflops(params: dict, ms: float) -> float:\n    # This is a memory copy operation, no significant arithmetic FLOPs.\n    return 0.0 \n\ndef calculate_reverse_range_gbps(params: dict, ms: float) -> float:\n    N_processed = KERNEL_FIXED_SIZE\n    dtype_str = params.get('dtype_str', 'fp32') \n\n    current_dtype = torch.float32\n    if dtype_str == 'fp16': current_dtype = torch.float16\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    # Reads N_processed elements, Writes N_processed elements\n    bytes_read = N_processed * element_size\n    bytes_write = N_processed * element_size \n    total_bytes = bytes_read + bytes_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"reverse_range_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Kernel size is fixed at 512. We can vary dtype and num_warps.\nREVERSE_RANGE_DTYPES_FOR_PERF = ['fp32', 'fp16', 'bf16'] \nREVERSE_RANGE_NUM_WARPS_FOR_PERF = [1, 2, 4] \n\n@pytest.mark.parametrize(\"dtype_str\", REVERSE_RANGE_DTYPES_FOR_PERF)\n@pytest.mark.parametrize(\"num_warps_launch\", REVERSE_RANGE_NUM_WARPS_FOR_PERF)\ndef test_performance(dtype_str, num_warps_launch, request, device='cuda'):\n    set_seed()\n    \n    if dtype_str == 'bf16':\n        cap = torch.cuda.get_device_capability()\n        if cap[0] < 8:\n            pytest.skip(\"bf16 requires Ampere+ (arch 80+)\")\n    \n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    # Input tensor `data_perf` needs to be large enough for kernel's access pattern.\n    # Kernel reads from in_ptr[1] to in_ptr[512].\n    # So, `data_perf` needs at least 513 elements if passed directly.\n    # Original test used size 516 for `data`.\n    data_perf = torch.randn((KERNEL_FIXED_SIZE + 4, ), dtype=current_dtype, device=device) # e.g. 516 elements\n    # Output buffer `res_perf` is size 512.\n    res_perf_buffer = torch.empty((KERNEL_FIXED_SIZE, ), dtype=current_dtype, device=device)\n    \n    op_lambda = lambda: reverse_range_triton_wrapper(\n        data_perf, res_perf_buffer, num_warps_launch\n    )\n\n    bench_config = do_bench_config(warm_up=100, repetition=1000) # Simple kernel, more reps\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"N_processed\": KERNEL_FIXED_SIZE, \n        \"dtype_str\": dtype_str,\n        \"num_warps\": num_warps_launch\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_reverse_range_gbps,\n                                            tflops_calculator=calculate_reverse_range_tflops)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "rmsnorm_fwd.py",
        "target_kernel_name": "rms_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `rms_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `rms_kernel`,  is designed to perform Root Mean Square (RMS) Normalization.\n\n**Your objective is to implement the body of `rms_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `rms_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `rms_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `rms_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n####################### Imports #####################\nimport argparse\nimport torch\nimport sys\nimport pytest\nfrom itertools import product\n\nimport triton\nimport triton.language as tl\n####################### Imports #####################\n\n############################ HELPER utils ############################\n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef is_hip():\n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n\n\ndef get_num_sms():\n    current_device_index = torch.cuda.current_device()\n    current_device = torch.cuda.get_device_properties(current_device_index)\n    num_sms = current_device.multi_processor_count\n    return num_sms\n\n\ndef get_cuda_autotune_config():\n    return [\n        triton.Config({}, num_warps=4, num_stages=1),\n        triton.Config({}, num_warps=8, num_stages=1),\n        triton.Config({}, num_warps=16, num_stages=1),\n    ]\n\n\ndef get_hip_autotune_config():\n    return [triton.Config({'waves_per_eu': we}, num_warps=nw) for (we, nw) in product([0, 1, 2, 4], [4, 8, 16])]\n\n\ndef get_autotune_config():\n    if is_cuda():\n        return get_cuda_autotune_config()\n    else:\n        return get_hip_autotune_config()\n############################ HELPER utils ############################\n\n\n@triton.autotune(configs=get_autotune_config(), key=['n_rows', 'n_cols'], use_cuda_graph=True)\n@triton.jit\ndef rms_kernel(output_ptr, input_ptr, g_ptr, rsigma_ptr, input_row_stride, output_row_stride, n_rows, n_cols, epsilon,\n               ZERO_CENTERED_GAMMA: tl.constexpr, BLOCK_SIZE: tl.constexpr, USE_BLOCKED: tl.constexpr,\n               NUM_PRGMS: tl.constexpr):\n    \"\"\"\n    Triton kernel for performing Root Mean Square (RMS) Normalization.\n\n    This kernel normalizes each row of the input tensor by its RMS value,\n    applies a learnable scaling factor (gamma), and stores the result.\n    It also stores the reciprocal of the standard deviation (rsigma) for each row.\n    The kernel supports two modes of operation: a simple row-wise processing\n    and a blocked processing for potentially better performance on wider rows.\n    It is designed as a persistent kernel where each program instance can handle\n    multiple rows.\n\n    Parameters:\n    output_ptr: Pointer to the output tensor where the normalized values will be stored.\n                Shape: (n_rows, n_cols)\n    input_ptr: Pointer to the input tensor.\n               Shape: (n_rows, n_cols)\n    g_ptr: Pointer to the gamma (scale) tensor. This is a 1D tensor.\n           Shape: (n_cols,)\n    rsigma_ptr: Pointer to store the reciprocal of the standard deviation (or equivalent normalization factor)\n                for each row. This is a 1D tensor.\n                Shape: (n_rows,)\n    input_row_stride: Stride in number of elements to move from one row to the next in the input_ptr.\n    output_row_stride: Stride in number of elements to move from one row to the next in the output_ptr.\n    n_rows: The number of rows in the input and output tensors.\n    n_cols: The number of columns in the input and output tensors.\n    epsilon: A small float value added to the variance to prevent division by zero during normalization.\n    ZERO_CENTERED_GAMMA: tl.constexpr\n                         A compile-time boolean constant. If True, 1.0 is added to the gamma values\n                         before applying them, effectively making the provided gamma values adjustments\n                         around a mean of 1.0.\n    BLOCK_SIZE: tl.constexpr\n                A compile-time integer constant representing the size of blocks used for processing\n                columns. This is relevant for both the blocked and non-blocked execution paths\n                (e.g., for `tl.arange`).\n    USE_BLOCKED: tl.constexpr\n                 A compile-time boolean constant. If True, the kernel uses a blocked algorithm\n                 to iterate over columns, potentially improving cache utilization and performance\n                 for rows with many columns. If False, a simpler, direct row-wise computation is performed.\n    NUM_PRGMS: tl.constexpr\n               A compile-time integer constant. This represents the number of program instances\n               (effectively, persistent thread blocks) launched. Rows are distributed among these\n               program instances. For example, program `pid` handles rows `pid, pid + NUM_PRGMS, ...`.\n    \"\"\"\n    # Your code here\n\n",
        "label": "####################### Imports #####################\nimport argparse\nimport torch\nimport sys\nimport pytest\nfrom itertools import product\n\nimport triton\nimport triton.language as tl\n####################### Imports #####################\n\n############################ HELPER utils ############################\n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef is_hip():\n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n\n\ndef get_num_sms():\n    current_device_index = torch.cuda.current_device()\n    current_device = torch.cuda.get_device_properties(current_device_index)\n    num_sms = current_device.multi_processor_count\n    return num_sms\n\n\ndef get_cuda_autotune_config():\n    return [\n        triton.Config({}, num_warps=4, num_stages=1),\n        triton.Config({}, num_warps=8, num_stages=1),\n        triton.Config({}, num_warps=16, num_stages=1),\n    ]\n\n\ndef get_hip_autotune_config():\n    return [triton.Config({'waves_per_eu': we}, num_warps=nw) for (we, nw) in product([0, 1, 2, 4], [4, 8, 16])]\n\n\ndef get_autotune_config():\n    if is_cuda():\n        return get_cuda_autotune_config()\n    else:\n        return get_hip_autotune_config()\n############################ HELPER utils ############################\n\n\n@triton.autotune(configs=get_autotune_config(), key=['n_rows', 'n_cols'], use_cuda_graph=True)\n@triton.jit\ndef rms_kernel(output_ptr, input_ptr, g_ptr, rsigma_ptr, input_row_stride, output_row_stride, n_rows, n_cols, epsilon,\n               ZERO_CENTERED_GAMMA: tl.constexpr, BLOCK_SIZE: tl.constexpr, USE_BLOCKED: tl.constexpr,\n               NUM_PRGMS: tl.constexpr):\n    row_start = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    # as older version Triton doesn't support tl.assume and BUFF OPS, comment out for now\n    # tl.assume(input_row_stride >= 0)\n    # tl.assume(output_row_stride >= 0)\n    # tl.assume(row_start >= 0)\n\n    if USE_BLOCKED:\n\n        # Persistent loop for rows\n        for row_idx in tl.range(row_start, n_rows, NUM_PRGMS, num_stages=1):\n            row_input_ptr = input_ptr + row_idx * input_row_stride\n            row_output_ptr = output_ptr + row_idx * output_row_stride\n\n            # Accumulate sum of squares\n            n_cols_blks = tl.cdiv(n_cols, BLOCK_SIZE) - 1\n            # older version of triton doesn't accept below init\n            # sum_squares: tl.float32 = 0.\n            # however, with type promoting rule in triton, sum_squares should be always fp32 with below init\n            sum_squares = 0.\n            for blk_idx in tl.range(0, n_cols_blks, num_stages=2):\n                cols = blk_idx * BLOCK_SIZE + col_offsets\n                input_ptrs = row_input_ptr + cols\n                input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n                x = tl.load(input_ptrs).to(tl.float32)\n                sum_squares += tl.sum(x * x, axis=0)\n\n            # Handle remainder\n            cols = n_cols_blks * BLOCK_SIZE + col_offsets\n            mask = cols < n_cols\n            input_ptrs = row_input_ptr + cols\n            input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n            x = tl.load(input_ptrs, mask=mask, other=0.0, cache_modifier=\".cg\").to(tl.float32)\n            sum_squares += tl.sum(x * x, axis=0)\n\n            # Compute normalization factor\n            mean_square = sum_squares / n_cols\n            norm_factor = tl.rsqrt(mean_square + epsilon)\n\n            # Store rsigma (norm_factor)\n            tl.store(rsigma_ptr + row_idx, norm_factor)\n\n            # Normalize and write output\n            for blk_idx in tl.range(0, n_cols_blks, num_stages=2):\n                cols = blk_idx * BLOCK_SIZE + col_offsets\n                input_ptrs = row_input_ptr + cols\n                input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n                x = tl.load(input_ptrs).to(tl.float32)\n                g_ptrs = g_ptr + cols\n                g = tl.load(g_ptrs).to(tl.float32)\n                if (ZERO_CENTERED_GAMMA):\n                    g += 1\n                rms_norm = x * norm_factor * g\n                output_ptrs = row_output_ptr + cols\n                tl.store(output_ptrs, rms_norm.to(output_ptr.type.element_ty))\n\n            # Handle remainder\n            cols = n_cols_blks * BLOCK_SIZE + col_offsets\n            mask = cols < n_cols\n            input_ptrs = row_input_ptr + cols\n            x = tl.load(input_ptrs, mask=mask, other=0.0, cache_modifier=\".cg\").to(tl.float32)\n            g_ptrs = g_ptr + cols\n            g = tl.load(g_ptrs, mask=mask, other=0.0).to(tl.float32)\n            if (ZERO_CENTERED_GAMMA):\n                g += 1\n            rms_norm = x * norm_factor * g\n            output_ptrs = row_output_ptr + cols\n            tl.store(output_ptrs, rms_norm.to(output_ptr.type.element_ty), mask=mask)\n\n    else:\n        mask = col_offsets < n_cols\n        for row_idx in tl.range(row_start, n_rows, NUM_PRGMS, num_stages=2):\n            input_ptrs = input_ptr + row_idx * input_row_stride + col_offsets\n            input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n            row = tl.load(input_ptrs, mask=mask, other=0.0, cache_modifier=\".cg\").to(tl.float32)\n            g = tl.load(g_ptr + col_offsets, mask=mask, other=0.0).to(tl.float32)\n            row_norm = row * row\n            row_norm = tl.sum(row_norm, axis=-1)\n            norm_factor = tl.math.rsqrt((row_norm / n_cols) + epsilon)\n\n            # Store rsigma (norm_factor)\n            rsigma_output_ptr = rsigma_ptr + row_idx\n            tl.store(rsigma_output_ptr, norm_factor)\n\n            if (ZERO_CENTERED_GAMMA):\n                g += 1\n            rms_norm = row * norm_factor * g\n\n            output_ptrs = output_ptr + row_idx * output_row_stride + col_offsets\n            output_ptrs = tl.multiple_of(output_ptrs, (16, ))\n            tl.store(output_ptrs, rms_norm.to(output_ptr.type.element_ty), mask=mask)\n\n\n\n##################################################################################################################################################\n\n  \nimport argparse\nimport torch\nimport sys\nimport pytest\nfrom itertools import product\n\nimport triton\nimport triton.language as tl\n\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n######################################## HELPERS for Eval ######################################## \nimport numpy as np\nimport random\nimport torch \nimport os\n\nresult_gold = {}\nCONFIG = {\n  \"llama3\": {\n    \"8B\": {\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"vocab_size\": 128256\n    },\n    \"70B\": {\n      \"num_attention_heads\": 64,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 8192,\n      \"intermediate_size\": 28672,\n      \"vocab_size\": 128256\n    },\n    \"405B\": {\n      \"num_attention_heads\": 128,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 16384,\n      \"intermediate_size\": 53248,\n      \"vocab_size\": 128256\n    }\n  },\n  \"mistral\": {\n    \"7B\": {\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    },\n    \"22B\": {\n      \"hidden_size\": 6144,\n      \"intermediate_size\": 16384,\n      \"num_attention_heads\": 48,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    }\n\n  }\n}\n\n  \ndef get_model_configs(config_path='model_configs.json', model_families=[\"llama3\"], model=\"all\"):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_path (str): User-provided path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        dict: A dictionary of available models and their configurations for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    # Extract models and their configurations for the specified families  \n    filtered_configs = {}  \n  \n    for family in model_families:  \n        if family in configs:  \n            # Check if model filtering is required  \n            if model == \"all\":  \n                # Include all models in the family  \n                for model_size, model_configs in configs[family].items():  \n                    filtered_configs[f\"{family}-{model_size}\"] = model_configs  \n            else:  \n                # Parse the model string (e.g., llama3_8B or llama3-8B)  \n                delimiter = \"_\" if \"_\" in model else \"-\"  \n                model_parts = model.split(delimiter)  \n  \n                # Check if the family and size match  \n                if len(model_parts) == 2 and model_parts[0] == family:  \n                    model_size = model_parts[1]  \n                    if model_size in configs[family]:  \n                        filtered_configs[f\"{family}-{model_size}\"] = configs[family][model_size]  \n  \n    if not filtered_configs:  \n        print(f\"Warning: No models selected for families: {model_families} with filter: '{model}'\")  \n  \n    return filtered_configs  \n  \n  \ndef get_available_models(config_file='model_configs.json', model_families=[\"llama3\"]):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_file (str): Path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        list: A list of available models for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    models = [f\"{family}-{model}\" for family in model_families if family in configs for model in configs[family]]  \n  \n    return models  \n\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\nclass RMSNorm(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, g, y, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED,\n                NUM_PRGMS, epsilon=1e-6):\n        # heuristics for number of warps\n        #    num_warps = min(max(blk_size // 256, 1), 8)\n        num_warps = 8\n        grid = lambda meta: (NUM_PRGMS, )\n        rms_kernel[grid](y, x, g, rsigma, x.stride(0), y.stride(0), n_rows, n_cols, epsilon, ZERO_CENTERED_GAMMA,\n                         blk_size, USE_BLOCKED, NUM_PRGMS)\n\n        ctx.save_for_backward(x, g, rsigma)\n        ctx.n_rows = n_rows\n        ctx.n_cols = n_cols\n        ctx.ZERO_CENTERED_GAMMA = ZERO_CENTERED_GAMMA\n        ctx.blk_size = blk_size\n        ctx.USE_BLOCKED = USE_BLOCKED\n        ctx.NUM_PRGMS = NUM_PRGMS\n        ctx.num_warps = num_warps\n\n        ctx.dx = dx\n        ctx.dg_tmp = dg_tmp\n        ctx.dg = dg\n\n        return y\n\nrmsnorm = RMSNorm.apply\n\n\ndef torch_rmsnorm_fwd(x, g, ZERO_CENTERED_GAMMA, out_dtype=torch.float16, epsilon=1e-6):\n    M, N = x.shape\n    # cast to float32 as the triton kernel\n    x_f32 = x.float()\n    g_f32 = g.float()\n    rms = torch.sqrt(torch.sum(x_f32 * x_f32, dim=-1) * 1 / N)\n    rsigma = 1.0 / rms\n    if (ZERO_CENTERED_GAMMA):\n        g_f32 = g_f32 + 1\n    rms_norm_f32 = x_f32 * rsigma.unsqueeze(1) * g_f32\n    rms_norm = rms_norm_f32.to(out_dtype)\n    return rms_norm, rsigma\n\n\narg_to_torch_dtype = {'fp16': torch.float16, 'bf16': torch.bfloat16, 'fp32': torch.float32}\n\n\n@pytest.mark.parametrize(\"in_dtype_str\", [\"fp16\", \"bf16\"])\n@pytest.mark.parametrize(\"out_dtype_str\", [\"fp16\", \"bf16\"])\n@pytest.mark.parametrize('ZERO_CENTERED_GAMMA', [True, False])\n@pytest.mark.parametrize('M, N', [\n    (1, 4),\n    (2, 10),\n    (256, 4096),\n    (4096, 8192),\n    (1, 31744),\n    (8192, 65536),\n    (873, 1245),\n])\ndef test_rmsnorm(M, N, ZERO_CENTERED_GAMMA, in_dtype_str, out_dtype_str, request):\n    in_dtype = arg_to_torch_dtype[in_dtype_str]\n    out_dtype = arg_to_torch_dtype[out_dtype_str]\n    set_seed()\n\n    x = torch.randn(M, N, device='cuda', dtype=in_dtype, requires_grad=True)\n    g = torch.ones((1, N), device='cuda', dtype=in_dtype, requires_grad=True)\n    y = torch.zeros_like(x, device='cuda', dtype=out_dtype)\n    rsigma = torch.empty((M, ), device=x.device, dtype=torch.float32)\n\n    dx = torch.empty_like(x, dtype=in_dtype, requires_grad=False)\n    dg = torch.empty_like(g, dtype=in_dtype, requires_grad=False)\n    dg_tmp = torch.zeros(M, N, device='cuda', dtype=torch.float32, requires_grad=False)\n\n    n_rows, n_cols = x.shape\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))\n    USE_BLOCKED = n_cols > blk_size\n    NUM_PRGMS = min(n_rows, get_num_sms())\n\n    y_triton = rmsnorm(x, g, y, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED,\n                       NUM_PRGMS)\n\n    y_torch, rsigma_torch = torch_rmsnorm_fwd(x, g, ZERO_CENTERED_GAMMA, out_dtype)\n\n    if out_dtype in (torch.float16, torch.bfloat16):\n        atol, rtol = 1e-3, 1e-2\n    else:\n        # float32 typically can be tighter\n        atol, rtol = 1e-5, 1e-5\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    assert y_triton.dtype == out_dtype, f\"y_triton has dtype={y_triton.dtype}, expected {out_dtype}\"\n    assert y_torch.dtype == out_dtype, f\"y_torch has dtype={y_torch.dtype}, expected {out_dtype}\"\n\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\") + \"_y_triton\"\n    result_gold[sanitized_key_name] = y_triton.clone().detach().cpu()\n\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\") + \"_rsigma\"\n    result_gold[sanitized_key_name] = rsigma.clone().detach().cpu()\n    ################################################################### \n\n    \n    assert torch.allclose(y_triton, y_torch, atol=atol, rtol=rtol), \\\n        f\"Mismatch in 'y' (in={in_dtype_str}, out={out_dtype_str})\"\n    assert torch.allclose(rsigma, rsigma_torch, atol=atol, rtol=rtol), \\\n        f\"Mismatch in 'rsigma' (in={in_dtype_str}, out={out_dtype_str})\"\n\n# --- Define TFLOPS and GB/s calculators for RMSNorm Forward (Same as before) ---\ndef calculate_rmsnorm_fwd_gbps(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    dtype_str = params.get('dtype_str', 'fp16')\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    bytes_read_x = M * N * element_size\n    bytes_read_g = N * element_size # g is (N) or (1,N)\n    bytes_write_y = M * N * element_size\n    bytes_write_rsigma = M * 4\n    total_bytes = bytes_read_x + bytes_read_g + bytes_write_y + bytes_write_rsigma\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\ndef calculate_rmsnorm_fwd_tflops(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    flops_per_row = 4 * N + 5\n    if params.get('ZERO_CENTERED_GAMMA', False): flops_per_row += N\n    total_flops = M * flops_per_row\n    tflops = total_flops / (ms / 1000) / 1e12\n    return tflops\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"rmsnorm_fwd_triton_perf\" # Distinct name\n\n\n@pytest.mark.parametrize(\"in_dtype_str\", [\"fp16\", \"bf16\", \"fp32\"]) # Added fp32 back for perf\n@pytest.mark.parametrize(\"out_dtype_str\", [\"fp16\", \"bf16\", \"fp32\"])\n@pytest.mark.parametrize('ZERO_CENTERED_GAMMA', [True, False])\n@pytest.mark.parametrize('M, N', [\n    (256, 4096), (2048, 2048), (4096, 8192), (8192, 8192),\n    (1, 31744), (1, 131072), (512, 10240),\n    # (1, 4), (873, 1245), # Smaller/odd shapes can be added back if desired\n])\ndef test_performance(M, N, ZERO_CENTERED_GAMMA, in_dtype_str, out_dtype_str, request):\n    # Ensure in_dtype and out_dtype are compatible for RMSNorm (usually they are the same for x and y)\n    # For benchmarking, let's assume in_dtype is the primary type for x and g, and y.\n    if in_dtype_str != out_dtype_str:\n         pytest.skip(f\"Skipping perf test where in_dtype {in_dtype_str} != out_dtype {out_dtype_str} for simplicity.\")\n\n    current_dtype = arg_to_torch_dtype[in_dtype_str]\n    eps = 1e-5 # Standard epsilon\n    set_seed()\n\n    # Prepare inputs and output buffers\n    x = torch.randn(M, N, device='cuda', dtype=current_dtype)\n    g = torch.rand(N, device='cuda', dtype=current_dtype) # Kernel expects g_ptr + col_offsets, so 1D is fine.\n    y_buffer = torch.empty_like(x) # Output buffer for forward\n    rsigma_buffer = torch.empty(M, device='cuda', dtype=torch.float32)\n\n    # Dummy buffers for backward context (RMSNorm.apply signature expects them)\n    # These are not used by the forward pass logic being benchmarked.\n    dx_dummy = torch.empty(0, device='cuda') # Can be empty if not used\n    dg_dummy = torch.empty(0, device='cuda')\n    dg_tmp_dummy = torch.empty(0, device='cuda')\n\n    # Kernel launch parameters (from original test_rmsnorm)\n    n_rows, n_cols = x.shape\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    blk_size_fwd = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols if n_cols > 0 else 1))\n    if blk_size_fwd == 0: blk_size_fwd = 1\n    \n    USE_BLOCKED_fwd = n_cols > blk_size_fwd\n    num_sms_val = get_num_sms()\n    NUM_PRGMS_fwd = min(n_rows, num_sms_val) if n_rows > 0 and num_sms_val > 0 else 1\n\n    # --- Create op_lambda for benchmarking the forward pass ---\n    op_lambda = lambda: rmsnorm(\n        x, g, y_buffer, rsigma_buffer,\n        dx_dummy, dg_dummy, dg_tmp_dummy, # Pass dummies\n        n_rows, n_cols, ZERO_CENTERED_GAMMA,\n        blk_size_fwd, USE_BLOCKED_fwd, NUM_PRGMS_fwd, eps\n    )\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"ZERO_CENTERED_GAMMA\": ZERO_CENTERED_GAMMA,\n        \"dtype_str\": in_dtype_str, # Use in_dtype_str as the primary dtype for logging\n        \"eps\": eps,\n        \"blk_size_fwd\": blk_size_fwd, \"USE_BLOCKED_fwd\": USE_BLOCKED_fwd, \"NUM_PRGMS_fwd\": NUM_PRGMS_fwd\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_rmsnorm_fwd_gbps,\n                              tflops_calculator=calculate_rmsnorm_fwd_tflops)\n    \n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n######################################## HELPERS for Eval ########################################\n\n\n\n#Benchmark\ndef model_benchmark_configs(args):\n    config_file = args.model_configs\n    configs = get_model_configs(config_path=config_file, model_families=[\"llama3\"], model=args.model)\n\n    x_vals_list = []\n    batch_size = args.b if args.b else 1\n\n    for model_name, config in configs.items():\n        seq_len = args.sq if args.sq else 4096\n        x_vals_list.append((model_name, batch_size * seq_len, config[\"hidden_size\"]))\n\n    return x_vals_list\n\n\ndef run_benchmark(args):\n    config = []\n    if (args.M_benchmark):\n        val = args.M_start\n        x_vals_list = []\n        while val <= args.M_end:\n            x_vals_list.append(val)\n            val *= args.M_step\n        mn_args = {'N': args.N_start}\n        plot_name = str(\"rmsnorm-performance_\" + args.dtype + \"_N\" + str(args.N_start) + \"_M\" + str(args.M_start) +\n                        \"-\" + str(args.M_end) + \"-\" + str(args.M_step))\n        x_names = ['M']\n    else:\n        x_vals_list = [i for i in range(args.N_start, args.N_end, args.N_step)]\n        mn_args = {'M': args.M_start}\n        x_names = ['N']\n        plot_name = str(\"rmsnorm-performance_\" + args.dtype + \"_M\" + str(args.M_start) + \"_N\" + str(args.N_start) +\n                        \"-\" + str(args.N_end) + \"-\" + str(args.N_step))\n\n    if args.model:\n        assert not args.M_benchmark, \\\n            \"Trying to provide both -model benchmark and M_benchmark is not supported!\"\n        x_names = ['model', 'M', 'N']\n        mn_args = {}\n        plot_name = str(\"rmsnorm-performance_\" + args.dtype)\n        x_vals_list = model_benchmark_configs(args)\n\n    dtype = arg_to_torch_dtype[args.dtype]\n\n    print(plot_name)\n    config.append(\n        triton.testing.Benchmark(\n            x_names=x_names,\n            x_vals=x_vals_list,\n            line_arg='provider',\n            line_vals=['triton', 'torch'],\n            line_names=[\"Triton\", \"Torch\"],\n            styles=[('blue', '-'), ('green', '-')],\n            ylabel=\"GB/s\",\n            plot_name=plot_name,\n            args=mn_args,\n        ))\n\n    @triton.testing.perf_report(config)\n    def benchmark(M, N, provider, model=None):\n        mode = args.mode\n\n        x = torch.randn(M, N, device='cuda', dtype=dtype)\n        y = torch.zeros_like(x, device='cuda')\n        rsigma = torch.empty((M, ), device='cuda', dtype=torch.float32)\n        dx = torch.empty(M, N, device='cuda', dtype=dtype, requires_grad=False)\n        dg = torch.empty((1, N), device='cuda', dtype=dtype, requires_grad=False)\n        dg_tmp = torch.zeros(M, N, device='cuda', dtype=torch.float32, requires_grad=False)\n        n_rows, n_cols = x.shape\n        #        MAX_FUSED_SIZE = 65536 // x.element_size()\n        #        blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))\n        blk_size = 1024\n        USE_BLOCKED = n_cols > blk_size\n        NUM_PRGMS = min(n_rows, get_num_sms())\n        stream = torch.cuda.Stream()\n        torch.cuda.set_stream(stream)\n        g = torch.ones((1, N), device='cuda')\n        ZERO_CENTERED_GAMMA = False\n\n        def rms_fwd():\n            if provider == 'triton':\n                return rmsnorm(x, g, y, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size,\n                               USE_BLOCKED, NUM_PRGMS)\n            if provider == 'torch':\n                return torch_rmsnorm_fwd(x, g, ZERO_CENTERED_GAMMA)\n\n        if mode == 'fwd':\n            ms = triton.testing.do_bench(rms_fwd)\n        else:\n            raise ValueError(f\"mode {mode} is not supported!\")\n\n        global verbose\n        if verbose:\n            print(f'SIZE: {N} Best tuning config: ({rms_kernel.best_config})')\n            print(f'time: {ms}')\n        gbps = lambda ms_val: 2 * x.nelement() * x.element_size() * 1e-9 / (ms_val * 1e-3)\n        return gbps(ms)\n\n    benchmark.run(save_path=\".\", show_plots=True, print_data=True)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        prog=\"Benchmark RMSNorm\",\n        allow_abbrev=False,\n    )\n    parser.add_argument('-model_configs', type=str, default=\"model_configs.json\", help=\"Model config json file.\")\n\n    available_models = get_available_models(model_families=[\"llama3\"])  # Dynamically load model names\n    model_help = (\n        \"Model name to benchmark. Select from: [\" + \", \".join(available_models) +\n        \"]. Use 'all' to benchmark all models. Not providing runs the default benchmark script with custom configs.\")\n    parser.add_argument('-model', type=str, default=None, help=model_help)\n    parser.add_argument('-b', type=int, default=0, help=\"Batch size used together with model.\")\n    parser.add_argument('-sq', type=int, default=0, help=\"Sequence length used together with model.\")\n    parser.add_argument('-M', \"--M_start\", default=\"1\", type=int)\n    parser.add_argument('-Ms', \"--M_step\", default=\"2\", type=int)  #This is multiplicative step\n    parser.add_argument('-Me', \"--M_end\", default=\"512\", type=int)\n    parser.add_argument('-Mb', \"--M_benchmark\", default=False, type=bool)\n\n    parser.add_argument('-N', \"--N_start\", default=\"8192\", type=int)\n    parser.add_argument('-Ns', \"--N_step\", default=\"1024\", type=int)\n    parser.add_argument('-Ne', \"--N_end\", default=\"32768\", type=int)\n\n    parser.add_argument('-d', \"--dtype\", default=\"fp16\")\n    parser.add_argument('-nb', \"--no_benchmark\", default=False, type=bool)\n    parser.add_argument(\"-v\", action='store_true', default=False, help=\"Print out the best tuning config\")\n    parser.add_argument(\"--mode\", type=str, choices=[\"fwd\", \"bwd\"], default=\"fwd\",\n                        help=\"Benchmark mode: forward only, backward only, or both.\")\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    global verbose\n    if args.no_benchmark:\n        x = torch.randn(args.M_start, args.N_start, device='cuda', dtype=args.dtype)\n        y = torch.zeros_like(x, device='cuda')\n        rsigma = torch.empty((args.M_start, ), device='cuda', dtype=torch.float32)\n        dx = torch.empty(args.M_start, args.N_start, device='cuda', dtype=args.dtype, requires_grad=False)\n        dg = torch.empty((1, args.N_start), device='cuda', dtype=args.dtype, requires_grad=False)\n        dg_tmp = torch.zeros(args.M_start, args.N_start, device='cuda', dtype=torch.float32, requires_grad=False)\n        n_rows, n_cols = x.shape\n        MAX_FUSED_SIZE = 65536 // x.element_size()\n        blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))\n        USE_BLOCKED = n_cols > blk_size\n        NUM_PRGMS = min(n_rows, get_num_sms())\n        g = torch.ones((1, args.N_start), device='cuda', dtype=args.dtype)\n        ZERO_CENTERED_GAMMA = True\n        rmsnorm(x, y, g, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED, NUM_PRGMS)\n    else:\n        verbose = args.v\n        run_benchmark(args)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"
    },
    {
        "file": "rmsnorm_bwd.py",
        "target_kernel_name": "rms_bwd_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `rms_bwd_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `rms_bwd_kernel`,  is designed to calculate the backward pass for RMS Normalization\n\n**Your objective is to implement the body of `rms_bwd_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `rms_bwd_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `rms_bwd_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `rms_bwd_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\nOkay, here's the setup for implementing rms_bwd_kernel, along with descriptions of the related kernels.\n\nProvided Kernels (Assumed to be already implemented):\n\nrms_fwd_kernel:\n\nPurpose: This kernel performs the forward pass of RMS Normalization.\n\nGiven an input tensor x and a learnable scaling vector g, it computes the normalized output y and the reciprocal of the root mean square rsigma for each row.\n\nThe formula is roughly y_i = (x_i / sqrt(mean(x^2) + epsilon)) * effective_g_i, where effective_g_i is g_i or g_i + 1 depending on ZERO_CENTERED_GAMMA. rsigma is 1.0 / sqrt(mean(x^2) + epsilon).\n\n_rmsnorm_bwd_dg_reduce:\n\nPurpose: This kernel performs a reduction operation specifically for the gradient of the scaling parameter g.\n\nIt takes an intermediate gradient dg_tmp (which has dimensions n_rows x n_cols and is computed by rms_bwd_kernel) and sums it along the row dimension.\n\nThe result is the final gradient dg (with dimensions n_cols) for the learnable scaling parameter g.\n\nKernel to Implement: rms_bwd_kernel (function definition below)\n\n####################### Imports #####################\nimport argparse\nimport torch\nimport sys\nimport pytest\nfrom itertools import product\n\nimport triton\nimport triton.language as tl\n####################### Imports #####################\n\n@triton.jit\ndef rms_bwd_kernel(\n    grad_output_ptr,    # Pointer to the gradient of the loss w.r.t. the output of RMSNorm (dL/dy).\n                        # Shape: (n_rows, n_cols)\n    input_ptr,          # Pointer to the input tensor 'x' from the forward pass.\n                        # Shape: (n_rows, n_cols)\n    g_ptr,              # Pointer to the learnable scaling parameter 'g'.\n                        # Shape: (n_cols,)\n    rsigma_ptr,         # Pointer to the reciprocal of the root mean square (1 / sqrt(mean(x^2) + eps))\n                        # computed in the forward pass. Shape: (n_rows,)\n    dx_ptr,             # Pointer to store the computed gradient of the loss w.r.t. the input 'x' (dL/dx).\n                        # Shape: (n_rows, n_cols)\n    dg_ptr,             # Pointer to store the computed intermediate gradient of the loss w.r.t. the\n                        # scaling parameter 'g' (dL/dg_tmp). This is before reduction across rows.\n                        # Shape: (n_rows, n_cols)\n    input_row_stride,   # Stride of the 'input_ptr' and 'dx_ptr' tensors along the row dimension.\n    output_row_stride,  # Stride of the 'grad_output_ptr' tensor along the row dimension.\n                        # Note: 'dg_ptr' also uses 'input_row_stride' if it has the same layout as 'x'.\n    n_rows,             # Total number of rows to process (e.g., batch_size * sequence_length).\n    n_cols,             # Total number of columns (features) per row (e.g., hidden_dimension).\n    ZERO_CENTERED_GAMMA: tl.constexpr, # Compile-time boolean. If True, effective gamma is (g + 1), otherwise it's 'g'.\n    BLOCK_SIZE: tl.constexpr,          # Compile-time constant. Defines the size of blocks used for processing columns.\n                                       # This is typically a power of 2, e.g., 1024.\n    USE_BLOCKED: tl.constexpr,         # Compile-time boolean. If True, indicates a specialized blocked algorithm\n                                       # should be used for computing sums over columns, potentially involving\n                                       # multiple passes. This is often beneficial for large 'n_cols'.\n    NUM_PRGMS: tl.constexpr            # Compile-time constant. The number of program instances launched by Triton.\n                                       # Used for distributing row processing across different programs.\n):\n    \"\"\"\n    Computes the backward pass for RMS Normalization, calculating the gradients\n    with respect to the input 'x' (dL/dx) and an intermediate gradient\n    with respect to the scaling parameter 'g' (dL/dg_tmp).\n\n    The core computations for dL/dx_i (grad_input) and dL/dg_i (dg) are:\n    Let norm_factor = rsigma\n    Let effective_g = g (or g + 1 if ZERO_CENTERED_GAMMA)\n\n    1. grad_sum_per_row = sum_cols(grad_output * input * effective_g)\n    2. dL/dx = grad_output * norm_factor * effective_g - (norm_factor^3 * input / n_cols) * grad_sum_per_row\n    3. dL/dg_intermediate = grad_output * input * norm_factor\n\n    This kernel handles parallelization over rows and, depending on USE_BLOCKED,\n    may use a blocked approach for iterating over columns to manage memory and\n    computation efficiently, especially for the `grad_sum_per_row` calculation.\n\n    The `dg_ptr` output of this kernel (dL/dg_intermediate) will typically be\n    further processed by `_rmsnorm_bwd_dg_reduce` to sum contributions across\n    all rows to get the final dL/dg.\n    \"\"\"\n    # Your code here\n\n",
        "label": "####################### Imports #####################\nimport argparse\nimport torch\nimport sys\nimport pytest\nfrom itertools import product\n\nimport triton\nimport triton.language as tl\n####################### Imports #####################\n\n\n\n@triton.jit\ndef rms_bwd_kernel(grad_output_ptr, input_ptr, g_ptr, rsigma_ptr, dx_ptr, dg_ptr, input_row_stride, output_row_stride,\n                   n_rows, n_cols, ZERO_CENTERED_GAMMA: tl.constexpr, BLOCK_SIZE: tl.constexpr,\n                   USE_BLOCKED: tl.constexpr, NUM_PRGMS: tl.constexpr):\n    row_start = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    #   tl.assume(input_row_stride >= 0)\n    #   tl.assume(output_row_stride >= 0)\n    #   tl.assume(row_start >= 0)\n\n    if USE_BLOCKED:\n        for row_idx in tl.range(row_start, n_rows, NUM_PRGMS, num_stages=1):\n            row_input_ptr = input_ptr + row_idx * input_row_stride\n            row_grad_output_ptr = grad_output_ptr + row_idx * output_row_stride\n            row_dx_ptr = dx_ptr + row_idx * input_row_stride\n            row_dg_ptr = dg_ptr + row_idx * input_row_stride\n\n            # Compute gradients sum of all colums for each row\n            n_cols_blks = tl.cdiv(n_cols, BLOCK_SIZE) - 1\n            # older version of triton doesn't accept below init\n            # comment out for now to make it compatible with triton 3.1\n            # grad_sum: tl.float32 = 0.0\n            grad_sum = 0.0\n            for blk_idx in tl.range(0, n_cols_blks, num_stages=2):\n                cols = blk_idx * BLOCK_SIZE + col_offsets\n                input_ptrs = row_input_ptr + cols\n                grad_output_ptrs = row_grad_output_ptr + cols\n\n                input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n                grad_output_ptrs = tl.multiple_of(grad_output_ptrs, (16, ))\n\n                x = tl.load(input_ptrs).to(tl.float32)\n                grad_output = tl.load(grad_output_ptrs).to(tl.float32)\n                g_ptrs = g_ptr + cols\n                g = tl.load(g_ptrs).to(tl.float32)\n                if (ZERO_CENTERED_GAMMA):\n                    g += 1.\n                grad_sum += tl.sum(grad_output * x * g, axis=0)\n\n            # remainder for grad_sum:\n            cols = n_cols_blks * BLOCK_SIZE + col_offsets\n            mask = cols < n_cols\n            input_ptrs = row_input_ptr + cols\n            x = tl.load(input_ptrs, mask=mask, other=0.0).to(tl.float32)\n            grad_output_ptrs = row_grad_output_ptr + cols\n            grad_output = tl.load(grad_output_ptrs, mask=mask, other=0.0).to(tl.float32)\n            g_ptrs = g_ptr + cols\n            g = tl.load(g_ptrs, mask=mask, other=0.0).to(tl.float32)\n            if (ZERO_CENTERED_GAMMA):\n                g += 1.\n            grad_sum += tl.sum(grad_output * x * g, axis=0)\n\n            # Load r_sigma\n            norm_factor = tl.load(rsigma_ptr + row_idx).to(tl.float32)\n\n            for blk_idx in tl.range(0, n_cols_blks, num_stages=2):\n                cols = blk_idx * BLOCK_SIZE + col_offsets\n                input_ptrs = row_input_ptr + cols\n                grad_output_ptrs = row_grad_output_ptr + cols\n\n                input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n                grad_output_ptrs = tl.multiple_of(grad_output_ptrs, (16, ))\n\n                x = tl.load(input_ptrs).to(tl.float32)\n                grad_output = tl.load(grad_output_ptrs).to(tl.float32)\n\n                g_ptrs = g_ptr + cols\n                g = tl.load(g_ptrs).to(tl.float32)\n                if (ZERO_CENTERED_GAMMA):\n                    g += 1.\n                grad_input = grad_output * norm_factor * g - (norm_factor * norm_factor * norm_factor) * x * (grad_sum /\n                                                                                                              n_cols)\n\n                dx_ptrs = row_dx_ptr + cols\n                tl.store(dx_ptrs, grad_input.to(dx_ptr.type.element_ty))\n\n                dg = grad_output * x * norm_factor\n                dg_ptrs = row_dg_ptr + cols\n                tl.store(dg_ptrs, dg.to(tl.float32))\n\n            # Handle remainder\n            cols = n_cols_blks * BLOCK_SIZE + col_offsets\n            mask = cols < n_cols\n\n            input_ptrs = row_input_ptr + cols\n            x = tl.load(input_ptrs, mask=mask, other=0.0).to(tl.float32)\n            grad_output_ptrs = row_grad_output_ptr + cols\n            grad_output = tl.load(grad_output_ptrs, mask=mask, other=0.0).to(tl.float32)\n            g_ptrs = g_ptr + cols\n            g = tl.load(g_ptrs, mask=mask, other=0.0).to(tl.float32)\n            if (ZERO_CENTERED_GAMMA):\n                g += 1.\n            grad_input = grad_output * norm_factor * g - (norm_factor * norm_factor * norm_factor) * x * (grad_sum /\n                                                                                                          n_cols)\n\n            dx_ptrs = row_dx_ptr + cols\n            tl.store(dx_ptrs, grad_input.to(dx_ptr.type.element_ty), mask=mask)\n\n            dg = grad_output * x * norm_factor\n            dg_ptrs = row_dg_ptr + cols\n            tl.store(dg_ptrs, dg.to(tl.float32), mask=mask)\n\n    else:\n        mask = col_offsets < n_cols\n        for row_idx in tl.range(row_start, n_rows, NUM_PRGMS, num_stages=2):\n            input_ptrs = input_ptr + row_idx * input_row_stride + col_offsets\n            grad_output_ptrs = grad_output_ptr + row_idx * output_row_stride + col_offsets\n            dx_ptrs = dx_ptr + row_idx * input_row_stride + col_offsets\n            dg_ptrs = dg_ptr + row_idx * input_row_stride + col_offsets\n\n            input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n            grad_output_ptrs = tl.multiple_of(grad_output_ptrs, (16, ))\n            dx_ptrs = tl.multiple_of(dx_ptrs, (16, ))\n\n            x = tl.load(input_ptrs, mask=mask, other=0.0).to(tl.float32)\n            grad_output = tl.load(grad_output_ptrs, mask=mask, other=0.0).to(tl.float32)\n            g = tl.load(g_ptr + col_offsets, mask=mask, other=0.0).to(tl.float32)\n            if (ZERO_CENTERED_GAMMA):\n                g += 1.\n\n            norm_factor = tl.load(rsigma_ptr + row_idx).to(tl.float32)\n            grad_sum = tl.sum(grad_output * x * g, axis=0)\n\n            grad_input = grad_output * norm_factor * g - (norm_factor * norm_factor * norm_factor) * x * (grad_sum /\n                                                                                                          n_cols)\n            tl.store(dx_ptrs, grad_input.to(dx_ptr.type.element_ty), mask=mask)\n\n            dg = grad_output * x * norm_factor\n            tl.store(dg_ptrs, dg.to(tl.float32), mask=mask)\n\n\n\n\n\n##################################################################################################################################################\n\n######################################## HELPERS for Eval ######################################## \nimport numpy as np\nimport random\nimport torch \nimport os\nimport argparse\nimport sys\nimport pytest\nfrom itertools import product\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nimport triton\nimport triton.language as tl\n  \n\nresult_gold = {}\nCONFIG = {\n  \"llama3\": {\n    \"8B\": {\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"vocab_size\": 128256\n    },\n    \"70B\": {\n      \"num_attention_heads\": 64,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 8192,\n      \"intermediate_size\": 28672,\n      \"vocab_size\": 128256\n    },\n    \"405B\": {\n      \"num_attention_heads\": 128,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 16384,\n      \"intermediate_size\": 53248,\n      \"vocab_size\": 128256\n    }\n  },\n  \"mistral\": {\n    \"7B\": {\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    },\n    \"22B\": {\n      \"hidden_size\": 6144,\n      \"intermediate_size\": 16384,\n      \"num_attention_heads\": 48,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    }\n\n  }\n}\n\n  \ndef get_model_configs(config_path='model_configs.json', model_families=[\"llama3\"], model=\"all\"):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_path (str): User-provided path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        dict: A dictionary of available models and their configurations for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    # Extract models and their configurations for the specified families  \n    filtered_configs = {}  \n  \n    for family in model_families:  \n        if family in configs:  \n            # Check if model filtering is required  \n            if model == \"all\":  \n                # Include all models in the family  \n                for model_size, model_configs in configs[family].items():  \n                    filtered_configs[f\"{family}-{model_size}\"] = model_configs  \n            else:  \n                # Parse the model string (e.g., llama3_8B or llama3-8B)  \n                delimiter = \"_\" if \"_\" in model else \"-\"  \n                model_parts = model.split(delimiter)  \n  \n                # Check if the family and size match  \n                if len(model_parts) == 2 and model_parts[0] == family:  \n                    model_size = model_parts[1]  \n                    if model_size in configs[family]:  \n                        filtered_configs[f\"{family}-{model_size}\"] = configs[family][model_size]  \n  \n    if not filtered_configs:  \n        print(f\"Warning: No models selected for families: {model_families} with filter: '{model}'\")  \n  \n    return filtered_configs  \n  \n  \ndef get_available_models(config_file='model_configs.json', model_families=[\"llama3\"]):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_file (str): Path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        list: A list of available models for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    models = [f\"{family}-{model}\" for family in model_families if family in configs for model in configs[family]]  \n  \n    return models  \n\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n############################ HELPER utils ############################\n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef is_hip():\n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n\n\ndef get_num_sms():\n    current_device_index = torch.cuda.current_device()\n    current_device = torch.cuda.get_device_properties(current_device_index)\n    num_sms = current_device.multi_processor_count\n    return num_sms\n\n\ndef get_cuda_autotune_config():\n    return [\n        triton.Config({}, num_warps=4, num_stages=1),\n        triton.Config({}, num_warps=8, num_stages=1),\n        triton.Config({}, num_warps=16, num_stages=1),\n    ]\n\n\ndef get_hip_autotune_config():\n    return [triton.Config({'waves_per_eu': we}, num_warps=nw) for (we, nw) in product([0, 1, 2, 4], [4, 8, 16])]\n\n\ndef get_autotune_config():\n    if is_cuda():\n        return get_cuda_autotune_config()\n    else:\n        return get_hip_autotune_config()\n############################ HELPER utils ############################\n@triton.autotune(configs=get_autotune_config(), key=['n_rows', 'n_cols'], use_cuda_graph=True)\n@triton.jit\ndef rms_kernel(output_ptr, input_ptr, g_ptr, rsigma_ptr, input_row_stride, output_row_stride, n_rows, n_cols, epsilon,\n               ZERO_CENTERED_GAMMA: tl.constexpr, BLOCK_SIZE: tl.constexpr, USE_BLOCKED: tl.constexpr,\n               NUM_PRGMS: tl.constexpr):\n    row_start = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    # as older version Triton doesn't support tl.assume and BUFF OPS, comment out for now\n    # tl.assume(input_row_stride >= 0)\n    # tl.assume(output_row_stride >= 0)\n    # tl.assume(row_start >= 0)\n\n    if USE_BLOCKED:\n\n        # Persistent loop for rows\n        for row_idx in tl.range(row_start, n_rows, NUM_PRGMS, num_stages=1):\n            row_input_ptr = input_ptr + row_idx * input_row_stride\n            row_output_ptr = output_ptr + row_idx * output_row_stride\n\n            # Accumulate sum of squares\n            n_cols_blks = tl.cdiv(n_cols, BLOCK_SIZE) - 1\n            # older version of triton doesn't accept below init\n            # sum_squares: tl.float32 = 0.\n            # however, with type promoting rule in triton, sum_squares should be always fp32 with below init\n            sum_squares = 0.\n            for blk_idx in tl.range(0, n_cols_blks, num_stages=2):\n                cols = blk_idx * BLOCK_SIZE + col_offsets\n                input_ptrs = row_input_ptr + cols\n                input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n                x = tl.load(input_ptrs).to(tl.float32)\n                sum_squares += tl.sum(x * x, axis=0)\n\n            # Handle remainder\n            cols = n_cols_blks * BLOCK_SIZE + col_offsets\n            mask = cols < n_cols\n            input_ptrs = row_input_ptr + cols\n            input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n            x = tl.load(input_ptrs, mask=mask, other=0.0, cache_modifier=\".cg\").to(tl.float32)\n            sum_squares += tl.sum(x * x, axis=0)\n\n            # Compute normalization factor\n            mean_square = sum_squares / n_cols\n            norm_factor = tl.rsqrt(mean_square + epsilon)\n\n            # Store rsigma (norm_factor)\n            tl.store(rsigma_ptr + row_idx, norm_factor)\n\n            # Normalize and write output\n            for blk_idx in tl.range(0, n_cols_blks, num_stages=2):\n                cols = blk_idx * BLOCK_SIZE + col_offsets\n                input_ptrs = row_input_ptr + cols\n                input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n                x = tl.load(input_ptrs).to(tl.float32)\n                g_ptrs = g_ptr + cols\n                g = tl.load(g_ptrs).to(tl.float32)\n                if (ZERO_CENTERED_GAMMA):\n                    g += 1\n                rms_norm = x * norm_factor * g\n                output_ptrs = row_output_ptr + cols\n                tl.store(output_ptrs, rms_norm.to(output_ptr.type.element_ty))\n\n            # Handle remainder\n            cols = n_cols_blks * BLOCK_SIZE + col_offsets\n            mask = cols < n_cols\n            input_ptrs = row_input_ptr + cols\n            x = tl.load(input_ptrs, mask=mask, other=0.0, cache_modifier=\".cg\").to(tl.float32)\n            g_ptrs = g_ptr + cols\n            g = tl.load(g_ptrs, mask=mask, other=0.0).to(tl.float32)\n            if (ZERO_CENTERED_GAMMA):\n                g += 1\n            rms_norm = x * norm_factor * g\n            output_ptrs = row_output_ptr + cols\n            tl.store(output_ptrs, rms_norm.to(output_ptr.type.element_ty), mask=mask)\n\n    else:\n        mask = col_offsets < n_cols\n        for row_idx in tl.range(row_start, n_rows, NUM_PRGMS, num_stages=2):\n            input_ptrs = input_ptr + row_idx * input_row_stride + col_offsets\n            input_ptrs = tl.multiple_of(input_ptrs, (16, ))\n            row = tl.load(input_ptrs, mask=mask, other=0.0, cache_modifier=\".cg\").to(tl.float32)\n            g = tl.load(g_ptr + col_offsets, mask=mask, other=0.0).to(tl.float32)\n            row_norm = row * row\n            row_norm = tl.sum(row_norm, axis=-1)\n            norm_factor = tl.math.rsqrt((row_norm / n_cols) + epsilon)\n\n            # Store rsigma (norm_factor)\n            rsigma_output_ptr = rsigma_ptr + row_idx\n            tl.store(rsigma_output_ptr, norm_factor)\n\n            if (ZERO_CENTERED_GAMMA):\n                g += 1\n            rms_norm = row * norm_factor * g\n\n            output_ptrs = output_ptr + row_idx * output_row_stride + col_offsets\n            output_ptrs = tl.multiple_of(output_ptrs, (16, ))\n            tl.store(output_ptrs, rms_norm.to(output_ptr.type.element_ty), mask=mask)\n\n\n\n@triton.jit\ndef _rmsnorm_bwd_dg_reduce(dg_in_ptr, dg_out_ptr, dg_in_stride, n_rows, n_cols, BLOCK_SIZE_M: tl.constexpr,\n                           BLOCK_SIZE_N: tl.constexpr):\n    # we want parallelism in N direction\n    # if N is small, we will just use one CU,\n    # otherwise, it can be split by N/BLOCK_SIZE\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, n_rows, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < n_rows) & (cols[None, :] < n_cols)\n        offs = rows[:, None] * n_cols + cols[None, :]\n        acc += tl.load(dg_in_ptr + offs, mask=mask, other=0.).to(tl.float32)\n\n    sum_dg = tl.sum(acc, axis=0)\n    tl.store(dg_out_ptr + cols, sum_dg.to(dg_out_ptr.type.element_ty), mask=cols < n_cols)\n\n\nclass RMSNorm(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, g, y, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED,\n                NUM_PRGMS, epsilon=1e-6):\n        # heuristics for number of warps\n        #    num_warps = min(max(blk_size // 256, 1), 8)\n        num_warps = 8\n        grid = lambda meta: (NUM_PRGMS, )\n        rms_kernel[grid](y, x, g, rsigma, x.stride(0), y.stride(0), n_rows, n_cols, epsilon, ZERO_CENTERED_GAMMA,\n                         blk_size, USE_BLOCKED, NUM_PRGMS)\n\n        ctx.save_for_backward(x, g, rsigma)\n        ctx.n_rows = n_rows\n        ctx.n_cols = n_cols\n        ctx.ZERO_CENTERED_GAMMA = ZERO_CENTERED_GAMMA\n        ctx.blk_size = blk_size\n        ctx.USE_BLOCKED = USE_BLOCKED\n        ctx.NUM_PRGMS = NUM_PRGMS\n        ctx.num_warps = num_warps\n\n        ctx.dx = dx\n        ctx.dg_tmp = dg_tmp\n        ctx.dg = dg\n\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, g, rsigma = ctx.saved_tensors\n        dg_tmp = ctx.dg_tmp\n        dx = ctx.dx\n        dg = ctx.dg\n        n_rows = ctx.n_rows\n        n_cols = ctx.n_cols\n        ZERO_CENTERED_GAMMA = ctx.ZERO_CENTERED_GAMMA\n        blk_size = ctx.blk_size\n        USE_BLOCKED = ctx.USE_BLOCKED\n        NUM_PRGMS = ctx.NUM_PRGMS\n\n        grid_bwd = lambda meta: (NUM_PRGMS, )\n        rms_bwd_kernel[grid_bwd](grad_output, x, g, rsigma, dx, dg_tmp, x.stride(0), grad_output.stride(0), n_rows,\n                                 n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED, NUM_PRGMS, num_warps=ctx.num_warps)\n\n        #        grid_reduce = lambda meta: (triton.cdiv(n_cols, blk_size), )\n        grid_reduce = lambda meta: [triton.cdiv(n_cols, meta['BLOCK_SIZE_N'])]\n        _rmsnorm_bwd_dg_reduce[grid_reduce](dg_tmp, dg, dg_tmp.stride(0), n_rows, n_cols, BLOCK_SIZE_M=128,\n                                            BLOCK_SIZE_N=64)\n\n        return dx, dg, None, None, None, None, None, None, None, None, None, None, None\n\n\nrmsnorm = RMSNorm.apply\n\n\ndef torch_rmsnorm_fwd(x, g, ZERO_CENTERED_GAMMA, out_dtype=torch.float16, epsilon=1e-6):\n    M, N = x.shape\n    # cast to float32 as the triton kernel\n    x_f32 = x.float()\n    g_f32 = g.float()\n    rms = torch.sqrt(torch.sum(x_f32 * x_f32, dim=-1) * 1 / N)\n    rsigma = 1.0 / rms\n    if (ZERO_CENTERED_GAMMA):\n        g_f32 = g_f32 + 1\n    rms_norm_f32 = x_f32 * rsigma.unsqueeze(1) * g_f32\n    rms_norm = rms_norm_f32.to(out_dtype)\n    return rms_norm, rsigma\n\n\narg_to_torch_dtype = {'fp16': torch.float16, 'bf16': torch.bfloat16, 'fp32': torch.float32}\n\n\n    # (8192, 65536),\n\n#@pytest.mark.parametrize(\"in_dtype_str\", [\"fp32\", \"fp16\", \"bf16\"])\n#@pytest.mark.parametrize(\"out_dtype_str\", [\"fp32\", \"fp16\", \"bf16\"])\n@pytest.mark.parametrize(\"in_dtype_str\", [\"fp16\", \"bf16\"])\n@pytest.mark.parametrize(\"out_dtype_str\", [\"fp16\", \"bf16\"])\n@pytest.mark.parametrize('ZERO_CENTERED_GAMMA', [True, False])\n@pytest.mark.parametrize('M, N', [\n    (1, 4),\n    (2, 10),\n    (256, 4096),\n    (1, 31744),\n    (873, 1245),\n])\ndef test_rmsnorm(M, N, ZERO_CENTERED_GAMMA, in_dtype_str, out_dtype_str, request):\n    in_dtype = arg_to_torch_dtype[in_dtype_str]\n    out_dtype = arg_to_torch_dtype[out_dtype_str]\n    set_seed()\n\n    x = torch.randn(M, N, device='cuda', dtype=in_dtype, requires_grad=True)\n    g = torch.ones((1, N), device='cuda', dtype=in_dtype, requires_grad=True)\n    y = torch.zeros_like(x, device='cuda', dtype=out_dtype)\n    rsigma = torch.empty((M, ), device=x.device, dtype=torch.float32)\n\n    dx = torch.empty_like(x, dtype=in_dtype, requires_grad=False)\n    dg = torch.empty_like(g, dtype=in_dtype, requires_grad=False)\n    dg_tmp = torch.zeros(M, N, device='cuda', dtype=torch.float32, requires_grad=False)\n\n    n_rows, n_cols = x.shape\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))\n    USE_BLOCKED = n_cols > blk_size\n    NUM_PRGMS = min(n_rows, get_num_sms())\n\n    y_triton = rmsnorm(x, g, y, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED,\n                       NUM_PRGMS)\n\n    y_torch, rsigma_torch = torch_rmsnorm_fwd(x, g, ZERO_CENTERED_GAMMA, out_dtype)\n\n    if out_dtype in (torch.float16, torch.bfloat16):\n        atol, rtol = 1e-3, 1e-2\n    else:\n        # float32 typically can be tighter\n        atol, rtol = 1e-5, 1e-5\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    assert y_triton.dtype == out_dtype, f\"y_triton has dtype={y_triton.dtype}, expected {out_dtype}\"\n    assert y_torch.dtype == out_dtype, f\"y_torch has dtype={y_torch.dtype}, expected {out_dtype}\"\n\n    assert torch.allclose(y_triton, y_torch, atol=atol, rtol=rtol), \\\n        f\"Mismatch in 'y' (in={in_dtype_str}, out={out_dtype_str})\"\n    assert torch.allclose(rsigma, rsigma_torch, atol=atol, rtol=rtol), \\\n        f\"Mismatch in 'rsigma' (in={in_dtype_str}, out={out_dtype_str})\"\n\n    grad_output = torch.randn_like(y_torch)\n\n    # 1) PyTorch reference backward\n    # We must clone and set requires_grad = True for backward\n    x_ref = x.clone().detach().requires_grad_()\n    g_ref = g.clone().detach().requires_grad_()\n    y_ref, rsigma_ref = torch_rmsnorm_fwd(x_ref, g_ref, ZERO_CENTERED_GAMMA, out_dtype)\n\n    # Backpropagate through PyTorch\n    y_ref.backward(grad_output)\n    grad_x_ref = x_ref.grad.to(out_dtype)\n    grad_g_ref = g_ref.grad.to(out_dtype)\n\n    # 2) Triton backward\n    x_triton = x.clone().detach().requires_grad_()\n    g_triton = g.clone().detach().requires_grad_()\n\n    y_triton_buf = torch.empty_like(x_triton, dtype=out_dtype)\n    rsigma_triton = torch.empty((M, ), device=x_triton.device, dtype=torch.float32)\n\n    dx_b = torch.empty_like(x_triton, dtype=in_dtype, requires_grad=False)\n    dg_b = torch.empty_like(g_triton, dtype=in_dtype, requires_grad=False)\n    dg_tmp_b = torch.zeros(M, N, device=x_triton.device, dtype=torch.float32, requires_grad=False)\n\n    # Run Triton forward pass to build the graph for backward.\n    y_triton = rmsnorm(x_triton, g_triton, y_triton_buf, rsigma_triton, dx_b, dg_b, dg_tmp_b, n_rows, n_cols,\n                       ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED, NUM_PRGMS)\n    y_triton.backward(grad_output, retain_graph=True)\n    grad_x_triton = x_triton.grad.to(out_dtype)\n    grad_g_triton = g_triton.grad.to(out_dtype)\n\n    # Compare backward outputs (grad_x and grad_g)\n    err_x = (grad_x_triton - grad_x_ref).abs().max().item()\n\n\n    ################### save grad_x_triton, grad_g_triton in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\") + \"_grad_x_triton\"\n    result_gold[sanitized_key_name] = grad_x_triton.clone().detach().cpu()\n\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\") + \"_grad_g_triton\"\n    result_gold[sanitized_key_name] = grad_g_triton.clone().detach().cpu()\n    ################################################################### \n\n\n    assert torch.allclose(grad_x_triton, grad_x_ref, atol=atol, rtol=rtol), \\\n    f\"Triton dx mismatch (max error: {err_x:.4e})\\n\\n\"\n    f\"Triton grad x:\\n{grad_x_triton}\\n\\nPyTorch grad_x:\\n{grad_x_ref}\"\n\n    err_g = (grad_g_triton - grad_g_ref).abs().max().item()\n    assert torch.allclose(grad_g_triton, grad_g_ref, atol=atol, rtol=rtol), \\\n    f\"Triton dg mismatch (max error: {err_g:.4e})\\n\\n\"\n    f\"Triton grad g:\\n{grad_g_triton}\\n\\nPyTorch grad_g:\\n{grad_g_ref}\"\n\n\n# --- Define TFLOPS and GB/s calculators for RMSNorm Forward ---\ndef calculate_rmsnorm_fwd_gbps(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    dtype_str = params.get('dtype_str', 'fp16')\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    \n    # Read x (M,N), g (N)\n    # Write y (M,N), rsigma (M)\n    bytes_read_x = M * N * element_size\n    bytes_read_g = N * element_size\n    bytes_write_y = M * N * element_size\n    bytes_write_rsigma = M * 4 # rsigma is usually float32\n\n    total_bytes = bytes_read_x + bytes_read_g + bytes_write_y + bytes_write_rsigma\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\ndef calculate_rmsnorm_fwd_tflops(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    # FLOPs for RMSNorm forward:\n    # 1. Sum of squares: N squares, N-1 additions per row (2N-1 ops)\n    # 2. Mean square: 1 division per row (1 op)\n    # 3. rsqrt: (approx ~5-10 ops, let's say 5)\n    # 4. Normalize & Scale: N mult (x*rsigma), N mult (*g) per row (2N ops)\n    # (If ZERO_CENTERED_GAMMA, N additions for g = g+1)\n    # Total per row approx: (2N-1) + 1 + 5 + 2N = 4N + 5 ops\n    # If ZERO_CENTERED_GAMMA: add N ops => 5N + 5\n    flops_per_row = 4 * N + 5\n    if params.get('ZERO_CENTERED_GAMMA', False):\n        flops_per_row += N\n    total_flops = M * flops_per_row\n    tflops = total_flops / (ms / 1000) / 1e12\n    return tflops\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"rmsnorm_triton_bwd_perf\"\n\n# --- Pytest parametrize for performance testing ---\nRMSNORM_SHAPES_FOR_PERF = [\n    (256, 4096), (2048, 2048), (4096, 8192), (8192, 8192),\n    (1, 31744), (1, 131072),\n    (512, 10240) # Typical LLM shape\n]\nRMSNORM_DTYPES_FOR_PERF = ['fp16', 'bf16', 'fp32']\n\n@pytest.mark.parametrize('M, N', RMSNORM_SHAPES_FOR_PERF)\n@pytest.mark.parametrize('dtype_str', RMSNORM_DTYPES_FOR_PERF)\n@pytest.mark.parametrize('ZERO_CENTERED_GAMMA', [False, True]) # Test both gamma modes\ndef test_performance(M, N, ZERO_CENTERED_GAMMA, dtype_str, request):\n    set_seed()\n    eps = 1e-5\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    # Prepare inputs and output buffers for RMSNorm.apply\n    x = torch.randn(M, N, device='cuda', dtype=current_dtype)\n    g = torch.rand(N, device='cuda', dtype=current_dtype) # Original test_rmsnorm used (1,N)\n                                                       # Kernel expects g_ptr + col_offsets, so 1D g is fine.\n    y_buffer = torch.empty_like(x) # Output buffer for forward\n    rsigma_buffer = torch.empty(M, device='cuda', dtype=torch.float32) # rsigma output\n\n    # Dummy buffers for backward context (not used by fwd pass, but part of RMSNorm.apply signature)\n    # Their dtypes should match what backward pass would expect if it were called.\n    dx_dummy = torch.empty_like(x)\n    dg_dummy = torch.empty_like(g)\n    dg_tmp_dummy = torch.empty(M, N, device='cuda', dtype=torch.float32) # As per original test_rmsnorm\n\n    # Kernel launch parameters (from original test_rmsnorm)\n    n_rows, n_cols = x.shape\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    # Ensure blk_size is at least 1, next_power_of_2(0) or small N can be problematic\n    blk_size_fwd = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols if n_cols > 0 else 1))\n    if blk_size_fwd == 0 : blk_size_fwd = 1 # Safeguard\n    \n    USE_BLOCKED_fwd = n_cols > blk_size_fwd\n    # NUM_PRGMS should be > 0\n    NUM_PRGMS_fwd = min(n_rows, get_num_sms()) if n_rows > 0 and get_num_sms() > 0 else 1\n\n\n    # --- Create op_lambda for benchmarking the forward pass ---\n    op_lambda = lambda: rmsnorm(\n        x, g, y_buffer, rsigma_buffer,\n        dx_dummy, dg_dummy, dg_tmp_dummy,\n        n_rows, n_cols, ZERO_CENTERED_GAMMA,\n        blk_size_fwd, USE_BLOCKED_fwd, NUM_PRGMS_fwd, eps\n    )\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"ZERO_CENTERED_GAMMA\": ZERO_CENTERED_GAMMA,\n        \"dtype_str\": dtype_str, \"eps\": eps,\n        \"blk_size_fwd\": blk_size_fwd, \"USE_BLOCKED_fwd\": USE_BLOCKED_fwd, \"NUM_PRGMS_fwd\": NUM_PRGMS_fwd\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_rmsnorm_fwd_gbps,\n                              tflops_calculator=calculate_rmsnorm_fwd_tflops)\n    \n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n    \n######################################## HELPERS for Eval ########################################\n\n\n#Benchmark\ndef model_benchmark_configs(args):\n    config_file = args.model_configs\n    configs = get_model_configs(config_path=config_file, model_families=[\"llama3\"], model=args.model)\n\n    x_vals_list = []\n    batch_size = args.b if args.b else 1\n\n    for model_name, config in configs.items():\n        seq_len = args.sq if args.sq else 4096\n        x_vals_list.append((model_name, batch_size * seq_len, config[\"hidden_size\"]))\n\n    return x_vals_list\n\n\ndef run_benchmark(args):\n    config = []\n    if (args.M_benchmark):\n        val = args.M_start\n        x_vals_list = []\n        while val <= args.M_end:\n            x_vals_list.append(val)\n            val *= args.M_step\n        mn_args = {'N': args.N_start}\n        plot_name = str(\"rmsnorm-performance_\" + args.dtype + \"_N\" + str(args.N_start) + \"_M\" + str(args.M_start) +\n                        \"-\" + str(args.M_end) + \"-\" + str(args.M_step))\n        x_names = ['M']\n    else:\n        x_vals_list = [i for i in range(args.N_start, args.N_end, args.N_step)]\n        mn_args = {'M': args.M_start}\n        x_names = ['N']\n        plot_name = str(\"rmsnorm-performance_\" + args.dtype + \"_M\" + str(args.M_start) + \"_N\" + str(args.N_start) +\n                        \"-\" + str(args.N_end) + \"-\" + str(args.N_step))\n\n    if args.model:\n        assert not args.M_benchmark, \\\n            \"Trying to provide both -model benchmark and M_benchmark is not supported!\"\n        x_names = ['model', 'M', 'N']\n        mn_args = {}\n        plot_name = str(\"rmsnorm-performance_\" + args.dtype)\n        x_vals_list = model_benchmark_configs(args)\n\n    dtype = arg_to_torch_dtype[args.dtype]\n\n    print(plot_name)\n    config.append(\n        triton.testing.Benchmark(\n            x_names=x_names,\n            x_vals=x_vals_list,\n            line_arg='provider',\n            line_vals=['triton', 'torch'],\n            line_names=[\"Triton\", \"Torch\"],\n            styles=[('blue', '-'), ('green', '-')],\n            ylabel=\"GB/s\",\n            plot_name=plot_name,\n            args=mn_args,\n        ))\n\n    @triton.testing.perf_report(config)\n    def benchmark(M, N, provider, model=None):\n        mode = args.mode\n\n        x = torch.randn(M, N, device='cuda', dtype=dtype)\n        y = torch.zeros_like(x, device='cuda')\n        rsigma = torch.empty((M, ), device='cuda', dtype=torch.float32)\n        dx = torch.empty(M, N, device='cuda', dtype=dtype, requires_grad=False)\n        dg = torch.empty((1, N), device='cuda', dtype=dtype, requires_grad=False)\n        dg_tmp = torch.zeros(M, N, device='cuda', dtype=torch.float32, requires_grad=False)\n        n_rows, n_cols = x.shape\n        #        MAX_FUSED_SIZE = 65536 // x.element_size()\n        #        blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))\n        blk_size = 1024\n        USE_BLOCKED = n_cols > blk_size\n        NUM_PRGMS = min(n_rows, get_num_sms())\n        stream = torch.cuda.Stream()\n        torch.cuda.set_stream(stream)\n        g = torch.ones((1, N), device='cuda')\n        ZERO_CENTERED_GAMMA = False\n\n        def rms_fwd():\n            if provider == 'triton':\n                return rmsnorm(x, g, y, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size,\n                               USE_BLOCKED, NUM_PRGMS)\n            if provider == 'torch':\n                return torch_rmsnorm_fwd(x, g, ZERO_CENTERED_GAMMA)\n\n        if mode == 'fwd':\n            ms = triton.testing.do_bench(rms_fwd)\n\n        elif mode == 'bwd':\n            x_ = x.clone().detach().requires_grad_()\n            g_ = g.clone().detach().requires_grad_()\n            # Preallocate\n            y_ = torch.zeros_like(x_, dtype=dtype)\n            rsigma_ = torch.empty((M, ), device='cuda', dtype=torch.float32)\n            dx_ = torch.empty_like(x_, dtype=dtype)\n            dg_tmp_ = torch.empty_like(x_, dtype=torch.float32)\n            dg_ = torch.empty_like(g_, dtype=dtype)\n            grad_out = torch.randn_like(y_)\n\n            y_out = rmsnorm(x_, g_, y_, rsigma_, dx_, dg_, dg_tmp_, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size,\n                            USE_BLOCKED, NUM_PRGMS)\n\n            ms = triton.testing.do_bench(lambda: y_out.backward(grad_out, retain_graph=True), grad_to_none=[x_, g_])\n        else:\n            raise ValueError(f\"mode {mode} is not supported!\")\n\n        global verbose\n        if verbose:\n            print(f'SIZE: {N} Best tuning config: ({rms_kernel.best_config})')\n            print(f'time: {ms}')\n        gbps = lambda ms_val: 2 * x.nelement() * x.element_size() * 1e-9 / (ms_val * 1e-3)\n        return gbps(ms)\n\n    benchmark.run(save_path=\".\", show_plots=True, print_data=True)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        prog=\"Benchmark RMSNorm\",\n        allow_abbrev=False,\n    )\n    parser.add_argument('-model_configs', type=str, default=\"model_configs.json\", help=\"Model config json file.\")\n\n    available_models = get_available_models(model_families=[\"llama3\"])  # Dynamically load model names\n    model_help = (\n        \"Model name to benchmark. Select from: [\" + \", \".join(available_models) +\n        \"]. Use 'all' to benchmark all models. Not providing runs the default benchmark script with custom configs.\")\n    parser.add_argument('-model', type=str, default=None, help=model_help)\n    parser.add_argument('-b', type=int, default=0, help=\"Batch size used together with model.\")\n    parser.add_argument('-sq', type=int, default=0, help=\"Sequence length used together with model.\")\n    parser.add_argument('-M', \"--M_start\", default=\"1\", type=int)\n    parser.add_argument('-Ms', \"--M_step\", default=\"2\", type=int)  #This is multiplicative step\n    parser.add_argument('-Me', \"--M_end\", default=\"512\", type=int)\n    parser.add_argument('-Mb', \"--M_benchmark\", default=False, type=bool)\n\n    parser.add_argument('-N', \"--N_start\", default=\"8192\", type=int)\n    parser.add_argument('-Ns', \"--N_step\", default=\"1024\", type=int)\n    parser.add_argument('-Ne', \"--N_end\", default=\"32768\", type=int)\n\n    parser.add_argument('-d', \"--dtype\", default=\"fp16\")\n    parser.add_argument('-nb', \"--no_benchmark\", default=False, type=bool)\n    parser.add_argument(\"-v\", action='store_true', default=False, help=\"Print out the best tuning config\")\n    parser.add_argument(\"--mode\", type=str, choices=[\"fwd\", \"bwd\"], default=\"fwd\",\n                        help=\"Benchmark mode: forward only, backward only, or both.\")\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    global verbose\n    if args.no_benchmark:\n        x = torch.randn(args.M_start, args.N_start, device='cuda', dtype=args.dtype)\n        y = torch.zeros_like(x, device='cuda')\n        rsigma = torch.empty((args.M_start, ), device='cuda', dtype=torch.float32)\n        dx = torch.empty(args.M_start, args.N_start, device='cuda', dtype=args.dtype, requires_grad=False)\n        dg = torch.empty((1, args.N_start), device='cuda', dtype=args.dtype, requires_grad=False)\n        dg_tmp = torch.zeros(args.M_start, args.N_start, device='cuda', dtype=torch.float32, requires_grad=False)\n        n_rows, n_cols = x.shape\n        MAX_FUSED_SIZE = 65536 // x.element_size()\n        blk_size = min(MAX_FUSED_SIZE, triton.next_power_of_2(n_cols))\n        USE_BLOCKED = n_cols > blk_size\n        NUM_PRGMS = min(n_rows, get_num_sms())\n        g = torch.ones((1, args.N_start), device='cuda', dtype=args.dtype)\n        ZERO_CENTERED_GAMMA = True\n        rmsnorm(x, y, g, rsigma, dx, dg, dg_tmp, n_rows, n_cols, ZERO_CENTERED_GAMMA, blk_size, USE_BLOCKED, NUM_PRGMS)\n    else:\n        verbose = args.v\n        run_benchmark(args)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"
    },
    {
        "file": "test_block_pointer_matmul.py",
        "target_kernel_name": "matmul_no_scf_with_advance_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `matmul_no_scf_with_advance_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `matmul_no_scf_with_advance_kernel`,  is designed to use block pointers for a basic matrix multiplication(without explicit loops for the K dimension, hence \"no_scf\" - no structured control flow)\n\n**Your objective is to implement the body of `matmul_no_scf_with_advance_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_no_scf_with_advance_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `matmul_no_scf_with_advance_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `matmul_no_scf_with_advance_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\nimport os\n\n######################################## Imports ######################################## \n\n\n@triton.jit\ndef matmul_no_scf_with_advance_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    \"\"\"\n    Computes a block of the matrix multiplication C = A @ B.\n\n    This kernel is designed to calculate a single (BLOCK_M, BLOCK_N) tile of the output matrix C.\n    It loads a (BLOCK_M, BLOCK_K) tile from matrix A and a (BLOCK_K, BLOCK_N) tile from matrix B.\n    The kernel utilizes `tl.make_block_ptr` for creating pointers to blocks of A and B,\n    and demonstrates the use of `tl.advance` for adjusting these block pointers.\n    The core computation is performed using `tl.dot`. The resulting tile is then stored\n    back to the C matrix. This version does not use Triton's Structured Control Flow (SCF)\n    for iterating over the K dimension; it assumes BLOCK_K covers the necessary\n    portion of the K dimension for a single dot product accumulation or that accumulation\n    over K-blocks is handled externally.\n\n    Parameters:\n    -----------\n    a_ptr : tl.pointer_type\n        Pointer to the input matrix A.\n    b_ptr : tl.pointer_type\n        Pointer to the input matrix B.\n    c_ptr : tl.pointer_type\n        Pointer to the output matrix C.\n    M : int\n        The number of rows in matrix A and matrix C.\n    N : int\n        The number of columns in matrix B and matrix C.\n    K : int\n        The number of columns in matrix A and rows in matrix B.\n    stride_am : int\n        The stride (in elements) for moving from one row to the next in matrix A.\n    stride_ak : int\n        The stride (in elements) for moving from one column to the next in matrix A.\n    stride_bk : int\n        The stride (in elements) for moving from one row to the next in matrix B.\n    stride_bn : int\n        The stride (in elements) for moving from one column to the next in matrix B.\n    stride_cm : int\n        The stride (in elements) for moving from one row to the next in matrix C.\n    stride_cn : int\n        The stride (in elements) for moving from one column to the next in matrix C.\n    BLOCK_M : tl.constexpr\n        The height of the tiles processed from matrix A and C.\n    BLOCK_N : tl.constexpr\n        The width of the tiles processed from matrix B and C.\n    BLOCK_K : tl.constexpr\n        The depth of the tiles (common dimension K) processed from A and B for the dot product.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\nimport os\n######################################## Imports ######################################## \n\n@triton.jit\ndef matmul_no_scf_with_advance_kernel(  #\n        a_ptr, b_ptr, c_ptr,  #\n        M, N, K,  #\n        stride_am, stride_ak,  #\n        stride_bk, stride_bn,  #\n        stride_cm, stride_cn,  #\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr  #\n):\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn), offsets=(0, 0),\n                                    block_shape=(BLOCK_K, BLOCK_N), order=(1, 0))\n    # Below two lines are just for testing negative offsets for the `advance` API, which could be removed\n    a_block_ptr = tl.advance(a_block_ptr, (BLOCK_M, -BLOCK_K))\n    a_block_ptr = tl.advance(a_block_ptr, (-BLOCK_M, BLOCK_K))\n    a = tl.load(a_block_ptr, boundary_check=(1, ), padding_option=\"zero\")\n    b = tl.load(b_block_ptr, boundary_check=(0, ), padding_option=\"zero\")\n\n    c = tl.dot(a, b)\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptrs, c)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n@pytest.mark.interpreter\n@pytest.mark.parametrize(\"shape, num_warps\", [  #\n    (shape, num_warps) for shape in [\n        [64, 64, 16],\n        [64, 64, 32],\n        [64, 64, 64],\n    ] for num_warps in [4, 8]\n])\ndef test_block_ptr_matmul_no_scf(shape, num_warps, request, device='cuda'):\n    set_seed()\n\n    m, n, k = shape\n    a = torch.randn((m, k), device=device, dtype=torch.float16)\n    b = torch.randn((k, n), device=device, dtype=torch.float16)\n    c = torch.empty((m, n), device=device, dtype=torch.float32)\n\n    grid = lambda META: (1, )\n    matmul_no_scf_with_advance_kernel[grid](\n        a_ptr=a, b_ptr=b, c_ptr=c,  #\n        M=m, N=n, K=k,  #\n        stride_am=a.stride(0), stride_ak=a.stride(1),  #\n        stride_bk=b.stride(0), stride_bn=b.stride(1),  #\n        stride_cm=c.stride(0), stride_cn=c.stride(1),  #\n        BLOCK_M=m, BLOCK_N=n, BLOCK_K=k,  #\n        num_warps=num_warps)\n    golden = torch.matmul(a, b)\n\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = c.clone().detach().cpu()\n    ################################################################### \n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    torch.testing.assert_close(c, golden, check_dtype=False)\n\n# --- Python wrapper for the kernel ---\ndef block_pointer_matmul_triton_wrapper(a_tensor, b_tensor, c_tensor, num_warps_arg):\n    m, k_a = a_tensor.shape\n    k_b, n = b_tensor.shape\n    assert k_a == k_b, \"K dimension must match\"\n    \n    # The kernel is designed for a single block operation covering the whole matrix\n    # So, BLOCK_M=m, BLOCK_N=n, BLOCK_K=k_a\n    # Grid is (1,) as per original test\n    grid = (1,)\n    \n    matmul_no_scf_with_advance_kernel[grid](\n        a_ptr=a_tensor, b_ptr=b_tensor, c_ptr=c_tensor,\n        M=m, N=n, K=k_a,\n        stride_am=a_tensor.stride(0), stride_ak=a_tensor.stride(1),\n        stride_bk=b_tensor.stride(0), stride_bn=b_tensor.stride(1),\n        stride_cm=c_tensor.stride(0), stride_cn=c_tensor.stride(1),\n        BLOCK_M=m, BLOCK_N=n, BLOCK_K=k_a, # Kernel uses these as constexpr\n        num_warps=num_warps_arg # num_warps passed to launch, not directly used by this jit kernel signature\n                                # but can affect autotuning if kernel had it. This kernel is not autotuned.\n    )\n    return c_tensor\n\n# --- Define TFLOPS and GB/s calculators for this specific GEMM ---\ndef calculate_block_matmul_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    # Single dot product of (M,K) @ (K,N)\n    flops = 2 * M * N * K\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_block_matmul_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    dtype_str = params.get('dtype_str', 'fp16') # Original test uses fp16\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    bytes_a = M * K * element_size\n    bytes_b = K * N * element_size\n    bytes_c = M * N * element_size\n    total_bytes = bytes_a + bytes_b + bytes_c\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"block_pointer_matmul_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Original shapes are small. For perf, we might want larger, but respecting kernel's nature.\n# This kernel does M*K, K*N, M*N as ONE block.\n# So, M, N, K are also BLOCK_M, BLOCK_N, BLOCK_K.\n# Triton has limits on BLOCK_SIZEs (e.g., BLOCK_K often <= 1024 or 2048 due to shared memory).\n# Let's use shapes where M, N, K themselves are valid block sizes.\nBPM_SHAPES_FOR_PERF = [\n    # M,  N,   K\n    (64, 64,  64),\n    (128, 128, 128),\n    (64, 128, 256),\n    (256, 64,  128),\n    (128, 256, 64),\n]\nBPM_DTYPES_FOR_PERF = ['fp16', 'fp32'] # Add bf16 if relevant\n# num_warps is passed to launch but not a JIT param for this kernel.\n# It can influence scheduling. Let's fix it for perf or parametrize.\nBPM_NUM_WARPS_FOR_PERF = [4, 8]\n\n\n@pytest.mark.parametrize(\"shape\", BPM_SHAPES_FOR_PERF)\n@pytest.mark.parametrize(\"num_warps_arg\", BPM_NUM_WARPS_FOR_PERF)\n@pytest.mark.parametrize(\"dtype_str\", BPM_DTYPES_FOR_PERF)\ndef test_performance(shape, num_warps_arg, dtype_str, request, device='cuda'): # Renamed\n    set_seed()\n    m, n, k = shape\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    a = torch.randn((m, k), device=device, dtype=current_dtype)\n    b = torch.randn((k, n), device=device, dtype=current_dtype)\n    # Output c for this kernel is float32 if inputs are float16, due to tl.dot default accumulator.\n    # The kernel does `c_val.to(c_ptr.dtype.element_ty)`. So c's dtype matters.\n    # Let's make c's dtype match input for simplicity, or be float32 if inputs are lower precision.\n    # If a,b are fp16, c = torch.dot(a,b) accumulates in fp32, then cast to c.dtype.\n    # For perf, often C is same dtype as A, B or higher precision (fp32).\n    # Let C be same dtype as input for this benchmark.\n    c = torch.empty((m, n), device=device, dtype=current_dtype)\n\n\n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: block_pointer_matmul_triton_wrapper(a, b, c, num_warps_arg)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"M\": m, \"N\": n, \"K\": k,\n        \"num_warps\": num_warps_arg, \"dtype_str\": dtype_str\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_block_matmul_gbps,\n                              tflops_calculator=calculate_block_matmul_tflops)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_block_copy.py",
        "target_kernel_name": "block_copy_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `block_copy_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `block_copy_kernel`,  is designed to copy data using block pointers and, crucially, how out-of-bounds accesses are handled with different padding_options.\n\n**Your objective is to implement the body of `block_copy_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `block_copy_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `block_copy_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `block_copy_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\nimport os\n######################################## Imports ########################################\n\n@triton.jit\ndef block_copy_kernel(a_ptr, b_ptr, N, BLOCK_SIZE: tl.constexpr, padding_option: tl.constexpr):\n    \"\"\"\n    Performs a block-wise copy from an input tensor 'a' to an output tensor 'b',\n    with a focus on testing padding behavior during out-of-bounds loads from 'a'.\n\n    This kernel is designed such that the input tensor 'a' is logically half the size\n    of the output tensor 'b' (i.e., 'a' has N // 2 elements, 'b' has N elements).\n    When loading data from 'a', if a block read extends beyond its N // 2 elements,\n    the 'padding_option' determines the values used for the out-of-bounds elements.\n    The loaded (and potentially padded) data is then stored into 'b'.\n\n    Args:\n        a_ptr (tl.tensor): Pointer to the input tensor 'a'. Data will be loaded from here.\n        b_ptr (tl.tensor): Pointer to the output tensor 'b'. Data will be stored here.\n        N (int): The logical size of the output tensor 'b'. The input tensor 'a' is\n                 assumed to be of logical size N // 2.\n        BLOCK_SIZE (tl.constexpr): The size of the data block to be processed (loaded and stored)\n                                   by each program instance. This must be a compile-time constant.\n        padding_option (tl.constexpr): Specifies the padding behavior for out-of-bounds reads\n                                       from 'a_ptr'. Can be 'zero', 'nan', or other options\n                                       supported by tl.load. If None, default boundary behavior is used.\n                                       This must be a compile-time constant.\n    \"\"\"\n    # Kernel implementation to be filled.\n    # The goal is to load a block from a_ptr (with potential padding)\n    # and store it to b_ptr.\n\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\nimport os\n######################################## Imports ######################################## \n\n@triton.jit\ndef block_copy_kernel(a_ptr, b_ptr, N, BLOCK_SIZE: tl.constexpr, padding_option: tl.constexpr):\n    pid = tl.program_id(0)\n    # We only copy half of the data to see if the padding works\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(N // 2, ), strides=(1, ), offsets=(pid * BLOCK_SIZE, ),\n                                    block_shape=(BLOCK_SIZE, ), order=(0, ))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(N, ), strides=(1, ), offsets=(pid * BLOCK_SIZE, ),\n                                    block_shape=(BLOCK_SIZE, ), order=(0, ))\n    if padding_option is None:\n        a = tl.load(a_block_ptr, boundary_check=(0, ))\n    else:\n        a = tl.load(a_block_ptr, boundary_check=(0, ), padding_option=padding_option)\n    tl.store(b_block_ptr, a, boundary_check=(0, ))\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef is_interpreter():\n    return os.environ.get('TRITON_INTERPRET', '0') == '1'\n\ndef check_type_supported(dtype, device='cuda'):\n    '''\n    skip test if dtype is not supported on the current device\n    '''\n    if device in ['cuda']:\n        cc = torch.cuda.get_device_capability()\n        if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n            pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n        if cc[0] < 9 and dtype in {tl.float8e4nv, \"float8e4nv\", \"float8_e4m3fn\"}:\n            pytest.skip(\"float8e4nv is only supported on NVGPU with cc >= 90\")\n    if is_interpreter():\n        if dtype in [tl.bfloat16, \"bfloat16\", torch.bfloat16]:\n            pytest.skip(\"bfloat16 is not supported in the interpreter\")\n\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n@pytest.mark.interpreter\n@pytest.mark.parametrize(\"dtypes_str, n, padding_option\", [  #\n    (dtypes_str, n, padding)\n    for dtypes_str in ((\"bool\", \"bool\"), (\"int16\", \"int16\"), (\"int32\", \"int32\"), (\"float16\", \"float16\"),\n                       (\"float32\", \"float32\"), (\"bfloat16\", \"bfloat16\"))\n    for n in (64, 128, 256, 512, 1024)\n    for padding in (None, \"zero\", \"nan\")  #\n])\ndef test_block_copy(dtypes_str, n, padding_option, request, device='cuda'):\n    src_dtype_str = dtypes_str[0]\n    dst_dtype_str = dtypes_str[1]\n    src_dtype = getattr(torch, src_dtype_str)\n    dst_dtype = getattr(torch, dst_dtype_str)\n    check_type_supported(src_dtype, device)\n    check_type_supported(dst_dtype, device)\n    if src_dtype_str in (\"bool\", \"int16\", \"int32\"):\n        if padding_option == \"nan\":\n            pytest.skip(\"Padding with NaN is not supported for integer types\")\n        a = torch.randint(0, 2, (n, ), device=device, dtype=src_dtype)\n    else:\n        a = torch.randn((n, ), device=device, dtype=src_dtype)\n    b = torch.zeros((n, ), device=device, dtype=dst_dtype)\n\n    grid = lambda meta: (triton.cdiv(n, meta[\"BLOCK_SIZE\"]), )\n    block_copy_kernel[grid](a_ptr=a, b_ptr=b, N=n, BLOCK_SIZE=64, padding_option=padding_option)\n    a.to(dst_dtype)\n    \n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    assert torch.all(a[0:n // 2] == b[0:n // 2])\n    if padding_option == \"zero\":\n        assert torch.all(b[n // 2:n] == 0)\n    elif padding_option == \"nan\":\n        assert torch.all(torch.isnan(b[n // 2:n]))\n\n    \n    ################### save True in result_gold (indicates it passed block copy tests, implies the gen kernel worked) ###################\n    c = torch.tensor([[1.0]])\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = c.clone().detach().cpu()\n    ################################################################### \n\n# --- Define GB/s calculator for Block Copy ---\n# TFLOPS is not relevant for a copy operation.\ndef calculate_block_copy_gbps(params: dict, ms: float) -> float:\n    N = params['N']\n    # Kernel reads N/2 elements from A and writes N/2 elements to B (plus padding if any).\n    # For bandwidth, consider the actual data moved.\n    # If padding is \"zero\" or \"nan\", these are effectively writes.\n    # Let's count N/2 read and N/2 write of actual data from A.\n    # The padding writes up to BLOCK_SIZE elements per block, but only up to N for b_block_ptr.\n    # For simplicity, assume effective N/2 elements read, N/2 elements written from A's content.\n    # If padding fills the rest of B up to N, then N elements written in total.\n    # The kernel copies a_block_ptr (derived from N//2 elements) to b_block_ptr.\n    # The load is from a conceptual N//2 elements. The store is into N elements.\n    # Let's assume N/2 elements are read from 'a' and N/2 elements (from 'a') are written to 'b'.\n    # The padding part affects what's written to b[N//2:].\n    \n    # Data moved: read N/2 elements, write N/2 elements (from 'a')\n    # + potentially N/2 elements written as padding.\n    # Let's consider useful data copied: N/2 read, N/2 written.\n    elements_moved = (N // 2) + (N // 2) # Read from A, Write to B (meaningful part)\n    \n    dtype_str = params['src_dtype_str'] # Use source dtype for element size\n    if dtype_str == 'bool': element_size = 1 # Approx\n    elif dtype_str in ('int16', 'float16', 'bfloat16'): element_size = 2\n    elif dtype_str in ('int32', 'float32'): element_size = 4\n    else: element_size = 4 # Default\n\n    total_bytes = elements_moved * element_size\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"block_copy_triton_perf\"\n\n# --- Pytest test_block_copy function MODIFIED for performance benchmarking ---\n# Original parametrization is kept.\n# BLOCK_SIZE is fixed at 64 in the original kernel call.\n# If we want to test different BLOCK_SIZEs, it needs to be part of parametrize\n# and passed to the kernel. For now, stick to fixed BLOCK_SIZE=64.\nFIXED_BLOCK_SIZE_FOR_PERF = 64\n\n@pytest.mark.parametrize(\"dtypes_str_tuple, n, padding_option\", [\n    (dtypes_tuple, n_val, padding_val)\n    for dtypes_tuple in ((\"float16\", \"float16\"), (\"float32\", \"float32\"), (\"bfloat16\", \"bfloat16\"), (\"int32\", \"int32\")) # Focus on common perf types\n    for n_val in (1024, 8192, 65536, 262144, 1048576) # Larger N for perf\n    for padding_val in (None, \"zero\") # \"nan\" might be slow or less common for perf focus\n])\ndef test_performance(dtypes_str_tuple, n, padding_option, request, device='cuda'): # Renamed\n    src_dtype_str, dst_dtype_str = dtypes_str_tuple\n    # For performance, usually src_dtype == dst_dtype for simple copy.\n    # If they differ, it's a copy + cast. Let's assume they are same for perf test.\n    if src_dtype_str != dst_dtype_str:\n        pytest.skip(\"Skipping perf test where src_dtype != dst_dtype for block_copy.\")\n    \n    current_dtype = getattr(torch, src_dtype_str)\n    check_type_supported(current_dtype, device) # Check for bfloat16/float8 if added\n\n    if src_dtype_str in (\"bool\", \"int16\", \"int32\") and padding_option == \"nan\":\n        pytest.skip(\"Padding with NaN is not supported for integer types\")\n\n    set_seed()\n    if src_dtype_str in (\"bool\", \"int16\", \"int32\"):\n        a = torch.randint(0, 2, (n,), device=device, dtype=current_dtype)\n    else:\n        a = torch.randn((n,), device=device, dtype=current_dtype)\n    \n    # b is the output tensor. For benchmarking, its initial content doesn't matter as it's overwritten.\n    b = torch.empty((n,), device=device, dtype=current_dtype) # Use empty for perf\n\n    # Kernel launch grid\n    # The kernel processes N//2 elements of A. Grid should be based on this.\n    # If BLOCK_SIZE is 64, grid is cdiv(N//2, 64)\n    # However, the original test_block_copy uses cdiv(n, meta[\"BLOCK_SIZE\"])\n    # This seems to imply the kernel is launched as if processing N elements of A,\n    # but internally a_block_ptr shape is N//2.\n    # Let's follow the original grid logic for the launch.\n    # The kernel's pid will iterate based on this grid.\n    # Inside kernel: a_block_ptr offsets are pid * BLOCK_SIZE, shape N//2.\n    # This means if grid is cdiv(N, BLOCK_SIZE), pid can go up to N/BLOCK_SIZE -1.\n    # pid * BLOCK_SIZE can exceed N//2. This is handled by boundary_check in tl.load.\n    # This seems correct for testing boundary checks.\n    \n    grid = lambda meta: (triton.cdiv(n, meta[\"BLOCK_SIZE\"]), ) # BLOCK_SIZE is a key for autotune if used\n                                                              # Here, it's a direct constexpr.\n\n    # --- Create op_lambda for benchmarking ---\n    # BLOCK_SIZE and padding_option are constexpr in the kernel\n    op_lambda = lambda: block_copy_kernel[grid](\n        a_ptr=a, b_ptr=b, N=n,\n        BLOCK_SIZE=FIXED_BLOCK_SIZE_FOR_PERF, # Pass as constexpr\n        padding_option=padding_option         # Pass as constexpr\n    )\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=50, repetition=200) # Copy is fast\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"N\": n, \"src_dtype_str\": src_dtype_str, \"dst_dtype_str\": dst_dtype_str,\n        \"padding_option\": padding_option,\n        \"BLOCK_SIZE\": FIXED_BLOCK_SIZE_FOR_PERF # Log the fixed block size\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_block_copy_gbps,\n                              tflops_calculator=None) # TFLOPS not relevant\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_tma_store_gemm.py",
        "target_kernel_name": "matmul_tma_load_store",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `matmul_tma_load_store`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `matmul_tma_load_store`,  performs a single block matrix multiplication (C = A @ B) using Triton's block pointers using TMA (Tensor Memory Accelerator).\n\n**Your objective is to implement the body of `matmul_tma_load_store`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_tma_load_store` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `matmul_tma_load_store` and relevant helper utilities are provided in the context below. You only need to complete the code for `matmul_tma_load_store` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n######################################## Imports ########################################\n\n\n@triton.jit\ndef matmul_tma_load_store(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    OUTPUT_F16: tl.constexpr\n):\n    \"\"\"\n    Performs a single block matrix multiplication (C = A @ B) using Triton's\n    block pointers, potentially leveraging TMA (Tensor Memory Accelerator)\n    for efficient loads and stores on compatible hardware.\n\n    This kernel is designed to compute one `BLOCK_M x BLOCK_N` tile of the output matrix C.\n    Specifically, it loads a `BLOCK_M x BLOCK_K` tile from matrix A (starting from `a_ptr`\n    at offset (0,0)) and a `BLOCK_K x BLOCK_N` tile from matrix B (starting from `b_ptr`\n    at offset (0,0)). It then computes their dot product and stores the resulting\n    `BLOCK_M x BLOCK_N` tile into matrix C (starting at `c_ptr` at offset (0,0)).\n\n    The kernel uses `tl.load` and `tl.store` with block pointers configured as follows:\n    - Matrix A's tile is loaded assuming a row-major layout within the block (`order=(1,0)`).\n    - Matrix B's tile is loaded assuming a column-major layout within the block (`order=(0,1)`),\n      which is often beneficial for dot product operations.\n    - Matrix C's tile is stored assuming a row-major layout within the block (`order=(1,0)`).\n\n    Input matrices A and B are expected to have data types suitable for `tl.dot`\n    (e.g., tl.float16, tl.bfloat16, tl.float32). The accumulation for the dot\n    product is typically performed in tl.float32.\n\n    Args:\n        a_ptr: Pointer to the base of the input matrix A in global memory.\n        b_ptr: Pointer to the base of the input matrix B in global memory.\n        c_ptr: Pointer to the base of the output matrix C in global memory.\n        M (int): The total number of rows in the full matrix A and matrix C. Used for boundary checks.\n        N (int): The total number of columns in the full matrix B and matrix C. Used for boundary checks.\n        K (int): The total number of columns in the full matrix A and rows in matrix B\n                 (the common dimension for matrix multiplication). Used for boundary checks.\n        stride_am (int): Stride in number of elements to move from one row to the next in matrix A.\n        stride_ak (int): Stride in number of elements to move from one column to the next in matrix A.\n        stride_bk (int): Stride in number of elements to move from one row to the next in matrix B.\n        stride_bn (int): Stride in number of elements to move from one column to the next in matrix B.\n        stride_cm (int): Stride in number of elements to move from one row to the next in matrix C.\n        stride_cn (int): Stride in number of elements to move from one column to the next in matrix C.\n        BLOCK_M (tl.constexpr): The height (number of rows) of the tile to be processed from matrix A\n                                and written to matrix C. This defines the M-dimension of the block.\n        BLOCK_N (tl.constexpr): The width (number of columns) of the tile to be processed from matrix B\n                                and written to matrix C. This defines the N-dimension of the block.\n        BLOCK_K (tl.constexpr): The width (number of columns) of the tile from matrix A, and height\n                                (number of rows) of the tile from matrix B. This defines the\n                                K-dimension of the blocks used in the dot product.\n        OUTPUT_F16 (tl.constexpr): A boolean flag. If True, the resulting C tile is cast to\n                                   `tl.float16` before being stored. Otherwise, it is stored\n                                   in the accumulation data type (typically `tl.float32`).\n    \"\"\"\n    # Your code here\n\n",
        "label": "\n\n# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n######################################## Imports ######################################## \n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n\n@triton.jit\ndef matmul_tma_load_store(  #\n        a_ptr, b_ptr, c_ptr,  #\n        M, N, K,  #\n        stride_am, stride_ak,  #\n        stride_bk, stride_bn,  #\n        stride_cm, stride_cn,  #\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,  #\n        OUTPUT_F16: tl.constexpr  #\n):\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn), offsets=(0, 0),\n                                    block_shape=(BLOCK_K, BLOCK_N), order=(0, 1))\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    a = tl.load(a_block_ptr)\n    b = tl.load(b_block_ptr)\n\n    c = tl.dot(a, b)\n    if OUTPUT_F16:\n        c = c.to(tl.float16)\n\n    tl.store(c_block_ptr, c)\n\n##################################################################################################################################################  \n\n\nimport pytest\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom torch.testing import assert_close\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n@pytest.mark.parametrize('M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_F16', [\n    [64, 64, 16, 1, 4, False, True, False],\n    [64, 64, 16, 1, 4, False, True, True],\n    [128, 64, 32, 1, 4, False, True, False],\n    [128, 64, 32, 1, 4, False, True, True],\n    [64, 128, 32, 1, 4, False, True, False],\n    [64, 128, 32, 1, 4, False, True, True],\n    [128, 128, 64, 1, 4, False, True, False],\n    [128, 128, 64, 1, 4, False, True, True],\n])\ndef test_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F16, request):\n    set_seed()\n\n    if (TRANS_A):\n        a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n    else:\n        a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n    if (TRANS_B):\n        b = torch.randn((N, K), device='cuda', dtype=torch.float16).T\n    else:\n        b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n    if OUTPUT_F16:\n        c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n\n    matmul_tma_load_store[(1, 1)](\n        a_ptr=a, b_ptr=b, c_ptr=c,  #\n        M=M, N=N, K=K,  #\n        stride_am=a.stride(0), stride_ak=a.stride(1),  #\n        stride_bk=b.stride(0), stride_bn=b.stride(1),  #\n        stride_cm=c.stride(0), stride_cn=c.stride(1),  #\n        BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,  #\n        num_warps=NUM_WARPS, num_ctas=NUM_CTAS,  #\n        OUTPUT_F16=OUTPUT_F16)\n    golden = torch.matmul(a, b)\n    torch.set_printoptions(profile=\"full\")\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = c.clone().detach().cpu()\n    ###################################################################\n\n    assert_close(c, golden, rtol=1e-2, atol=1e-3, check_dtype=False)\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef tma_gemm_triton_wrapper(a_tensor, b_tensor, c_buffer, \n                            M_dim, N_dim, K_dim, \n                            output_f16_flag, num_warps_launch): # num_ctas_launch removed as not used by grid\n    grid = (1,) \n    matmul_tma_load_store[grid](\n        a_ptr=a_tensor, b_ptr=b_tensor, c_ptr=c_buffer,\n        M=M_dim, N=N_dim, K=K_dim,\n        stride_am=a_tensor.stride(0), stride_ak=a_tensor.stride(1),\n        stride_bk=b_tensor.stride(0), stride_bn=b_tensor.stride(1),\n        stride_cm=c_buffer.stride(0), stride_cn=c_buffer.stride(1),\n        BLOCK_M=M_dim, BLOCK_N=N_dim, BLOCK_K=K_dim, \n        OUTPUT_F16=output_f16_flag,\n        num_warps=num_warps_launch\n        # num_ctas=num_ctas_launch # Launch hint, not JIT param\n    )\n    return c_buffer\n\ndef calculate_tma_gemm_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    flops = 2 * M * N * K \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\n    \ndef calculate_tma_gemm_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    \n    # Input dtype is fp16 as per test setup\n    input_dtype_for_calc = torch.float16\n    # Output C can be fp16 or fp32\n    output_dtype_str = \"fp16\" if params['OUTPUT_F16'] else \"fp32\"\n    \n    output_torch_dtype_for_calc = torch.float16\n    if output_dtype_str == 'fp32': output_torch_dtype_for_calc = torch.float32\n    # No bf16 in this test's parametrize for output\n\n    in_element_size = torch.tensor([], dtype=input_dtype_for_calc).element_size()\n    out_element_size = torch.tensor([], dtype=output_torch_dtype_for_calc).element_size()\n    \n    bytes_a = M * K * in_element_size\n    bytes_b = K * N * in_element_size \n    bytes_c_write = M * N * out_element_size\n    total_bytes = bytes_a + bytes_b + bytes_c_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"tma_store_gemm_triton_perf\"\n\nTMA_GEMM_PERF_PARAMS = [ \n    [64, 64, 16, 1, 4, False, True, False], [64, 64, 16, 1, 4, False, True, True],\n    [128, 64, 32, 1, 4, False, True, False], [128, 64, 32, 1, 4, False, True, True],\n    [64, 128, 32, 1, 4, False, True, False], [64, 128, 32, 1, 4, False, True, True],\n    [128, 128, 64, 1, 4, False, True, False], [128, 128, 64, 1, 4, False, True, True],\n    [64, 64, 128, 1, 4, False, False, True], \n    [64, 64, 128, 1, 4, False, False, False],\n    [32, 32, 256, 1, 4, False, False, True],\n]\n\n@pytest.mark.parametrize('M,N,K,NUM_CTAS_param,NUM_WARPS_param,TRANS_A,TRANS_B,OUTPUT_F16_param', TMA_GEMM_PERF_PARAMS)\n# @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\ndef test_performance(M, N, K, NUM_CTAS_param, NUM_WARPS_param, TRANS_A, TRANS_B, OUTPUT_F16_param, request):\n    cap = torch.cuda.get_device_capability()\n    if cap[0] < 9: \n        pytest.skip(\"Requires compute capability >= 9\")\n\n    # --- CORRECTED Shared memory skip logic ---\n    input_torch_dtype_for_smem_check = torch.float16 # Inputs A and B are fp16 in this test\n    element_size_bytes_for_smem_check = torch.tensor([], dtype=input_torch_dtype_for_smem_check).element_size()\n    \n    smem_elements_needed = (M * K) + (K * N) # For one dot A(M,K) @ B(K,N)\n    if smem_elements_needed * element_size_bytes_for_smem_check > 65536: # 64KB limit\n        pytest.skip(f\"Skipping M{M}N{N}K{K} due to estimated shared memory for inputs: \"\n                    f\"{smem_elements_needed * element_size_bytes_for_smem_check} > 65536\")\n    # --- End of corrected skip logic ---\n\n    set_seed()\n        \n    # Input setup from original test, input dtype is fp16\n    input_torch_dtype = torch.float16\n    a_orig_shape = (K, M) if TRANS_A else (M, K)\n    b_orig_shape = (N, K) if TRANS_B else (K, N) # If TRANS_B, B is (N,K), so B.T is (K,N) for matmul\n    \n    a = torch.randn(a_orig_shape, device='cuda', dtype=input_torch_dtype)\n    if TRANS_A: a = a.T \n    \n    b = torch.randn(b_orig_shape, device='cuda', dtype=input_torch_dtype)\n    if TRANS_B: b = b.T \n\n    c_out_torch_dtype = torch.float16 if OUTPUT_F16_param else torch.float32\n    c_buffer = torch.empty((M, N), device=a.device, dtype=c_out_torch_dtype)\n\n    op_lambda = lambda: tma_gemm_triton_wrapper(\n        a, b, c_buffer, M, N, K, \n        OUTPUT_F16_param, NUM_WARPS_param\n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K, \n        \"NUM_CTAS\": NUM_CTAS_param, \"NUM_WARPS\": NUM_WARPS_param,\n        \"TRANS_A\": TRANS_A, \"TRANS_B\": TRANS_B, \n        \"OUTPUT_F16\": OUTPUT_F16_param, # This is for the calculator\n        \"input_dtype_str\": \"fp16\" # For the calculator\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_tma_gemm_gbps,\n                                            tflops_calculator=calculate_tma_gemm_tflops)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_kernel_dot.py",
        "target_kernel_name": "kernel_dot",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `kernel_dot`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `kernel_dot`,  performs a in-place dot product (matrix multiplication)\n\n**Your objective is to implement the body of `kernel_dot`.**\n\nYou must ensure that:\n1.  All arguments received by `kernel_dot` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `kernel_dot` and relevant helper utilities are provided in the context below. You only need to complete the code for `kernel_dot` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport multiprocessing\nimport shutil\nimport tempfile\nimport os\nimport pytest\n\nimport triton\nimport triton.language as tl\nfrom triton.backends.compiler import AttrsDescriptor\nfrom triton.compiler import ASTSource\n######################################## Imports ########################################\n\n\n@triton.jit\ndef kernel_dot(Z):\n    \"\"\"\n    This Triton kernel performs an in-place dot product (matrix multiplication)\n    of a 16x16 block of a given tensor Z with itself.\n\n    Parameters\n    ----------\n    Z : tl.tensor (pointer)\n        A Triton tensor pointer representing a 2D matrix.\n        The kernel will load a 16x16 block from this tensor,\n        compute its dot product with itself (i.e., block @ block),\n        and store the result back into the same location in Z.\n        This tensor serves as both input and output.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ########################################\nimport multiprocessing\nimport shutil\nimport tempfile\nimport os\nimport pytest\n\nimport triton\nimport triton.language as tl\nfrom triton.backends.compiler import AttrsDescriptor\nfrom triton.compiler import ASTSource\n######################################## Imports ########################################\n\n@triton.jit\ndef kernel_dot(Z):\n    offs = tl.arange(0, 16)[:, None] * 16 + tl.arange(0, 16)[None, :]\n    z = tl.load(Z + offs)\n    z = tl.dot(z, z)\n    tl.store(Z + offs, z)\n\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nimport multiprocessing\nimport shutil\nimport tempfile\nimport os\nimport pytest\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\nimport triton\nimport triton.language as tl\nfrom triton.backends.compiler import AttrsDescriptor\nfrom triton.compiler import ASTSource\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n# --- Fixture Definition ---\n@pytest.fixture\ndef fresh_triton_cache(monkeypatch):\n    temp_dir = tempfile.mkdtemp()\n    cache_dir = os.path.join(temp_dir, \".triton\")\n    os.makedirs(cache_dir, exist_ok=True)\n    monkeypatch.setenv(\"TRITON_CACHE_DIR\", cache_dir)\n    yield cache_dir\n    shutil.rmtree(temp_dir)\n# --- End Fixture Definition ---\n\n# Check if a target is available. Skip tests if not.\ntry:\n    target = triton.runtime.driver.active.get_current_target()\n    TARGET_AVAILABLE = True\nexcept Exception:\n    TARGET_AVAILABLE = False\n    target = None\n\n# Decorator to skip tests if target is not available\nskip_if_no_target = pytest.mark.skipif(not TARGET_AVAILABLE, reason=\"Triton target not available (e.g., no GPU or CUDA/ROCm setup)\")\n\n@skip_if_no_target\ndef compile_kernel_dot_for_test(attrs): # Renamed to be specific\n    # kernel_dot is defined globally above\n    src = ASTSource(fn=kernel_dot, signature={'Z': \"*fp32\"}, attrs=attrs, constants={})\n    triton.compile(src=src, target=target)\n\n@skip_if_no_target\ndef test_compile_kernel_dot_in_forked_subproc(fresh_triton_cache, request) -> None: # Test name updated for clarity\n    config = AttrsDescriptor.from_hints({0: 16})\n    current_start_method = multiprocessing.get_start_method(allow_none=True)\n    if current_start_method is None:\n        try:\n            multiprocessing.set_start_method('fork', force=True)\n        except RuntimeError:\n            print(\"Warning: Could not force 'fork' start method. Using default.\")\n    if multiprocessing.get_start_method(allow_none=True) != 'fork':\n        pytest.skip(\"Test requires 'fork' multiprocessing start method.\")\n\n    proc = multiprocessing.Process(target=compile_kernel_dot_for_test, args=(config, ))\n    proc.start()\n    proc.join(timeout=60)\n    if proc.is_alive():\n        proc.terminate()\n        proc.join()\n        pytest.fail(\"Process timed out\")\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = torch.tensor([[0.0]]).clone().detach().cpu()\n    ################################################################### \n\n    assert proc.exitcode == 0\n\n    result_gold[sanitized_key_name] = torch.tensor([[1.0]]).clone().detach().cpu()\n\n\n\n# --- Python wrapper for launching kernel_dot for benchmarking ---\ndef kernel_dot_triton_wrapper(Z_tensor, num_warps_launch):\n    # Kernel operates on a 16x16 tile.\n    # Grid is (1,) because the kernel itself doesn't use program_id for tiling.\n    grid = (1,)\n    kernel_dot[grid](Z_tensor, num_warps=num_warps_launch) # Z_tensor is modified in-place\n    return Z_tensor # Return for consistency, though modified in-place\n\n# --- Define TFLOPS and GB/s calculators for the 16x16 dot product ---\nFIXED_DIM_FOR_KERNEL_DOT = 16\n\ndef calculate_kernel_dot_tflops(params: dict, ms: float) -> float:\n    M = N = K = FIXED_DIM_FOR_KERNEL_DOT\n    # Operation: Z_out = Z_in @ Z_in\n    flops = 2 * M * N * K \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_kernel_dot_gbps(params: dict, ms: float) -> float:\n    M = N = K = FIXED_DIM_FOR_KERNEL_DOT\n    dtype_str = params.get('dtype_str', 'fp32') # Original signature was *fp32\n\n    current_dtype = torch.float32\n    if dtype_str == 'fp16': current_dtype = torch.float16\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    # Read Z (16,16) twice (as A and B), Write Z (16,16)\n    bytes_read_z1 = M * K * element_size\n    bytes_read_z2 = K * N * element_size \n    bytes_write_z = M * N * element_size\n    total_bytes = bytes_read_z1 + bytes_read_z2 + bytes_write_z\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"kernel_dot_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Kernel is fixed to 16x16. We can vary dtype and num_warps.\nKERNEL_DOT_DTYPES_FOR_PERF = ['fp32', 'fp16'] # bf16 if supported and desired\nKERNEL_DOT_NUM_WARPS_FOR_PERF = [1, 2, 4] \n\n@pytest.mark.parametrize(\"dtype_str\", KERNEL_DOT_DTYPES_FOR_PERF)\n@pytest.mark.parametrize(\"num_warps_launch\", KERNEL_DOT_NUM_WARPS_FOR_PERF)\n@skip_if_no_target # Use the skip if target not available\ndef test_performance(dtype_str, num_warps_launch, request):\n    set_seed()\n    \n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    # Input tensor Z (16x16)\n    # The kernel signature in ASTSource used *fp32. For perf, we might test other dtypes\n    # if the kernel JIT itself is flexible or if we compile variants.\n    # For now, assume the JIT `kernel_dot` can handle the dtype of the passed tensor.\n    Z_tensor = torch.randn((FIXED_DIM_FOR_KERNEL_DOT, FIXED_DIM_FOR_KERNEL_DOT), \n                           device='cuda', dtype=current_dtype)\n    \n    # --- Create op_lambda for benchmarking ---\n    # The kernel modifies Z_tensor in-place.\n    op_lambda = lambda: kernel_dot_triton_wrapper(Z_tensor, num_warps_launch)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=100, repetition=1000) # Kernel is tiny, need more reps\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    current_params_for_logs_and_calc = {\n        \"DIM\": FIXED_DIM_FOR_KERNEL_DOT, # M=N=K=16\n        \"dtype_str\": dtype_str,\n        \"num_warps\": num_warps_launch\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_kernel_dot_gbps,\n                                            tflops_calculator=calculate_kernel_dot_tflops)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_kernel_sub.py",
        "target_kernel_name": "kernel_sub",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `kernel_sub`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `kernel_sub`,  performs  element-wise operation: output[i] = a[i] - (b[i] * 777)\n\n**Your objective is to implement the body of `kernel_sub`.**\n\nYou must ensure that:\n1.  All arguments received by `kernel_sub` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `kernel_sub` and relevant helper utilities are provided in the context below. You only need to complete the code for `kernel_sub` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport multiprocessing\nimport shutil\nimport tempfile\nimport os\nimport pytest\n\nimport triton\nimport triton.language as tl\nfrom triton.backends.compiler import AttrsDescriptor\nfrom triton.compiler import ASTSource\n######################################## Imports ########################################\n\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef kernel_sub(a, b, o, N: tl.constexpr):\n    \"\"\"\n    Performs an element-wise operation: output[i] = a[i] - (b[i] * 777)\n    for N elements.\n\n    Parameters\n    ----------\n    a\n        Pointer to the first input tensor (minuend).\n        Its elements are loaded and used in the subtraction.\n    b\n        Pointer to the second input tensor (subtrahend).\n        Its elements are loaded, multiplied by 777, and then subtracted from 'a'.\n    o\n        Pointer to the output tensor.\n        The results of a[i] - (b[i] * 777) are stored here.\n    N : tl.constexpr\n        A compile-time constant specifying the number of elements\n        to process in tensors a, b, and o. This typically corresponds\n        to the block size or a dimension of the tensors.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ########################################\nimport multiprocessing\nimport shutil\nimport tempfile\nimport os\nimport pytest\n\nimport triton\nimport triton.language as tl\nfrom triton.backends.compiler import AttrsDescriptor\nfrom triton.compiler import ASTSource\n######################################## Imports ########################################\n\n@triton.jit\ndef kernel_sub(a, b, o, N: tl.constexpr):\n    idx = tl.arange(0, N)\n    tl.store(o + idx, tl.load(a + idx) - tl.load(b + idx) * 777)\n\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nimport multiprocessing\nimport shutil\nimport tempfile\nimport os\nimport pytest\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nimport triton\nimport triton.language as tl\nfrom triton.backends.compiler import AttrsDescriptor\nfrom triton.compiler import ASTSource\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n# --- Fixture Definition ---\n@pytest.fixture\ndef fresh_triton_cache(monkeypatch):\n    temp_dir = tempfile.mkdtemp()\n    cache_dir = os.path.join(temp_dir, \".triton\")\n    os.makedirs(cache_dir, exist_ok=True)\n    monkeypatch.setenv(\"TRITON_CACHE_DIR\", cache_dir)\n    yield cache_dir\n    shutil.rmtree(temp_dir)\n# --- End Fixture Definition ---\n\n# Check if a target is available. Skip tests if not.\ntry:\n    target = triton.runtime.driver.active.get_current_target()\n    TARGET_AVAILABLE = True\nexcept Exception:\n    TARGET_AVAILABLE = False\n    target = None\n\n# Decorator to skip tests if target is not available\nskip_if_no_target = pytest.mark.skipif(not TARGET_AVAILABLE, reason=\"Triton target not available (e.g., no GPU or CUDA/ROCm setup)\")\n\n@skip_if_no_target\ndef compile_kernel_sub_for_test(attrs): # Renamed to be specific\n    # kernel_sub is defined globally above\n    src = ASTSource(\n        fn=kernel_sub,\n        constants={'N': 32},\n        signature={'a': \"*fp32\", 'b': \"*fp32\", 'o': \"*fp32\"},\n        attrs=attrs,\n    )\n    triton.compile(src=src, target=target)\n\n@skip_if_no_target\ndef test_compile_kernel_sub_in_subproc(fresh_triton_cache, request) -> None: # Test name updated for clarity\n\n    set_seed()\n    \n    config = AttrsDescriptor.from_hints({i: 16 for i in range(4)})\n    try:\n        multiprocessing.set_start_method('fork', force=True)\n    except RuntimeError:\n        print(\"Warning: Could not force 'fork' start method. Using default.\")\n        if multiprocessing.get_start_method(allow_none=True) != 'fork': # allow_none for safety\n            pytest.skip(\"Test requires 'fork' multiprocessing start method.\")\n\n    proc = multiprocessing.Process(target=compile_kernel_sub_for_test, args=(config, ))\n    proc.start()\n    proc.join(timeout=60)\n    if proc.is_alive():\n        proc.terminate()\n        proc.join()\n        pytest.fail(\"Process timed out\")\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = torch.tensor([[0.0]]).clone().detach().cpu()\n    ################################################################### \n\n    assert proc.exitcode == 0\n\n    result_gold[sanitized_key_name] = torch.tensor([[1.0]]).clone().detach().cpu()\n\n\ndef kernel_sub_triton_wrapper(a_tensor, b_tensor, o_buffer, \n                              N_val_const, num_warps_launch): # N_val becomes constexpr N\n    grid = (1,) \n    kernel_sub[grid](\n        a_tensor, b_tensor, o_buffer, \n        N=N_val_const, \n        num_warps=num_warps_launch\n    )\n    return o_buffer \n\ndef calculate_kernel_sub_tflops(params: dict, ms: float) -> float:\n    N = params['N_val']\n    flops = 2 * N \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_kernel_sub_gbps(params: dict, ms: float) -> float:\n    N = params['N_val']\n    dtype_str = params.get('dtype_str', 'fp32') \n    current_dtype = torch.float32\n    if dtype_str == 'fp16': current_dtype = torch.float16\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    bytes_a_read, bytes_b_read, bytes_o_write = [N * element_size] * 3\n    total_bytes = bytes_a_read + bytes_b_read + bytes_o_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"kernel_sub_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# MODIFIED: N_val significantly reduced to avoid HSA_STATUS_ERROR_OUT_OF_RESOURCES\n# This kernel is not tiled, so N must be small.\n# Max N around 16384 to 32768 might be the limit for a single SM program without tiling.\n# Let's test up to 65536, but expect larger ones might fail.\nKERNEL_SUB_N_VALS_FOR_PERF = [2**i for i in range(10, 17)] # 1024 to 65536\n# KERNEL_SUB_N_VALS_FOR_PERF = [2**i for i in range(10, 21)] # Original problematic range\n\nKERNEL_SUB_DTYPES_FOR_PERF = ['fp32', 'fp16'] \nKERNEL_SUB_NUM_WARPS_FOR_PERF = [1, 2, 4] \n\n@pytest.mark.parametrize(\"N_val\", KERNEL_SUB_N_VALS_FOR_PERF)\n@pytest.mark.parametrize(\"dtype_str\", KERNEL_SUB_DTYPES_FOR_PERF)\n@pytest.mark.parametrize(\"num_warps_launch\", KERNEL_SUB_NUM_WARPS_FOR_PERF)\n@skip_if_no_target \ndef test_performance(N_val, dtype_str, num_warps_launch, request):\n    set_seed()\n    \n    # Proactive skip for very large N for this non-tiled kernel\n    # This limit is empirical and might need adjustment based on specific GPU/driver\n    # For ROCm, even 65536 might be too large without tiling for some internal resources.\n    # The error at N=524288 confirms this.\n    if N_val > 65536 * 2 : # A more conservative upper bound for non-tiled kernel\n         pytest.skip(f\"Skipping N_val={N_val} as it's too large for this non-tiled kernel_sub.\")\n\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16 \n    else: current_dtype = torch.float16\n\n    a = torch.randn(N_val, device='cuda', dtype=current_dtype)\n    b = torch.randn(N_val, device='cuda', dtype=current_dtype)\n    o_buffer = torch.empty(N_val, device='cuda', dtype=current_dtype) \n    \n    op_lambda = lambda: kernel_sub_triton_wrapper(\n        a, b, o_buffer, N_val, num_warps_launch\n    )\n\n    bench_config = do_bench_config(warm_up=100, repetition=500) \n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"N_val\": N_val, \n        \"dtype_str\": dtype_str,\n        \"num_warps\": num_warps_launch\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_kernel_sub_gbps,\n                                            tflops_calculator=calculate_kernel_sub_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_triton_flip.py",
        "target_kernel_name": "flip_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `flip_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `flip_kernel`,  Flips each row of a 2D tensor horizontally.\n\n**Your objective is to implement the body of `flip_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `flip_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `flip_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `flip_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport pytest\nimport torch\nimport triton\nimport triton.language as tl\nimport numpy as np\n######################################## Imports ########################################\n\n@triton.jit\ndef flip_kernel(X, Z, N: tl.constexpr, M: tl.constexpr):\n    \"\"\"\n    Processes 2D blocks of data, flipping each block horizontally.\n\n    This kernel loads a 2D block of data of shape (N, M) from an input tensor `X`.\n    It then flips this block along its second dimension (columns), meaning each\n    row within the block is reversed. The resulting flipped block is then\n    stored into an output tensor `Z` at the corresponding offset.\n\n    Parameters\n    ----------\n    X\n        Pointer to the input tensor. Each kernel instance will load an (N, M) block from this tensor.\n    Z\n        Pointer to the output tensor. Each kernel instance will store the flipped (N, M) block to this tensor.\n        It can be the same as X for an in-place operation if memory layout and access patterns allow.\n    N : tl.constexpr\n        A compile-time constant specifying the size of the first dimension\n        (e.g., number of rows) of the 2D data block to be processed by each kernel instance.\n    M : tl.constexpr\n        A compile-time constant specifying the size of the second dimension\n        (e.g., number of columns) of the 2D data block to be processed by each kernel instance.\n        The flip operation occurs along this dimension.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ########################################\nimport pytest\nimport torch\nimport triton\nimport triton.language as tl\nimport numpy as np\n######################################## Imports ########################################\n\n@triton.jit\ndef flip_kernel(X, Z, N: tl.constexpr, M: tl.constexpr):\n    offx = tl.arange(0, M)\n    offy = tl.arange(0, N) * M\n    off2d = offx[None, :] + offy[:, None]\n    x = tl.load(X + off2d)\n    x = tl.flip(x)\n    tl.store(Z + off2d, x)\n\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n\n# Common helper code (similar to test_triton_sort.py)\ndef np_dtype_from_str(dtype_str):\n    if dtype_str == \"bfloat16\": return np.float32\n    return getattr(np, dtype_str)\n\ndef gen_numpy_array_for_torch_conversion(shape, dtype_str, low=0, high=100):\n    np_dtype = np_dtype_from_str(dtype_str)\n    actual_low = int(low)\n    actual_high = int(high)\n\n    if dtype_str == 'bfloat16':\n        return np.random.uniform(actual_low, actual_high, shape).astype(np.float32)\n    if 'float' in dtype_str:\n        return np.random.uniform(actual_low, actual_high, shape).astype(np_dtype)\n    elif 'int' in dtype_str:\n        if actual_high <= actual_low: actual_high = actual_low + 1\n        if np.issubdtype(np_dtype, np.integer):\n            iinfo = np.iinfo(np_dtype)\n            actual_low = max(iinfo.min, actual_low)\n            effective_high = min(iinfo.max, actual_high)\n            if effective_high <= actual_low: effective_high = actual_low +1\n            if effective_high > iinfo.max : effective_high = iinfo.max\n            return np.random.randint(actual_low, effective_high + 1 if effective_high < iinfo.max else effective_high, shape, dtype=np_dtype)\n    raise ValueError(f\"Unsupported dtype_str for gen_numpy_array_for_torch_conversion: {dtype_str}\")\n\ndef torch_dtype_from_str(dtype_str):\n    if dtype_str == \"bfloat16\": return torch.bfloat16\n    if dtype_str == \"float16\": return torch.float16\n    if dtype_str == \"float32\": return torch.float32\n    if dtype_str == \"int32\": return torch.int32\n    raise ValueError(f\"Unsupported dtype_str for torch: {dtype_str}\")\n\n\n@pytest.mark.interpreter\n@pytest.mark.parametrize(\"M_cols, N_rows\", [[512, 1], [64, 8], [16, 256], [8, 512]])\n@pytest.mark.parametrize(\"dtype_str\", ['int32', 'float16', 'float32', 'bfloat16'])\ndef test_flip(N_rows, M_cols, dtype_str, request, device='cuda'):\n    set_seed()\n\n    x_np = gen_numpy_array_for_torch_conversion((N_rows, M_cols), dtype_str=dtype_str)\n    \n    torch_dtype = torch_dtype_from_str(dtype_str)\n    x = torch.from_numpy(x_np).to(dtype=torch_dtype, device=device)\n\n    y_ref = torch.flip(x, dims=(1,)) # Flip along the columns (dimension 1)\n    z_triton = torch.empty_like(x)\n\n    grid = (N_rows,) # Each program flips one row\n    flip_kernel[grid](x, z_triton, N_rows, M_cols) # num_warps removed\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = z_triton.clone().detach().cpu()\n    ################################################################### \n\n    assert torch.allclose(y_ref.float(), z_triton.float(), atol=1e-2, rtol=1e-2), \\\n        f\"Flip mismatch for dtype {dtype_str}, shape ({N_rows}, {M_cols}).\\nReference: {y_ref}\\nTriton: {z_triton}\"\n\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef flip_triton_wrapper(in_tensor, out_buffer, N_const, M_const, num_warps_launch):\n    # For benchmarking, we assume the kernel processes one NxM tile.\n    # The grid will be (1,) as the kernel is not tiled with program_id.\n    grid = (1,) \n    flip_kernel[grid](\n        in_tensor, out_buffer, \n        N=N_const, M=M_const, # Pass N, M as constexpr\n        num_warps=num_warps_launch # Launch hint\n    )\n    return out_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_flip_tflops(params: dict, ms: float) -> float:\n    return 0.0 # Memory operation\n\ndef calculate_flip_gbps(params: dict, ms: float) -> float:\n    N_rows = params['N_rows_const'] # Kernel's N constexpr (number of rows in the tile)\n    M_cols = params['M_cols_const'] # Kernel's M constexpr (number of columns in the tile)\n    dtype_str = params.get('dtype_str', 'fp32') \n\n    current_dtype = torch_dtype_from_str(dtype_str) # Use existing helper\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    elements_processed = N_rows * M_cols\n    bytes_read = elements_processed * element_size\n    bytes_write = elements_processed * element_size \n    total_bytes = bytes_read + bytes_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"triton_flip_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Kernel's N and M are constexpr. We will parametrize these.\nFLIP_TILE_SHAPES_FOR_PERF = [\n    # N_rows_const, M_cols_const (for the single tile processed by kernel)\n    (128, 512), (1024, 64)\n]\nFLIP_DTYPES_FOR_PERF = ['int32', 'float16', 'float32', 'bfloat16'] \nFLIP_NUM_WARPS_FOR_PERF = [1, 2, 4] \n\n@pytest.mark.parametrize(\"N_const, M_const\", FLIP_TILE_SHAPES_FOR_PERF)\n@pytest.mark.parametrize(\"dtype_str\", FLIP_DTYPES_FOR_PERF)\n@pytest.mark.parametrize(\"num_warps_launch\", FLIP_NUM_WARPS_FOR_PERF)\ndef test_performance(N_const, M_const, dtype_str, num_warps_launch, request, device='cuda'):\n    set_seed()\n    \n    if dtype_str == 'bfloat16':\n        cap = torch.cuda.get_device_capability()\n        if cap[0] < 8:\n            pytest.skip(\"bfloat16 requires Ampere+ (arch 80+)\")\n    \n    current_torch_dtype = torch_dtype_from_str(dtype_str)\n\n    # Input tensor `x_perf` has shape (N_const, M_const)\n    x_perf = gen_numpy_array_for_torch_conversion((N_const, M_const), dtype_str)\n    x_perf_tensor = torch.from_numpy(x_perf).to(dtype=current_torch_dtype, device=device)\n    \n    # Output buffer `z_perf_buffer` has shape (N_const, M_const)\n    z_perf_buffer = torch.empty_like(x_perf_tensor)\n    \n    op_lambda = lambda: flip_triton_wrapper(\n        x_perf_tensor, z_perf_buffer, N_const, M_const, num_warps_launch\n    )\n\n    bench_config = do_bench_config(warm_up=100, repetition=1000) # Simple kernel\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"N_rows_const\": N_const, \n        \"M_cols_const\": M_const,\n        \"dtype_str\": dtype_str,\n        \"num_warps\": num_warps_launch\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_flip_gbps,\n                                            tflops_calculator=calculate_flip_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_triton_sort.py",
        "target_kernel_name": "sort_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `sort_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `sort_kernel`,  Sorts each row of a 2D input tensor independently.\n\n**Your objective is to implement the body of `sort_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `sort_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `sort_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `sort_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport pytest\nimport torch\nimport triton\nimport triton.language as tl\nimport numpy as np\n######################################## Imports ########################################\n\n@triton.jit\ndef sort_kernel(X, Z, N: tl.constexpr, M: tl.constexpr, descending: tl.constexpr):\n    \"\"\"\n    Sorts each row of a 2D input tensor independently.\n\n    This kernel loads a 2D block of data of shape (N, M) from the input tensor X.\n    It then sorts each of the N rows (each of length M) independently.\n    The sorted rows are stored in the output tensor Z.\n    The sorting order (ascending or descending) is determined by the `descending` flag.\n\n    Parameters\n    ----------\n    X : tl.pointer_type\n        Pointer to the input tensor. The kernel expects to read a 2D block of data\n        logically arranged as N rows and M columns.\n    Z : tl.pointer_type\n        Pointer to the output tensor. The sorted 2D block of data (N rows, M columns)\n        will be stored here.\n    N : tl.constexpr\n        A compile-time constant representing the number of rows in the 2D block to be processed.\n        Each of these N rows will be sorted independently.\n    M : tl.constexpr\n        A compile-time constant representing the number of columns (elements per row)\n        in the 2D block. Sorting is performed along this dimension for each row.\n    descending : tl.constexpr\n        A compile-time constant boolean. If True, each row is sorted in descending order.\n        If False, each row is sorted in ascending order.\n    \"\"\"\n\n    # Your code here\n\n",
        "label": "######################################## Imports ########################################\nimport pytest\nimport torch\nimport triton\nimport triton.language as tl\nimport numpy as np\n######################################## Imports ########################################\n\n@triton.jit\ndef sort_kernel(X, Z, N: tl.constexpr, M: tl.constexpr, descending: tl.constexpr):\n    offx = tl.arange(0, M)\n    offy = tl.arange(0, N) * M\n    off2d = offx[None, :] + offy[:, None]\n    x = tl.load(X + off2d)\n    x = tl.sort(x, descending=descending)\n    tl.store(Z + off2d, x)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n\n# Common helper code\ndef np_dtype_from_str(dtype_str):\n    if dtype_str == \"bfloat16\": return np.float32\n    return getattr(np, dtype_str)\n\ndef gen_numpy_array_for_torch_conversion(shape, dtype_str, low=0, high=100):\n    np_dtype = np_dtype_from_str(dtype_str)\n    actual_low = int(low)\n    actual_high = int(high)\n\n    if dtype_str == 'bfloat16':\n        return np.random.uniform(actual_low, actual_high, shape).astype(np.float32)\n    if 'float' in dtype_str:\n        return np.random.uniform(actual_low, actual_high, shape).astype(np_dtype)\n    elif 'int' in dtype_str:\n        if actual_high <= actual_low: actual_high = actual_low + 1\n        if np.issubdtype(np_dtype, np.integer):\n            iinfo = np.iinfo(np_dtype)\n            actual_low = max(iinfo.min, actual_low)\n            # For randint, high is exclusive. Cap at iinfo.max, so use iinfo.max + 1 for np.random.randint\n            # but ensure it does not overflow if iinfo.max is already the true max value.\n            effective_high = min(iinfo.max, actual_high)\n            if effective_high <= actual_low: effective_high = actual_low +1 \n            if effective_high > iinfo.max : effective_high = iinfo.max # Ensure it doesn't exceed max for dtype for randint high bound\n            \n            # np.random.randint's high parameter is exclusive\n            return np.random.randint(actual_low, effective_high + 1 if effective_high < iinfo.max else effective_high, shape, dtype=np_dtype)\n\n\n    raise ValueError(f\"Unsupported dtype_str for gen_numpy_array_for_torch_conversion: {dtype_str}\")\n\ndef torch_dtype_from_str(dtype_str): # Also needed for torch.from_numpy().to(torch_dtype)\n    if dtype_str == \"bfloat16\": return torch.bfloat16\n    if dtype_str == \"float16\": return torch.float16\n    if dtype_str == \"float32\": return torch.float32\n    # No float64 in original parametrize for sort, but good to have\n    if dtype_str == \"int32\": return torch.int32\n    raise ValueError(f\"Unsupported dtype_str for torch: {dtype_str}\")\n\n\n@pytest.mark.interpreter\n@pytest.mark.parametrize(\"M_cols, N_rows\", [[512, 1], [64, 8], [16, 256], [8, 512]])\n@pytest.mark.parametrize(\"descending\", [False, True])\n@pytest.mark.parametrize(\"dtype_str\", ['int32', 'float16', 'float32', 'bfloat16'])\ndef test_sort(N_rows, M_cols, descending, dtype_str, request, device='cuda'):\n    set_seed()\n\n    x_np = gen_numpy_array_for_torch_conversion((N_rows, M_cols), dtype_str=dtype_str)\n    \n    torch_dtype = torch_dtype_from_str(dtype_str)\n    x = torch.from_numpy(x_np).to(dtype=torch_dtype, device=device) # Ensure correct torch dtype\n\n    y_ref = torch.sort(x, dim=1, descending=descending)[0]\n    z_triton = torch.empty_like(x)\n\n    # Grid is (N_rows,) since each program sorts one row\n    grid = (N_rows,)\n    sort_kernel[grid](x, z_triton, N_rows, M_cols, descending) # num_warps removed, let triton decide or set default\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = z_triton.clone().detach().cpu()\n    ################################################################### \n\n    assert torch.allclose(y_ref.float(), z_triton.float(), atol=1e-2, rtol=1e-2), \\\n        f\"Sort mismatch for dtype {dtype_str}, shape ({N_rows}, {M_cols}), descending={descending}.\\nReference: {y_ref}\\nTriton: {z_triton}\"\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef sort_triton_wrapper(x_tensor, z_buffer, N_const, M_const, descending_const, num_warps_launch):\n    # For benchmarking, we assume the kernel processes one N_const x M_const tile.\n    # The grid will be (1,) as the kernel is not tiled with program_id for larger inputs.\n    grid = (1,) \n    sort_kernel[grid](\n        x_tensor, z_buffer, \n        N=N_const, M=M_const, # Pass as constexpr\n        descending=descending_const,\n        num_warps=num_warps_launch # Launch hint\n    )\n    return z_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_sort_tflops(params: dict, ms: float) -> float:\n    # Sorting N_rows * (M_cols * log(M_cols)) comparisons approx.\n    # Each comparison is 1 FLOP. This is a rough lower bound.\n    N_rows = params['N_tile_rows'] \n    M_cols = params['M_tile_cols']\n    if M_cols <= 1: return 0.0 # No comparisons if 0 or 1 element per row\n    \n    # For tl.sort on each row of M_cols elements\n    flops_per_row = M_cols * np.log2(M_cols) if M_cols > 0 else 0 # O(M log M) comparisons\n    total_flops = N_rows * flops_per_row\n    tflops = total_flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_sort_gbps(params: dict, ms: float) -> float:\n    N_rows = params['N_tile_rows'] \n    M_cols = params['M_tile_cols']\n    dtype_str = params.get('dtype_str', 'fp32') \n\n    current_torch_dtype = torch_dtype_from_str(dtype_str)\n    element_size = torch.tensor([], dtype=current_torch_dtype).element_size()\n\n    elements_processed = N_rows * M_cols\n    bytes_read = elements_processed * element_size\n    bytes_write = elements_processed * element_size \n    total_bytes = bytes_read + bytes_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"triton_sort_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Kernel's N and M are constexpr, defining the tile size it processes.\nSORT_TILE_SHAPES_FOR_PERF = [\n    # N_tile_rows, M_tile_cols (for the single tile processed by kernel)\n    (1, 1024), (1, 4096), (1, 8192), (1, 16384), # Sorting long single rows\n    (8, 512), (8, 1024), (8, 2048),             # Sorting a few medium rows\n    (64, 128), (64, 256),            # Sorting more, shorter rows\n    (256, 64), (256, 128), (256, 256)\n]\nSORT_DTYPES_FOR_PERF = ['int32', 'float16', 'float32', 'bfloat16'] \nSORT_DESCENDING_FOR_PERF = [False, True]\n# NUM_WARPS_FOR_PERF = [1, 2, 4, 8]\n\n@pytest.mark.parametrize(\"N_tile_rows, M_tile_cols\", SORT_TILE_SHAPES_FOR_PERF)\n@pytest.mark.parametrize(\"descending_val\", SORT_DESCENDING_FOR_PERF)\n@pytest.mark.parametrize(\"dtype_str\", SORT_DTYPES_FOR_PERF)\n# @pytest.mark.parametrize(\"num_warps_launch\", NUM_WARPS_FOR_PERF) # Optional\ndef test_performance(N_tile_rows, M_tile_cols, descending_val, dtype_str, request, device='cuda'):\n    # num_warps_launch = 4 # Or from parametrize\n    \n    if dtype_str == 'bfloat16':\n        cap = torch.cuda.get_device_capability()\n        if cap[0] < 8:\n            pytest.skip(\"bfloat16 requires Ampere+ (arch 80+)\")\n    \n    set_seed()\n    current_torch_dtype = torch_dtype_from_str(dtype_str)\n\n    # Input tensor `x_perf` has shape (N_tile_rows, M_tile_cols)\n    # as the kernel is designed to process one such tile.\n    x_np_perf = gen_numpy_array_for_torch_conversion((N_tile_rows, M_tile_cols), dtype_str)\n    x_perf_tensor = torch.from_numpy(x_np_perf).to(dtype=current_torch_dtype, device=device)\n    \n    z_perf_buffer = torch.empty_like(x_perf_tensor)\n    \n    op_lambda = lambda: sort_triton_wrapper(\n        x_perf_tensor, z_perf_buffer, \n        N_tile_rows, M_tile_cols, descending_val,\n        num_warps_launch=4 # Example, can be parametrized\n    )\n\n    bench_config = do_bench_config(warm_up=50, repetition=200) # Sorting can vary\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"N_tile_rows\": N_tile_rows, \n        \"M_tile_cols\": M_tile_cols,\n        \"descending\": descending_val,\n        \"dtype_str\": dtype_str,\n        \"num_warps\": 4 # Log the fixed num_warps\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_sort_gbps,\n                                            tflops_calculator=calculate_sort_tflops)\n\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_triton_swizzle2d.py",
        "target_kernel_name": "swizzle2d_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `swizzle2d_kernel`. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `swizzle2d_kernel`,  perform 2D swizzling operation.\n\n**Your objective is to implement the body of `swizzle2d_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `swizzle2d_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definition for `swizzle2d_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `swizzle2d_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport pytest\nimport torch\nimport triton\nimport triton.language as tl\nimport numpy as np\n######################################## Imports ########################################\n@triton.jit\ndef swizzle2d_kernel(output, size_i, size_j, size_g):\n    \"\"\"\n    Performs a 2D swizzling operation and stores the original linear index\n    at the new swizzled memory location. You can use tl.swizzle2d\n\n    Args:\n        output (tl.pointer_type): Pointer to the output tensor where the results\n            will be stored. This tensor should be large enough to hold\n            `size_i * size_j` elements. The elements stored will be the\n            original linear indices.\n        size_i (int): The size of the first dimension of the 2D grid to be\n            swizzled. This is equivalent to the number of rows in the conceptual\n            input matrix.\n        size_j (int): The size of the second dimension of the 2D grid to be\n            swizzled. This is equivalent to the number of columns in the\n            conceptual input matrix.\n        size_g (int): The group size used for the swizzling operation. This\n            parameter controls the granularity of the permutation.\n            Elements within a group of this size along one dimension are\n            interleaved with elements from other groups. Typically a power of 2.\n    \"\"\"\n    pass\n\n    # Your code here\n\n",
        "label": "######################################## Imports ########################################\nimport pytest\nimport torch\nimport triton\nimport triton.language as tl\n# numpy is not strictly needed here if output is directly compared\n######################################## Imports ########################################\n\n@triton.jit\ndef swizzle2d_kernel(output, size_i, size_j, size_g):\n    for i in tl.range(0, size_i, 1):\n        for j in tl.range(0, size_j, 1):\n            new_i, new_j = tl.swizzle2d(i, j, size_i, size_j, size_g)\n            tl.store(output + new_i * size_j + new_j, i * size_j + j)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n\n\n@pytest.mark.interpreter\n@pytest.mark.parametrize(\"size_i, size_j, size_g\", [[5, 7, 3]])\ndef test_swizzle2d(size_i, size_j, size_g, request, device='cuda'):\n    # Output tensor to store results, initialized to a value like -1 to see what's written\n\n    set_seed()\n    \n    output = torch.zeros(size_i, size_j).to(device)\n    swizzle2d_kernel[(1, )](output, size_i, size_j, size_g)\n    expected_order = torch.tensor([[0, 3, 6, 9, 12, 15, 18], [1, 4, 7, 10, 13, 16, 19], [2, 5, 8, 11, 14, 17, 20],\n                                   [21, 23, 25, 27, 29, 31, 33], [22, 24, 26, 28, 30, 32, 34]]).to(device)\n    \n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = output.clone().detach().cpu()\n    ################################################################### \n\n    assert (output == expected_order).all(), (output, expected_order)\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef swizzle2d_triton_wrapper(output_buffer, size_i_k, size_j_k, size_g_k, num_warps_launch):\n    grid = (1,) # Kernel is sequential, not tiled by program_id\n    swizzle2d_kernel[grid](\n        output_buffer, \n        size_i_k, size_j_k, size_g_k, # Runtime args to kernel\n        num_warps=num_warps_launch # Launch hint\n    )\n    return output_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_swizzle2d_tflops(params: dict, ms: float) -> float:\n    # tl.swizzle2d involves bitwise ops (shifts, xors, ands) and some arithmetic.\n    # A rough estimate might be ~10-20 simple ops per (i,j) pair.\n    # Number of pairs = size_i * size_j\n    size_i = params['size_i_kernel']\n    size_j = params['size_j_kernel']\n    ops_per_swizzle = 15 # Rough estimate\n    total_ops = size_i * size_j * ops_per_swizzle\n    tflops = total_ops / (ms / 1000) / 1e12 # These are integer ops, not float ops\n    return tflops # Reporting \"integer TOPS\" effectively\n\ndef calculate_swizzle2d_gbps(params: dict, ms: float) -> float:\n    size_i = params['size_i_kernel']\n    size_j = params['size_j_kernel']\n    dtype_str = params.get('output_dtype_str', 'int32') # Kernel stores integers\n\n    current_dtype = torch.int32 # Default\n    if dtype_str == 'int64': current_dtype = torch.int64\n    # Add other int types if parametrized\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    # Writes size_i * size_j elements to output. No global memory reads for input data.\n    elements_written = size_i * size_j\n    total_bytes = elements_written * element_size\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"triton_swizzle2d_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# Kernel processes one tile of size_i x size_j sequentially.\nSWIZZLE_SHAPES_FOR_PERF = [\n    # size_i_kernel, size_j_kernel, size_g_kernel\n    (32, 32, 4), (64, 64, 8), (128, 128, 16),\n    (256, 256, 16), (256, 256, 32),\n    (512, 512, 32), #(512, 512, 64) -> size_g should be < size_i, size_j for typical swizzle\n    (512, 64, 8), (64, 512, 8)\n]\nSWIZZLE_DTYPES_FOR_PERF = ['int32', 'int64'] # Output tensor dtype\n# NUM_WARPS_FOR_PERF = [1, 2, 4] # Kernel is sequential, num_warps might not have big impact\n\n@pytest.mark.parametrize(\"size_i_k, size_j_k, size_g_k\", SWIZZLE_SHAPES_FOR_PERF)\n@pytest.mark.parametrize(\"output_dtype_str\", SWIZZLE_DTYPES_FOR_PERF)\n# @pytest.mark.parametrize(\"num_warps_launch\", NUM_WARPS_FOR_PERF)\ndef test_performance(size_i_k, size_j_k, size_g_k, output_dtype_str, request, device='cuda'):\n    # num_warps_launch = 4 # Or from parametrize\n    set_seed()\n    \n    # Ensure size_g is valid for swizzle2d (typically power of 2, < size_i, size_j)\n    # tl.swizzle2d doesn't strictly require power of 2 for size_g but it's common.\n    # It does require size_g > 0.\n    if not (size_g_k > 0 and size_g_k <= size_i_k and size_g_k <= size_j_k):\n        pytest.skip(f\"Invalid size_g={size_g_k} for tile ({size_i_k}, {size_j_k})\")\n\n    if output_dtype_str == 'int64': current_out_dtype = torch.int64\n    else: current_out_dtype = torch.int32\n        \n    # Output buffer. Kernel stores original flattened indices (integers).\n    output_buffer = torch.empty((size_i_k * size_j_k,), dtype=current_out_dtype, device=device)\n    # The kernel writes to output_ptr + new_i * size_j + new_j.\n    # If output_ptr is 1D, this is fine. If output_ptr is 2D, strides are needed.\n    # The original test passed a 2D tensor to kernel. Let's match that.\n    # Kernel expects output_ptr to be a flat pointer essentially, and calculates 2D offsets.\n    # So, a 1D buffer for output_ptr is fine if kernel does `output_ptr + flat_offset`.\n    # The kernel does `output_ptr + new_i * size_j + new_j`. This implies `output_ptr` is base of 2D array.\n    # So, `output_buffer` should be 2D for the kernel call.\n    output_buffer_2d = torch.empty((size_i_k, size_j_k), dtype=current_out_dtype, device=device)\n\n\n    op_lambda = lambda: swizzle2d_triton_wrapper(\n        output_buffer_2d, # Pass the 2D buffer\n        size_i_k, size_j_k, size_g_k,\n        num_warps_launch=4 \n    )\n\n    # This kernel is very fast for small sizes, might need many reps.\n    bench_config = do_bench_config(warm_up=100, repetition=1000 if size_i_k*size_j_k < 256*256 else 200) \n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"size_i_kernel\": size_i_k, \n        \"size_j_kernel\": size_j_k,\n        \"size_g_kernel\": size_g_k,\n        \"output_dtype_str\": output_dtype_str,\n        \"num_warps\": 4 \n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_swizzle2d_gbps,\n                                            tflops_calculator=calculate_swizzle2d_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_random_int.py",
        "target_kernel_name": "randint_kernel_runtime_seed,randint_kernel_const_seed",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `randint_kernel_runtime_seed,randint_kernel_const_seed` kernels. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThese kernels, `randint_kernel_runtime_seed,randint_kernel_const_seed`,  tests both ways Triton can handle arguments: Ensuring that tl.randint works correctly whether the seed is provided as a runtime variable or a compile-time constant\n\n**Your objective is to implement the body of both the kernels `randint_kernel_runtime_seed,randint_kernel_const_seed`.**\n\nYou must ensure that:\n1.  All arguments received by `randint_kernel_runtime_seed and randint_kernel_const_seed` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `randint_kernel_runtime_seed,randint_kernel_const_seed` and relevant helper utilities are provided in the context below. You only need to complete the code for `randint_kernel_runtime_seed,randint_kernel_const_seed` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\nBLOCK: tl.constexpr = 1024\n######################################## Imports ######################################## \n\n#####################################\n# Triton Kernels for randint\n#####################################\n\n@triton.jit\ndef randint_kernel_runtime_seed(X, N, seed_val):\n    \"\"\"\n    Generates random integers and stores them in the output tensor X,\n    using a seed value provided as a runtime argument.\n\n    Each program instance computes a block of random numbers.\n    The kernel is designed to test the tl.randint function with a runtime seed.\n\n    Args:\n        X (tl.tensor): Pointer to the output tensor where random integers will be stored.\n                       The data type of X's elements is used for pid consistency if X is an integer type.\n        N (int): The total number of random integers to generate. This is used to mask\n                 out-of-bounds writes.\n        seed_val (int): The seed value for the random number generator, passed as a runtime argument.\n                        This seed is used by tl.randint to initialize its state.\n    \"\"\"\n    # Your code here\n\n\n@triton.jit\ndef randint_kernel_const_seed(X, N, seed_val: tl.constexpr):\n    \"\"\"\n    Generates random integers and stores them in the output tensor X,\n    using a seed value provided as a compile-time constant.\n\n    Each program instance computes a block of random numbers.\n    The kernel is designed to test the tl.randint function with a compile-time constant seed.\n\n    Args:\n        X (tl.tensor): Pointer to the output tensor where random integers will be stored.\n                       The data type of X's elements is used for pid consistency if X is an integer type.\n        N (int): The total number of random integers to generate. This is used to mask\n                 out-of-bounds writes.\n        seed_val (tl.constexpr): The seed value for the random number generator, passed as a\n                                 compile-time constant. This seed is used by tl.randint\n                                 to initialize its state.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n#####################################\n# Triton Kernels for randint\n#####################################\n\nBLOCK: tl.constexpr = 1024\n\n@triton.jit\ndef randint_kernel_runtime_seed(X, N, seed_val): # Kernel for runtime seed\n    pid = tl.program_id(0).to(X.dtype.element_ty) # pid uses X's dtype for consistency if X is int\n    offset = pid * BLOCK + tl.arange(0, BLOCK)\n    rand = tl.randint(seed_val, offset)\n    tl.store(X + offset, rand, mask=offset < N)\n\n@triton.jit\ndef randint_kernel_const_seed(X, N, seed_val: tl.constexpr): # Kernel for const seed\n    pid = tl.program_id(0).to(X.dtype.element_ty) # pid uses X's dtype for consistency\n    offset = pid * BLOCK + tl.arange(0, BLOCK)\n    rand = tl.randint(seed_val, offset)\n    tl.store(X + offset, rand, mask=offset < N)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nimport triton\nimport triton.language as tl\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\nBLOCK: tl.constexpr = 1024\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n#####################################\n# Reference Philox Implementation\n#####################################\n\n\nclass PhiloxConfig:\n\n    def __init__(self, PHILOX_ROUND_A, PHILOX_ROUND_B, PHILOX_KEY_A, PHILOX_KEY_B, DTYPE):\n        self.PHILOX_ROUND_A = np.array(PHILOX_ROUND_A, dtype=DTYPE)\n        self.PHILOX_ROUND_B = np.array(PHILOX_ROUND_B, dtype=DTYPE)\n        self.PHILOX_KEY_A = np.array(PHILOX_KEY_A, dtype=DTYPE)\n        self.PHILOX_KEY_B = np.array(PHILOX_KEY_B, dtype=DTYPE)\n        self.DTYPE = DTYPE\n\n\n# This is better for GPU\nPHILOX_32 = PhiloxConfig(\n    PHILOX_KEY_A=0x9E3779B9,\n    PHILOX_KEY_B=0xBB67AE85,\n    PHILOX_ROUND_A=0xD2511F53,\n    PHILOX_ROUND_B=0xCD9E8D57,\n    DTYPE=np.uint32,\n)\n\n# This is what numpy implements\nPHILOX_64 = PhiloxConfig(\n    PHILOX_KEY_A=0x9E3779B97F4A7C15,\n    PHILOX_KEY_B=0xBB67AE8584CAA73B,\n    PHILOX_ROUND_A=0xD2E7470EE14C6C93,\n    PHILOX_ROUND_B=0xCA5A826395121157,\n    DTYPE=np.uint64,\n)\n\n\nclass CustomPhilox4x:\n\n    def __init__(self, seed, config):\n        self._config = config\n        seed = self._into_pieces(seed)\n        self._key = np.array(seed[:2], dtype=self._dtype)\n        self._counter = np.array((0, 0) + seed[2:], dtype=self._dtype)\n\n    @property\n    def _dtype(self):\n        return self._config.DTYPE\n\n    def _into_pieces(self, n, pad=4):\n        res = []\n        bits = np.dtype(self._dtype).itemsize * 8\n        while len(res) < pad:\n            res.append(np.array((n & ((1 << bits) - 1)), dtype=self._dtype))\n            n >>= bits\n        assert n == 0\n        return tuple(res)\n\n    def _multiply_low_high(self, a, b):\n        low = a * b\n        high = int(a) * int(b)\n        high = np.array(high >> (np.dtype(self._dtype).itemsize * 8), dtype=self._dtype)\n        return low, high\n\n    def _single_round(self, counter, key):\n        lo0, hi0 = self._multiply_low_high(self._config.PHILOX_ROUND_A, counter[0])\n        lo1, hi1 = self._multiply_low_high(self._config.PHILOX_ROUND_B, counter[2])\n        ret0 = hi1 ^ counter[1] ^ key[0]\n        ret1 = lo1\n        ret2 = hi0 ^ counter[3] ^ key[1]\n        ret3 = lo0\n        return np.array([ret0, ret1, ret2, ret3], dtype=self._dtype)\n\n    def _raise_key(self, key):\n        pk = [self._config.PHILOX_KEY_A, self._config.PHILOX_KEY_B]\n        return key + np.array(pk, dtype=self._dtype)\n\n    def random_raw(self):\n        counter = self._counter\n        key = self._key\n        for _ in range(10):\n            counter = self._single_round(counter, key)\n            key = self._raise_key(key)\n        self.advance(1)\n        return counter\n\n    def advance(self, n_steps):\n        self._counter[0] += n_steps\n        assert self._counter[0] < 2**32, \"FIXME: doesn't work for large offsets\"\n\n\nclass CustomPhilox(CustomPhilox4x):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.buffer = []\n\n    def random_raw(self):\n        if len(self.buffer) == 0:\n            self.buffer = list(super().random_raw())[::-1]\n        return int(self.buffer.pop())\n\n\n#####################################\n# Unit Test for randint\n#####################################\n\n@pytest.mark.interpreter\n@pytest.mark.parametrize('size, seed, dtype, const_seed', [(size, seed, dtype, const_seed)\n                                                           for size_str in ['10', '4,53', '400'] # Renamed to size_str\n                                                           for size in [[int(s) for s in size_str.split(',')]] # Process size here\n                                                           for seed in [0, 42, 124, 54, 0xffffffff, 0x0000000fcafeb0ba]\n                                                           for dtype in ['int32', 'int64']\n                                                           for const_seed in [True, False]])\ndef test_randint(size, seed, dtype, const_seed, request, device='cuda'):\n    # size = list(map(int, size.split(','))) # Moved to parametrize\n    set_seed()\n\n    torch_dtype = getattr(torch, dtype)\n    numpy_dtype = getattr(np, f\"u{dtype}\") # Philox generates unsigned integers\n    config = {'int32': PHILOX_32, 'int64': PHILOX_64}[dtype]\n\n    # triton result\n    x = torch.empty(size, dtype=torch_dtype, device=device)\n    N = x.numel()\n    grid = (triton.cdiv(N, BLOCK), )\n    if N > 0: # Ensure grid is not (0,) if N is 0\n        if const_seed:\n            randint_kernel_const_seed[grid](x, N, seed_val=seed)\n        else:\n            randint_kernel_runtime_seed[grid](x, N, seed_val=seed)\n    \n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = x.clone().detach().cpu()\n    ################################################################### \n\n    out_tri = x.cpu().numpy().astype(numpy_dtype).flatten().tolist()\n    \n    # reference result\n    if N > 0:\n        gen = CustomPhilox4x(seed, config=config)\n        out_ref = [gen.random_raw()[0] for _ in out_tri]\n        assert out_tri == out_ref\n    else:\n        assert len(out_tri) == 0 # If N is 0, out_tri should be empty\n\n\n\n\n# --- Python wrapper for launching randint kernels ---\ndef randint_triton_wrapper(X_buffer, N_elements, seed_val, \n                           is_const_seed: bool, \n                           num_warps_launch):\n    grid = (triton.cdiv(N_elements, PYTHON_BLOCK_SIZE_CONST_randint),)\n    \n    if is_const_seed:\n        randint_kernel_const_seed[grid](\n            X_buffer, N_elements, \n            seed_val=seed_val, # Passed as constexpr\n            num_warps=num_warps_launch\n        )\n    else:\n        randint_kernel_runtime_seed[grid](\n            X_buffer, N_elements, \n            seed_val=seed_val, # Passed as runtime arg\n            num_warps=num_warps_launch\n        )\n    return X_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_randint_tflops(params: dict, ms: float) -> float:\n    N = params['N_elements']\n    # Philox RNG involves ~10 rounds, each with integer ops.\n    # Very rough estimate, similar to tl.rand.\n    ops_per_rand_int = 75 \n    flops = N * ops_per_rand_int # Treat integer ops as \"FLOP-equivalents\" for this metric\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_randint_gbps(params: dict, ms: float) -> float:\n    N = params['N_elements']\n    dtype_str = params.get('output_dtype_str', 'int32') \n\n    current_dtype = torch.int32 # Default\n    if dtype_str == 'int64': current_dtype = torch.int64\n    # Add other int types if parametrized (e.g. int8, int16)\n    \n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    bytes_x_write = N * element_size\n    total_bytes = bytes_x_write # Only considering output write bandwidth\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"triton_randint_perf\"\n\n# --- Pytest parametrize for performance testing ---\nRANDINT_SIZES_FOR_PERF = [2**i for i in range(16, 24)] # 65536 to 8,388,608 elements\nRANDINT_SEEDS_FOR_PERF = [0, 42] \nRANDINT_OUTPUT_DTYPES_FOR_PERF = ['int32', 'int64'] \nRANDINT_CONST_SEED_BOOL_FOR_PERF = [True, False]\n# NUM_WARPS_FOR_PERF = [4, 8] \nPYTHON_BLOCK_SIZE_CONST_randint = 1024\n\n@pytest.mark.parametrize(\"size_val\", RANDINT_SIZES_FOR_PERF)\n@pytest.mark.parametrize(\"seed_val\", RANDINT_SEEDS_FOR_PERF)\n@pytest.mark.parametrize(\"output_dtype_str\", RANDINT_OUTPUT_DTYPES_FOR_PERF)\n@pytest.mark.parametrize(\"const_seed_bool\", RANDINT_CONST_SEED_BOOL_FOR_PERF)\n# @pytest.mark.parametrize(\"num_warps_val\", NUM_WARPS_FOR_PERF)\ndef test_performance(size_val, seed_val, output_dtype_str, const_seed_bool, request, device='cuda'):\n    # num_warps_val = 4 # Or from parametrize\n    set_seed() \n    \n    if output_dtype_str == 'int64':\n        current_out_dtype = torch.int64\n    else: # Default to int32\n        current_out_dtype = torch.int32\n        \n    x_output_buffer = torch.empty(size_val, dtype=current_out_dtype, device=device)\n    N_elements = x_output_buffer.numel()\n        \n    op_lambda = lambda: randint_triton_wrapper(\n        x_output_buffer, N_elements, seed_val,\n        const_seed_bool,\n        num_warps_launch=4 \n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100) \n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"N_elements\": N_elements, \n        \"seed\": seed_val,\n        \"output_dtype_str\": output_dtype_str, # Dtype of the output tensor X\n        \"const_seed\": const_seed_bool,\n        \"num_warps\": 4 \n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_randint_gbps,\n                                            tflops_calculator=calculate_randint_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n\n    \n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_randn.py",
        "target_kernel_name": "randn_kernel_runtime_seed,randn_kernel_const_seed",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `randn_kernel_runtime_seed,randn_kernel_const_seed` kernels. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThese kernels, `randn_kernel_runtime_seed,randn_kernel_const_seed`,  tests both ways Triton can handle arguments: Ensuring that tl.randn works correctly whether the seed is provided as a runtime variable or a compile-time constant\n\n**Your objective is to implement the body of both the kernels `randn_kernel_runtime_seed,randn_kernel_const_seed`.**\n\nYou must ensure that:\n1.  All arguments received by `randn_kernel_runtime_seed and randn_kernel_const_seed` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `randn_kernel_runtime_seed,randn_kernel_const_seed` and relevant helper utilities are provided in the context below. You only need to complete the code for `randn_kernel_runtime_seed,randn_kernel_const_seed` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n#####################################\n# Triton Kernels for randn\n#####################################\n\nBLOCK: tl.constexpr = 1024\n\n@triton.jit\ndef randn_kernel_runtime_seed(X: tl.pointer_type, N: tl.int32, seed: tl.int66, dtype: tl.constexpr):\n    \"\"\"\n    Generates random numbers and stores them into an output tensor.\n    The seed for the random number generator is passed at runtime.\n\n    :param X: Pointer to the output tensor where random numbers will be stored (e.g., *tl.float32).\n    :param N: Total number of elements to generate.\n    :param seed: Seed for the random number generator (runtime value, typically int64 or int66).\n    :param dtype: Data type used for kernel indexing/calculations (compile-time constant, e.g., tl.int32).\n    \"\"\"\n    # Your code here\n\n\n@triton.jit\ndef randn_kernel_const_seed(X: tl.pointer_type, N: tl.int32, seed: tl.constexpr, dtype: tl.constexpr):\n    \"\"\"\n    Generates random numbers and stores them into an output tensor.\n    The seed for the random number generator must be a compile-time constant.\n\n    :param X: Pointer to the output tensor where random numbers will be stored (e.g., *tl.float32).\n    :param N: Total number of elements to generate.\n    :param seed: Seed for the random number generator (must be a compile-time constant).\n    :param dtype: Data type used for kernel indexing/calculations (compile-time constant, e.g., tl.int32).\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport numpy as np\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n#####################################\n# Triton Kernels for randint\n#####################################\n\nBLOCK: tl.constexpr = 1024\n\n@triton.jit\ndef randn_kernel_runtime_seed(X, N, seed, dtype: tl.constexpr):\n    pid = tl.program_id(0).to(dtype)\n    offset = pid * BLOCK + tl.arange(0, BLOCK)\n    rand = tl.rand(seed, offset)\n    tl.store(X + offset, rand, mask=offset < N)\n\n@triton.jit\ndef randn_kernel_const_seed(X, N, seed: tl.constexpr, dtype: tl.constexpr):\n    pid = tl.program_id(0).to(dtype)\n    offset = pid * BLOCK + tl.arange(0, BLOCK)\n    rand = tl.rand(seed, offset)\n    tl.store(X + offset, rand, mask=offset < N)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nimport triton\nimport scipy.stats\nimport triton.language as tl\n\nresult_gold = {}\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\nBLOCK: tl.constexpr = 1024\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n#####################################\n# Reference Philox Implementation\n#####################################\n\n\nclass PhiloxConfig:\n\n    def __init__(self, PHILOX_ROUND_A, PHILOX_ROUND_B, PHILOX_KEY_A, PHILOX_KEY_B, DTYPE):\n        self.PHILOX_ROUND_A = np.array(PHILOX_ROUND_A, dtype=DTYPE)\n        self.PHILOX_ROUND_B = np.array(PHILOX_ROUND_B, dtype=DTYPE)\n        self.PHILOX_KEY_A = np.array(PHILOX_KEY_A, dtype=DTYPE)\n        self.PHILOX_KEY_B = np.array(PHILOX_KEY_B, dtype=DTYPE)\n        self.DTYPE = DTYPE\n\n\n# This is better for GPU\nPHILOX_32 = PhiloxConfig(\n    PHILOX_KEY_A=0x9E3779B9,\n    PHILOX_KEY_B=0xBB67AE85,\n    PHILOX_ROUND_A=0xD2511F53,\n    PHILOX_ROUND_B=0xCD9E8D57,\n    DTYPE=np.uint32,\n)\n\n# This is what numpy implements\nPHILOX_64 = PhiloxConfig(\n    PHILOX_KEY_A=0x9E3779B97F4A7C15,\n    PHILOX_KEY_B=0xBB67AE8584CAA73B,\n    PHILOX_ROUND_A=0xD2E7470EE14C6C93,\n    PHILOX_ROUND_B=0xCA5A826395121157,\n    DTYPE=np.uint64,\n)\n\n\nclass CustomPhilox4x:\n\n    def __init__(self, seed, config):\n        self._config = config\n        seed = self._into_pieces(seed)\n        self._key = np.array(seed[:2], dtype=self._dtype)\n        self._counter = np.array((0, 0) + seed[2:], dtype=self._dtype)\n\n    @property\n    def _dtype(self):\n        return self._config.DTYPE\n\n    def _into_pieces(self, n, pad=4):\n        res = []\n        bits = np.dtype(self._dtype).itemsize * 8\n        while len(res) < pad:\n            res.append(np.array((n & ((1 << bits) - 1)), dtype=self._dtype))\n            n >>= bits\n        assert n == 0\n        return tuple(res)\n\n    def _multiply_low_high(self, a, b):\n        low = a * b\n        high = int(a) * int(b)\n        high = np.array(high >> (np.dtype(self._dtype).itemsize * 8), dtype=self._dtype)\n        return low, high\n\n    def _single_round(self, counter, key):\n        lo0, hi0 = self._multiply_low_high(self._config.PHILOX_ROUND_A, counter[0])\n        lo1, hi1 = self._multiply_low_high(self._config.PHILOX_ROUND_B, counter[2])\n        ret0 = hi1 ^ counter[1] ^ key[0]\n        ret1 = lo1\n        ret2 = hi0 ^ counter[3] ^ key[1]\n        ret3 = lo0\n        return np.array([ret0, ret1, ret2, ret3], dtype=self._dtype)\n\n    def _raise_key(self, key):\n        pk = [self._config.PHILOX_KEY_A, self._config.PHILOX_KEY_B]\n        return key + np.array(pk, dtype=self._dtype)\n\n    def random_raw(self):\n        counter = self._counter\n        key = self._key\n        for _ in range(10):\n            counter = self._single_round(counter, key)\n            key = self._raise_key(key)\n        self.advance(1)\n        return counter\n\n    def advance(self, n_steps):\n        self._counter[0] += n_steps\n        assert self._counter[0] < 2**32, \"FIXME: doesn't work for large offsets\"\n\n\nclass CustomPhilox(CustomPhilox4x):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.buffer = []\n\n    def random_raw(self):\n        if len(self.buffer) == 0:\n            self.buffer = list(super().random_raw())[::-1]\n        return int(self.buffer.pop())\n\n\n@pytest.mark.interpreter\n@pytest.mark.parametrize('size, seed, dtype, const_seed', [(size, seed, dtype, const_seed)\n                                                           for size in [100000]\n                                                           for seed in [0, 42, 124, 54]\n                                                           for dtype in ['int32', 'int64']\n                                                           for const_seed in [True, False]])\ndef test_rand(size, seed, dtype, const_seed,  request, device='cuda'):\n\n\n    # triton result\n    set_seed(seed)\n\n    x = torch.empty(size, dtype=torch.float32, device=device)\n    N = x.numel()\n    grid = (triton.cdiv(N, BLOCK), )\n    if const_seed:\n        randn_kernel_const_seed[grid](x, N, seed=seed, dtype=getattr(tl, dtype))\n    else:\n        randn_kernel_runtime_seed[grid](x, N, seed, dtype=getattr(tl, dtype))\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = x.clone().detach().cpu()\n    ################################################################### \n\n    assert all((x >= 0) & (x <= 1))\n    assert scipy.stats.kstest(x.tolist(), 'uniform', args=(0, 1)).statistic < 0.01\n\n\n\n# tl.rand() should never produce >=1.0\n\n\ndef randn_triton_wrapper(X_buffer, N_elements, seed_val, \n                         is_const_seed: bool, tl_dtype_for_kernel_arg, # This is the tl.dtype \n                         num_warps_launch):\n    grid = (triton.cdiv(N_elements, PYTHON_BLOCK_SIZE_CONST),)\n    \n    if is_const_seed:\n        randn_kernel_const_seed[grid](\n            X_buffer, N_elements, \n            seed=seed_val, \n            dtype=tl_dtype_for_kernel_arg, # *** CORRECTED: Use 'dtype' as keyword arg ***\n            num_warps=num_warps_launch\n        )\n    else:\n        randn_kernel_runtime_seed[grid](\n            X_buffer, N_elements, \n            seed_val, \n            dtype=tl_dtype_for_kernel_arg, # *** CORRECTED: Use 'dtype' as keyword arg ***\n            num_warps=num_warps_launch\n        )\n    return X_buffer\n\n# --- TFLOPS and GB/s calculators (Unchanged from previous correct version) ---\ndef calculate_randn_tflops(params: dict, ms: float) -> float:\n    N = params['N_elements']\n    ops_per_rand = 75 \n    flops = N * ops_per_rand\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_randn_gbps(params: dict, ms: float) -> float:\n    N = params['N_elements']\n    element_size = torch.tensor([], dtype=torch.float32).element_size() # tl.rand output is float\n    bytes_x_write = N * element_size\n    total_bytes = bytes_x_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"triton_rand_perf\"\n\n# --- Pytest parametrize for performance testing (Unchanged from previous correct version) ---\nKERNEL_SUB_N_VALS_FOR_PERF = [2**i for i in range(10, 21)] # 1024 to 1,048,576 (original had KERNEL_SUB_... this should be RAND_...)\nRAND_SIZES_FOR_PERF = [2**i for i in range(10, 23)] # Up to 4M elements, adjust if too slow/large\nRAND_SEEDS_FOR_PERF = [0, 42] \nRAND_OFFSET_DTYPES_FOR_PERF = ['int32', 'int64'] \nRAND_CONST_SEED_BOOL_FOR_PERF = [True, False]\n# NUM_WARPS_FOR_PERF = [4, 8] \nPYTHON_BLOCK_SIZE_CONST = 1024\n\n@pytest.mark.parametrize(\"size_val\", RAND_SIZES_FOR_PERF)\n@pytest.mark.parametrize(\"seed_val\", RAND_SEEDS_FOR_PERF)\n@pytest.mark.parametrize(\"offset_tl_dtype_str\", RAND_OFFSET_DTYPES_FOR_PERF)\n@pytest.mark.parametrize(\"const_seed_bool\", RAND_CONST_SEED_BOOL_FOR_PERF)\n# @pytest.mark.parametrize(\"num_warps_val\", NUM_WARPS_FOR_PERF) # Can add back if desired\ndef test_performance(size_val, seed_val, offset_tl_dtype_str, const_seed_bool, request, device='cuda'):\n    # num_warps_val = 4 # Or from parametrize\n    set_seed() \n    \n    x_output_buffer = torch.empty(size_val, dtype=torch.float32, device=device)\n    N_elements = x_output_buffer.numel()\n    \n    if offset_tl_dtype_str == 'int64':\n        tl_dtype_for_offset_arg = tl.int64\n    else: \n        tl_dtype_for_offset_arg = tl.int32\n        \n    op_lambda = lambda: randn_triton_wrapper(\n        x_output_buffer, N_elements, seed_val,\n        const_seed_bool, tl_dtype_for_offset_arg, # Pass the tl.dtype object\n        num_warps_launch=4 \n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100) \n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"N_elements\": N_elements, \n        \"seed\": seed_val,\n        \"offset_dtype\": offset_tl_dtype_str, # Log the string\n        \"const_seed\": const_seed_bool,\n        \"output_dtype_str\": \"float32\",\n        \"num_warps\": 4 # Log the fixed num_warps\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_randn_gbps,\n                                            tflops_calculator=calculate_randn_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n\n    \n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_matmul_MXFP.py",
        "target_kernel_name": "matmul_kernel,mxfp_to_bf16_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `matmul_kernel,mxfp_to_bf16_kernel` kernels. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThese kernels, `matmul_kernel,mxfp_to_bf16_kernel`,   performs matrix multiplication (C = A @ B) with optional support for scaled MXFP (Microscaling Format) inputs.\n\n**Your objective is to implement the body of both the kernels `matmul_kernel,mxfp_to_bf16_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_kernel and mxfp_to_bf16_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `matmul_kernel,mxfp_to_bf16_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `matmul_kernel,mxfp_to_bf16_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, scale_ptr, b_ptr, output_ptr,\n    M, N, K_MXFP,\n    stride_am, stride_ak,\n    stride_sm, stride_sk,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    NUM_STAGES: tl.constexpr, a_type: tl.constexpr, b_type: tl.constexpr\n):\n    \"\"\"\n    Performs a matrix multiplication (C = A @ B) with optional support for scaled MXFP (Microscaling Format) inputs.\n    If `a_type` and `b_type` are provided, it performs a scaled dot product where matrix A is scaled\n    by `scale_ptr`. Otherwise, it performs a standard matrix multiplication.\n    The computation is tiled and can be pipelined for efficiency.\n\n    Parameters:\n    ----------\n    a_ptr: tl.pointer_type\n        Pointer to the first input matrix (A).\n    scale_ptr: tl.pointer_type\n        Pointer to the scale tensor for matrix A. Used only if `a_type` and `b_type` are not None.\n    b_ptr: tl.pointer_type\n        Pointer to the second input matrix (B).\n    output_ptr: tl.pointer_type\n        Pointer to the output matrix (C).\n    M: int\n        Number of rows in matrix A and output matrix C.\n    N: int\n        Number of columns in matrix B and output matrix C.\n    K_MXFP: int\n        The inner dimension. If performing scaled MXFP matmul, this represents the number of MXFP vectors\n        (groups of elements, e.g., 32 for FP8, or 16 for FP4 if DIV_FACTOR is 2 due to packing)\n        along the K dimension of matrix A. Otherwise, it's the standard K dimension.\n    stride_am: int\n        Stride of matrix A along the M dimension (row stride).\n    stride_ak: int\n        Stride of matrix A along the K dimension (column/inner dimension stride).\n    stride_sm: int\n        Stride of the scale tensor along its M dimension (row stride for scales).\n    stride_sk: int\n        Stride of the scale tensor along its K dimension (stride between scale values).\n    stride_bk: int\n        Stride of matrix B along the K dimension (row stride).\n    stride_bn: int\n        Stride of matrix B along the N dimension (column/inner dimension stride).\n    stride_cm: int\n        Stride of the output matrix C along the M dimension (row stride).\n    stride_cn: int\n        Stride of the output matrix C along the N dimension (column/inner dimension stride).\n    BLOCK_M: tl.constexpr\n        Tile size for the M dimension.\n    BLOCK_N: tl.constexpr\n        Tile size for the N dimension.\n    BLOCK_K: tl.constexpr\n        Tile size for the K dimension (primarily for loading B and influencing A loads,\n        also relates to how many elements are processed in the inner loop iteration).\n    NUM_STAGES: tl.constexpr\n        Number of stages for software pipelining.\n    a_type: tl.constexpr (str or None)\n        String specifying the MXFP type for matrix A (e.g., \"e2m1\", \"e4m3\", \"e5m2\").\n        If None, standard matrix multiplication is assumed for A.\n    b_type: tl.constexpr (str or None)\n        String specifying the MXFP type for matrix B (e.g., \"e4m3\", \"e5m2\").\n        If None, standard matrix multiplication is assumed for B.\n    \"\"\"\n    # Your code here\n\n\n@triton.jit\ndef mxfp_to_bf16_kernel(\n    x_ptr,\n    scale_ptr,\n    mxfp_ptr,\n    N,\n    e_bits: tl.constexpr,\n    m_bits: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    Converts input data `x_ptr` (assumed to be in a packed MXFP format stored as uint8)\n    to bfloat16 format, by applying scaling factors from `scale_ptr`.\n    The kernel handles different MXFP types (e.g., FP8, FP4 variants) based on\n    `e_bits` and `m_bits`. The output is stored in `mxfp_ptr`.\n\n    Parameters:\n    ----------\n    x_ptr: tl.pointer_type\n        Pointer to the input MXFP data, stored as uint8.\n        Expected shape for processing: (N, 32) for FP8 or (N, 16) for FP4 (where 16 means 32 FP4 numbers packed).\n    scale_ptr: tl.pointer_type\n        Pointer to the scale values, stored as uint8. Expected shape: (N,).\n    mxfp_ptr: tl.pointer_type\n        Pointer to the output tensor where the bfloat16 results will be stored.\n        Expected shape after processing: (N, 32).\n    N: int\n        The number of scale values, which typically corresponds to the number of rows or groups\n        in the `x_ptr` data that are independently scaled.\n    e_bits: tl.constexpr\n        Number of exponent bits in the input MXFP format.\n    m_bits: tl.constexpr\n        Number of mantissa bits in the input MXFP format.\n    BLOCK_SIZE: tl.constexpr\n        The total number of bfloat16 elements in the output `mxfp_ptr` that a single\n        program instance (or a block of threads launched by `tl.program_id(0)`) will compute and store.\n        This is used to tile the processing of the output tensor.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport pytest\nimport torch\nimport triton\nimport triton.language as tl\nimport triton.tools.experimental_descriptor\n######################################## Imports ######################################## \n\n\n\n@triton.jit\ndef matmul_kernel(  #\n        a_ptr, scale_ptr, b_ptr, output_ptr,  #\n        M, N, K_MXFP,  # K_MXFP is the number of mxfp vectors in a row of a. Otherwise it's just K\n        stride_am, stride_ak,  #\n        stride_sm, stride_sk,  #\n        stride_bk, stride_bn,  #\n        stride_cm, stride_cn,  #\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,  #\n        NUM_STAGES: tl.constexpr, a_type: tl.constexpr, b_type: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    IS_SCALED: tl.constexpr = a_type is not None and b_type is not None\n    DIV_FACTOR: tl.constexpr = 2 if IS_SCALED and a_type == \"e2m1\" else 1\n    # We pass K_MXFP to make explicit that KB is multiple of 32 and KA is multiple of 16 or 32\n    # for the pipeliner divisibility condition\n    KA = K_MXFP if not IS_SCALED else K_MXFP * (32 // DIV_FACTOR)\n    KB = K_MXFP if not IS_SCALED else K_MXFP * 32\n    BLOCK_AK: tl.constexpr = BLOCK_K // DIV_FACTOR\n    offs_k = tl.arange(0, BLOCK_K)\n    offs_ak = tl.arange(0, BLOCK_AK)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_ak[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    if IS_SCALED:\n        BLOCK_SK: tl.constexpr = BLOCK_K // 32\n        offs_sk = tl.arange(0, BLOCK_SK)\n        scale_ptrs = scale_ptr + (offs_am[:, None] * stride_sm + offs_sk[None, :] * stride_sk)\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in tl.range(0, tl.cdiv(KB, BLOCK_K), num_stages=NUM_STAGES):\n        mask_a = (offs_am[:, None] < M) & (offs_ak[None, :] + k * BLOCK_AK < KA)\n        mask_b = ((offs_k[:, None] + k * BLOCK_K) < KB) & (offs_bn[None, :] < N)\n        a = tl.load(a_ptrs, mask=mask_a, other=0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0)\n        if IS_SCALED:\n            # Adapted scale indexing and dot_scaled operation\n            mask_scale = (offs_am[:, None] < M) & (offs_sk[None, :] + k * BLOCK_SK < K_MXFP)\n            a_scale = tl.load(scale_ptrs, mask=mask_scale, other=0)\n            accumulator = tl.dot_scaled(a, a_scale, a_type, b, None, b_type, acc=accumulator)\n        else:\n            accumulator = tl.dot(a, b, acc=accumulator)\n        a_ptrs += BLOCK_AK * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n        if IS_SCALED:\n            scale_ptrs += BLOCK_SK * stride_sk\n    OUT_DTYPE = tl.bfloat16 if IS_SCALED else tl.float16\n    accumulator = accumulator.to(OUT_DTYPE)\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    mask_c = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    output_ptrs = output_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(output_ptrs, accumulator, mask=mask_c)\n\n\n@triton.jit\ndef mxfp_to_bf16_kernel(\n    x_ptr,\n    scale_ptr,\n    mxfp_ptr,\n    N,\n    e_bits: tl.constexpr,\n    m_bits: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # x.shape ==     (N, 32) for fp8 or (N, 16) for fp4\n    # scale.shape == (N,)\n    # out.shape   == (N, 32)\n    is_fp8: tl.constexpr = e_bits + m_bits == 7\n    # fp8: BLOCK_SIZE -> BLOCK_SIZE // 32, 32\n    # fp4: BLOCK_SIZE // 2 -> BLOCK_SIZE // 32 , 16\n    PARALLEL_DIM: tl.constexpr = BLOCK_SIZE // 32\n    LAST_DIM: tl.constexpr = 32 if is_fp8 else 16\n    LOAD_SIZE: tl.constexpr = LAST_DIM * PARALLEL_DIM\n\n    offsets = (tl.program_id(0) * LOAD_SIZE + tl.arange(0, PARALLEL_DIM)[:, None] * LAST_DIM +\n               tl.arange(0, LAST_DIM)[None, :])\n    x = tl.load(x_ptr + offsets, mask=offsets < N * LAST_DIM)\n\n    offsets = tl.program_id(0) * PARALLEL_DIM + tl.arange(0, PARALLEL_DIM)[:, None]\n    scale = tl.load(scale_ptr + offsets, mask=offsets < N)\n    tl.static_assert(scale.dtype == tl.uint8)\n    tl.static_assert(x.dtype == tl.uint8)\n\n    scale_bf16 = (scale.to(tl.uint16) << 7).to(tl.bfloat16, bitcast=True)\n    if is_fp8:\n        if e_bits == 5 and m_bits == 2:\n            x_f8 = x.to(tl.float8e5, bitcast=True)\n            x_bf16 = x_f8.to(tl.bfloat16)\n            # Preserve infs and nans. FIXME Fp8E5M2_to_Bf16 doesn't preserve them!\n            non_finite_mask: tl.constexpr = ((1 << e_bits) - 1) << m_bits\n            non_finite_mask_bf16: tl.constexpr = ((1 << 8) - 1) << 7\n            x_bf16 = tl.where(\n                x & non_finite_mask == non_finite_mask,\n                (x_bf16.to(tl.uint16, bitcast=True) | non_finite_mask_bf16).to(tl.bfloat16, bitcast=True),\n                x_bf16,\n            )\n        else:\n            tl.static_assert(e_bits == 4 and m_bits == 3)\n            x_f8 = x.to(tl.float8e4nv, bitcast=True)\n            x_bf16 = x_f8.to(tl.bfloat16)\n    else:\n        # e2m1\n        em0 = x & 0x70\n        em1 = x & 0x7\n        x0 = (em0.to(tl.uint16) << 2) | ((x & 0x80).to(tl.uint16) << 8)\n        x1 = (em1.to(tl.uint16) << (2 + 4)) | ((x & 0x8).to(tl.uint16) << (8 + 4))\n        # Three cases:\n        # 1) x is normal and non-zero: Correct bias\n        x0 = tl.where((em0 & 0x60) != 0, x0 + ((127 - 1) << 7), x0)\n        x1 = tl.where((em1 & 0x6) != 0, x1 + ((127 - 1) << 7), x1)\n        # 2) x is subnormal (x == 0bs001 where s is the sign): Map to +-0.5 in bf16\n        x0 = tl.where(em0 == 0x10, 16128 | (x0 & 0x8000), x0)\n        x1 = tl.where(em1 == 0x1, 16128 | (x1 & 0x8000), x1)\n        # 3) x is zero, do nothing\n        x_bf16 = tl.interleave(x0, x1).to(tl.bfloat16, bitcast=True)\n    # Multiplication preserves infs and NaNs in x_bf16\n    mxfp = x_bf16 * scale_bf16\n    # If scale is NaN, we encode it as an bf16 inf, so we need to correct for that\n    mxfp = tl.where(scale == 0xFF, float(\"nan\"), mxfp)\n\n    offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    tl.store(mxfp_ptr + offsets, tl.ravel(mxfp), mask=offsets < N * 32)\n\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\n\nresult_gold = {}\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef is_hopper():\n    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n\n\ndef is_hip():\n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n\n\ndef is_hip_mi200():\n    target = triton.runtime.driver.active.get_current_target()\n    return target.backend == 'hip' and target.arch == 'gfx90a'\n\n\ndef check_capabilities():\n    if is_cuda():\n        cc = torch.cuda.get_device_capability()\n        if cc[0] < 8:\n            pytest.skip(\"CUDA 8.0+ required\")\n\n\ndef dot_scale_ref(x, scale, y, type_x, type_y):\n    e_bits, m_bits = {\"e2m1\": (2, 1), \"e4m3\": (4, 3), \"e5m2\": (5, 2)}[type_x]\n    type_fp8_y = {\"e4m3\": torch.float8_e4m3fn, \"e5m2\": torch.float8_e5m2}[type_y]\n\n    comp_dtype = torch.float32\n    out_dtype = torch.bfloat16\n\n    x = x.contiguous()\n    x_upcast = x.new_empty(scale.shape[:-1] + (32 * scale.shape[-1], ), dtype=comp_dtype)\n\n    N = x_upcast.numel()\n    BLOCK_SIZE = 512\n    grid = ((N + BLOCK_SIZE - 1) // BLOCK_SIZE, )\n    mxfp_to_bf16_kernel[grid](x, scale, x_upcast, scale.numel(), e_bits, m_bits, BLOCK_SIZE, num_warps=4)\n    y_upcast = y.view(type_fp8_y)\n\n    class AccumulateInFp32:\n\n        def __enter__(self):\n            self.prev_value = torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction\n            torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\n        def __exit__(self, exc_type, exc_val, exc_tb):\n            torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = self.prev_value\n\n    with AccumulateInFp32():\n        return torch.matmul(x_upcast.to(out_dtype), y_upcast.to(out_dtype))\n\n\n@pytest.mark.parametrize(\"scale\", [True, False])\ndef test_pipeline_matmul(scale, request, device='cuda'):\n    check_capabilities()\n    set_seed()\n    if scale and not is_cuda():\n        pytest.skip(\"NYI: scale_dot just implemented in CUDA\")\n    M, N, K = 512, 512, 128\n    BLOCK_M, BLOCK_N, BLOCK_K = 64, 64, 32\n    NUM_STAGES = 4\n\n    if scale:\n        # TODO Use e5m2 for Ampere, as it does not support fp_to_fp conversions for fp8e4m3\n        BLOCK_K = 64  # 32 NYI\n        K = BLOCK_K * NUM_STAGES\n        a_type = \"e2m1\"\n        DIV_FACTOR = 2 if a_type == \"e2m1\" else 1\n        a = torch.randint(256, (M, K // DIV_FACTOR), device=device, dtype=torch.uint8)\n        # Sample small-ish scales to avoid overflow\n        scale_a = torch.randint(74, (M, K // 32), device=device, dtype=torch.uint8)\n        # Ampere does not support fp8e4m3\n        b_type = \"e4m3\" if is_hopper() else \"e5m2\"\n        b = torch.randint(256, (K, N), device=device, dtype=torch.uint8)\n        # e5m2 has too many non-finite values when sampled uniformly (1 / 32) and\n        # Fp8E5M2_to_Bf16 doesn't preserve NaNs (fixme)\n        if b_type == \"e5m2\":\n            finite = torch.arange(K * N, device=device, dtype=torch.uint8).reshape(K, N) % 0x7C\n            b = torch.where(b & 0x7C == 0x7C, finite | (0x80 & b), b)\n        output = torch.empty((M, N), dtype=torch.bfloat16, device=device)\n    else:\n        a = torch.randn(M, K, device=device, dtype=torch.float16)\n        b = torch.randn(K, N, device=device, dtype=torch.float16)\n        scale_a = None\n        a_type, b_type = None, None\n        output = torch.empty((M, N), dtype=torch.float16, device=device)\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N), 1)\n\n\n    # Pass K_MXFP to make explicit that KB is multiple of 32 and KA is multiple of 16 or 32\u00ba\n    if scale:\n        K = scale_a.shape[-1]\n    stride_sm, stride_sk = scale_a.stride() if scale else (0, 0)\n    handler = matmul_kernel[grid](a, scale_a, b, output, M, N, K, a.stride(0), a.stride(1), stride_sm, stride_sk,\n                                    b.stride(0), b.stride(1), output.stride(0), output.stride(1), BLOCK_M, BLOCK_N,\n                                    BLOCK_K, NUM_STAGES=NUM_STAGES, a_type=a_type, b_type=b_type)\n    if scale:\n        ref_out = dot_scale_ref(a, scale_a, b, a_type, b_type)\n    else:\n        ref_out = torch.matmul(a, b)\n    # Bigger tolerance for AMD MI200 devices.\n    # MI200 devices use reduced precision fp16 and bf16 and flush input and\n    # output denormal values to zero. Detailed info is at: https://pytorch.org/docs/stable/notes/numerical_accuracy.html#reduced-precision-fp16-and-bf16-gemms-and-convolutions-on-amd-instinct-mi200-devices\n    atol = 1e-2 if is_hip_mi200() or scale else None\n    rtol = 1e-2 if is_hip_mi200() or scale else None\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = output.clone().detach().cpu()\n    ################################################################### \n\n\n    torch.testing.assert_close(ref_out, output, atol=atol, rtol=rtol, equal_nan=scale)\n\n\n# Define these globally so they are accessible by test_matmul_mxfp_performance\nFIXED_BLOCK_M_perf = 64\nFIXED_BLOCK_N_perf = 64\nFIXED_BLOCK_K_tile_perf = 32 # This is BLOCK_K in kernel, for tiling K_eff\nFIXED_NUM_STAGES_perf = 2    \nFIXED_NUM_WARPS_perf = 4     \n\n# --- Python wrapper for the kernel for benchmarking (UNCHANGED from previous corrected version) ---\ndef matmul_mxfp_triton_wrapper(a_tensor, scale_a_tensor, b_tensor, output_buffer,\n                               M_dim, N_dim, K_runtime_dim, \n                               block_m_const, block_n_const, block_k_const_tile, \n                               num_stages_const, a_type_str, b_type_str,\n                               num_warps_launch): \n    grid = (triton.cdiv(M_dim, block_m_const) * triton.cdiv(N_dim, block_n_const), 1)\n    stride_sm, stride_sk = (scale_a_tensor.stride(0), scale_a_tensor.stride(1)) \\\n                           if scale_a_tensor is not None else (0,0)\n    matmul_kernel[grid](\n        a_tensor, scale_a_tensor, b_tensor, output_buffer,\n        M_dim, N_dim, K_runtime_dim, \n        a_tensor.stride(0), a_tensor.stride(1),\n        stride_sm, stride_sk,\n        b_tensor.stride(0), b_tensor.stride(1),\n        output_buffer.stride(0), output_buffer.stride(1),\n        BLOCK_M=block_m_const, BLOCK_N=block_n_const, BLOCK_K=block_k_const_tile, \n        NUM_STAGES=num_stages_const, a_type=a_type_str, b_type=b_type_str,\n        num_warps=num_warps_launch\n    )\n    return output_buffer\n\n# --- CORRECTED TFLOPS and GB/s Calculators ---\ndef calculate_mxfp_matmul_tflops(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    K_for_flops = params['K_for_flops'] # Use the K dim relevant for FLOPs\n    flops = 2 * M * N * K_for_flops\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef get_torch_dtype_from_str(dtype_str: str, default_dtype=torch.float16):\n    if dtype_str == 'fp32': return torch.float32\n    if dtype_str == 'bf16': return torch.bfloat16\n    if dtype_str == 'fp16': return torch.float16\n    if dtype_str == 'uint8': return torch.uint8\n    return default_dtype\n\ndef calculate_mxfp_matmul_gbps(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    K_for_A_storage = params['K_for_A_storage'] \n    K_for_B_storage = params['K_for_B_storage'] \n    \n    is_scaled = params['is_scaled_mode']\n    \n    if is_scaled:\n        elem_size_a_data = torch.tensor([], dtype=torch.uint8).element_size()\n        elem_size_scale_a = torch.tensor([], dtype=torch.uint8).element_size()\n        elem_size_b_data = torch.tensor([], dtype=torch.uint8).element_size()\n        elem_size_out = torch.tensor([], dtype=torch.bfloat16).element_size()\n        K_mxfp_vectors_for_scale = params['K_MXFP_runtime'] \n    else: \n        input_dtype_str_calc = params.get('input_dtype_str', 'fp16')\n        output_dtype_str_calc = params.get('output_dtype_str_actual', 'fp16')\n\n        torch_input_dtype = get_torch_dtype_from_str(input_dtype_str_calc)\n        torch_output_dtype = get_torch_dtype_from_str(output_dtype_str_calc)\n        \n        elem_size_a_data = torch.tensor([], dtype=torch_input_dtype).element_size()\n        elem_size_b_data = torch.tensor([], dtype=torch_input_dtype).element_size()\n        elem_size_out = torch.tensor([], dtype=torch_output_dtype).element_size()\n        elem_size_scale_a = 0 \n        K_mxfp_vectors_for_scale = 0\n\n    bytes_a = M * K_for_A_storage * elem_size_a_data\n    bytes_b = K_for_B_storage * N * elem_size_b_data \n    bytes_scale_a = M * K_mxfp_vectors_for_scale * elem_size_scale_a if is_scaled and elem_size_scale_a > 0 else 0\n    bytes_out_write = M * N * elem_size_out\n    \n    total_bytes = bytes_a + bytes_b + bytes_scale_a + bytes_out_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"matmul_mxfp_triton_perf\"\n\nMXFP_PERF_CONFIGS = []\nshapes_perf = [(512, 512, 128), (1024, 1024, 64), (2048, 1024, 32)] \n\nfor M_val, N_val, K_actual_val in shapes_perf:\n    for input_dtype_s in ['fp16', 'fp32']: \n        MXFP_PERF_CONFIGS.append({\n            'M':M_val, 'N':N_val, 'K_param':K_actual_val, 'is_scaled':False, \n            'a_type':None, 'b_type':None, 'input_dtype':input_dtype_s\n        })\n\nif is_cuda() and torch.cuda.get_device_capability()[0] >=9 : \n    for M_val, N_val, K_mxfp_val in shapes_perf:\n        if K_mxfp_val == 0 : continue # K_mxfp must be > 0 for scaled mode\n        if K_mxfp_val % (FIXED_BLOCK_K_tile_perf // (32 // 2)) != 0 and K_mxfp_val % (FIXED_BLOCK_K_tile_perf // (32 // 1)) != 0 :\n             # K_MXFP must be such that KA_eff and KB_eff are reasonable for K_tile\n             # This check is complex, for now assume K_mxfp_val is okay.\n             # A simpler check: K_mxfp_val should be multiple of BLOCK_SK_eff (BLOCK_K_tile/32)\n             if FIXED_BLOCK_K_tile_perf % 32 == 0 and K_mxfp_val % (FIXED_BLOCK_K_tile_perf // 32) != 0:\n                 # print(f\"Skipping K_mxfp={K_mxfp_val} with BLOCK_K_tile={FIXED_BLOCK_K_tile_perf} due to scale alignment\")\n                 continue\n\n        MXFP_PERF_CONFIGS.append({ \n            'M':M_val, 'N':N_val, 'K_param':K_mxfp_val, 'is_scaled':True, \n            'a_type':'e2m1', 'b_type':'e4m3', 'input_dtype':'uint8'\n        })\n        if is_hopper(): \n            MXFP_PERF_CONFIGS.append({ \n                'M':M_val, 'N':N_val, 'K_param':K_mxfp_val, 'is_scaled':True, \n                'a_type':'e2m1', 'b_type':'e5m2', 'input_dtype':'uint8'\n            })\n\n@pytest.mark.parametrize(\"test_cfg_dict\", MXFP_PERF_CONFIGS)\ndef test_performance(test_cfg_dict, request):\n    check_capabilities() \n    set_seed()\n    device = 'cuda'\n\n    M = test_cfg_dict['M']\n    N = test_cfg_dict['N']\n    K_param = test_cfg_dict['K_param'] \n    is_scaled_mode = test_cfg_dict['is_scaled']\n    a_type_kernel_str = test_cfg_dict['a_type'] \n    b_type_kernel_str = test_cfg_dict['b_type'] \n    \n    K_MXFP_runtime = 0\n    K_eff_A_storage = 0 \n    K_eff_B_storage = 0 \n    K_for_flops_calc = 0 \n    scale_a_tensor = None \n    input_dtype_for_calc = test_cfg_dict['input_dtype'] \n\n    if is_scaled_mode:\n        if not is_cuda(): pytest.skip(\"Scaled MXFP test part currently CUDA-specific\")\n        if a_type_kernel_str is None or b_type_kernel_str is None :\n             pytest.skip(\"a_type and b_type must be specified for scaled mode\")\n\n        K_MXFP_runtime = K_param \n        DIV_FACTOR = 2 if a_type_kernel_str == \"e2m1\" else 1\n        \n        K_eff_A_storage = K_MXFP_runtime * (32 // DIV_FACTOR)\n        K_eff_B_storage = K_MXFP_runtime * 32            \n        \n        # For dot(A(M,K1), B(K1,N)), K_for_flops is K1.\n        # Here a is (M, KA_eff) and b is (KB_eff, N) for the kernel's view.\n        # The dot_scaled implies A's K dim must match B's K dim logically.\n        # KA_eff = K_MXFP * (32/DIV_FACTOR), KB_eff = K_MXFP * 32.\n        # These are generally different unless DIV_FACTOR=1.\n        # tl.dot_scaled is flexible: `dot_scaled(a, scale_a, \"eXmY\", b, scale_b, \"eZmQ\", acc)`\n        # It handles the internal expansion. The logical K for FLOPs is related to K_MXFP * 32.\n        K_for_flops_calc = K_eff_B_storage # Use the larger effective K from B for FLOPs\n\n        if K_eff_A_storage == 0 or K_eff_B_storage == 0 : pytest.skip(\"Effective K is zero for scaled mode.\")\n\n        a_tensor = torch.randint(256, (M, K_eff_A_storage), device=device, dtype=torch.uint8)\n        scale_a_tensor = torch.randint(74, (M, K_MXFP_runtime), device=device, dtype=torch.uint8)\n        b_tensor = torch.randint(256, (K_eff_B_storage, N), device=device, dtype=torch.uint8)\n        output_buffer = torch.empty((M, N), dtype=torch.bfloat16, device=device)\n        actual_output_dtype_str = \"bf16\"\n    else: \n        K_MXFP_runtime = K_param \n        K_eff_A_storage = K_param\n        K_eff_B_storage = K_param\n        K_for_flops_calc = K_param\n        \n        current_input_torch_dtype = get_torch_dtype_from_str(input_dtype_for_calc)\n        \n        a_tensor = torch.randn(M, K_param, device=device, dtype=current_input_torch_dtype)\n        b_tensor = torch.randn(K_param, N, device=device, dtype=current_input_torch_dtype)\n        output_buffer = torch.empty((M, N), dtype=torch.float16, device=device)\n        actual_output_dtype_str = \"fp16\"\n\n    # Use globally defined fixed block sizes for performance test\n    block_m_const = FIXED_BLOCK_M_perf\n    block_n_const = FIXED_BLOCK_N_perf\n    block_k_const_tile = FIXED_BLOCK_K_tile_perf \n    num_stages_const = FIXED_NUM_STAGES_perf\n    num_warps_launch = FIXED_NUM_WARPS_perf\n\n    op_lambda = lambda: matmul_mxfp_triton_wrapper(\n        a_tensor, scale_a_tensor, b_tensor, output_buffer,\n        M, N, K_MXFP_runtime, \n        block_m_const, block_n_const, block_k_const_tile,\n        num_stages_const, a_type_kernel_str, b_type_kernel_str,\n        num_warps_launch\n    )\n\n    bench_config = do_bench_config(warm_up=10, repetition=50)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \n        \"K_param_input\": K_param, \n        \"K_MXFP_runtime\": K_MXFP_runtime,\n        \"K_for_flops\": K_for_flops_calc, \n        \"K_for_A_storage\": K_eff_A_storage, \n        \"K_for_B_storage\": K_eff_B_storage, \n        \"is_scaled_mode\": is_scaled_mode, \n        \"a_type_str\": a_type_kernel_str, \"b_type_str\": b_type_kernel_str,\n        \"input_dtype_str\": input_dtype_for_calc, \n        \"output_dtype_str_actual\": actual_output_dtype_str, \n        \"BLOCK_M\": block_m_const, \"BLOCK_N\": block_n_const, \"BLOCK_K_tile\": block_k_const_tile,\n        \"NUM_STAGES\": num_stages_const, \"num_warps\": num_warps_launch\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_mxfp_matmul_gbps,\n                                            tflops_calculator=calculate_mxfp_matmul_tflops)\n\n\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_load_reduce.py",
        "target_kernel_name": "load_reduce_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `load_reduce_kernel` kernels. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThese kernels, `load_reduce_kernel`,  performs a block-wise load followed by a row-wise maximum reduction.\n\n**Your objective is to implement the body of both the kernels `load_reduce_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_kernel and mxfp_to_bf16_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `load_reduce_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `load_reduce_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n######################################## Imports ########################################\n\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n}\n\n\n@triton.jit\ndef load_reduce_kernel(\n    x_ptr,\n    y_ptr,\n    stride_xm,\n    stride_xn,\n    stride_y,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    \"\"\"\n    This Triton kernel loads a 2D block of data from an input tensor `x_ptr`\n    and performs a reduction operation (maximum) along one of its dimensions.\n\n    Parameters\n    ----------\n    x_ptr\n        Pointer to the input tensor X from which data will be loaded.\n        This tensor is expected to be at least 2D.\n    y_ptr\n        Pointer to the output tensor Y where the reduced results will be stored.\n        This tensor is expected to be 1D (or have a shape compatible with storing BLOCK_M elements).\n    stride_xm\n        Stride of the input tensor X along the M dimension (typically rows).\n        It indicates the number of elements to skip in memory to move from one\n        element to the next in the M dimension (e.g., from X[i, j] to X[i+1, j]).\n    stride_xn\n        Stride of the input tensor X along the N dimension (typically columns).\n        It indicates the number of elements to skip in memory to move from one\n        element to the next in the N dimension (e.g., from X[i, j] to X[i, j+1]).\n    stride_y\n        Stride of the output tensor Y.\n        It indicates the number of elements to skip in memory to move from one\n        element to the next in the output tensor Y (e.g., from Y[i] to Y[i+1]).\n    BLOCK_M: tl.constexpr\n        The size of the tile (or block) in the M dimension. This defines how many\n        \"rows\" of the input data are processed by this kernel instance and consequently\n        the number of output elements produced.\n    BLOCK_N: tl.constexpr\n        The size of the tile (or block) in the N dimension. This defines how many\n        \"columns\" of the input data are processed for each \"row\" in the M dimension,\n        and it's the dimension over which the reduction (max) is performed.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n}\n\n\n@triton.jit\ndef load_reduce_kernel(\n    x_ptr,\n    y_ptr,\n    stride_xm,\n    stride_xn,\n    stride_y,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    x_ptr = tl.make_block_ptr(base=x_ptr, shape=(BLOCK_M, BLOCK_N), strides=(stride_xm, stride_xn), offsets=(0, 0),\n                              block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    x = tl.load(x_ptr)\n    y = tl.max(x, axis=1)\n    tl.store(y_ptr + tl.arange(0, BLOCK_M), y)\n\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n@pytest.mark.parametrize('BLOCK_M,BLOCK_N,dtype_str', [(128, 64, dtype_str) for dtype_str in ['float16']])\ndef test_load_reduce(BLOCK_M, BLOCK_N, dtype_str, request):\n    set_seed()\n\n    dtype = dtype_mapping[dtype_str]\n    x = torch.randn((BLOCK_M, BLOCK_N), device='cuda', dtype=dtype)\n    y = torch.empty((BLOCK_M, ), device='cuda', dtype=dtype)\n\n    load_reduce_kernel[(1, )](x, y, x.stride(0), x.stride(1), y.stride(0), BLOCK_M, BLOCK_N)\n\n    golden = x.max(dim=1)[0]\n    torch.set_printoptions(profile='full')\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = y.clone().detach().cpu()\n    ################################################################### \n\n    assert_close(y, golden, rtol=1e-2, atol=1e-3, check_dtype=False)\n\n\n# --- Python wrapper for the kernel for benchmarking ---\ndef load_reduce_triton_wrapper(x_tensor, y_buffer, block_m_const, block_n_const):\n    # For this kernel, the full dimensions processed are defined by block_m_const, block_n_const\n    # The grid is (1,) as the kernel does not use program_id for further tiling.\n    grid = (1,)\n    \n    load_reduce_kernel[grid](\n        x_tensor, y_buffer,\n        x_tensor.stride(0), x_tensor.stride(1),\n        y_buffer.stride(0), # Stride of y (typically 1 for contiguous 1D)\n        BLOCK_M=block_m_const, \n        BLOCK_N=block_n_const\n        # num_warps can be added as a launch hint if desired\n    )\n    return y_buffer\n\n# --- Define TFLOPS and GB/s calculators ---\ndef calculate_load_reduce_tflops(params: dict, ms: float) -> float:\n    # Operation: M rows, N elements per row. For each row, N-1 comparisons for max.\n    M_dim = params['M_dim'] # This is BLOCK_M for the kernel\n    N_dim = params['N_dim'] # This is BLOCK_N for the kernel\n    \n    flops = M_dim * (N_dim -1) if N_dim > 0 else 0 # Approx N comparisons per row\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_load_reduce_gbps(params: dict, ms: float) -> float:\n    M_dim = params['M_dim']\n    N_dim = params['N_dim']\n    dtype_str = params.get('dtype_str', 'fp16')\n\n    current_dtype = torch.float16\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n\n    # Read X (M,N), Write Y (M)\n    bytes_x_read = M_dim * N_dim * element_size\n    bytes_y_write = M_dim * element_size \n    total_bytes = bytes_x_read + bytes_y_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"load_reduce_triton_perf\"\n\n# --- Pytest parametrize for performance testing ---\n# The kernel processes a single block of size BLOCK_M x BLOCK_N.\n# So, M_dim and N_dim for the test will be BLOCK_M and BLOCK_N for the kernel.\nLOAD_REDUCE_BLOCK_SHAPES_FOR_PERF = [\n    # BLOCK_M, BLOCK_N (these are effectively M_dim, N_dim for the single block kernel)\n    (128, 64), (128, 128), (128, 256), (128, 512), (128, 1024),\n    (256, 64), (256, 128), (256, 256), (256, 512),\n    (512, 64), (512, 128), (512, 256),\n    (1024, 64), (1024, 128),\n    # (4096, 64) # Larger M\n]\nLOAD_REDUCE_DTYPES_FOR_PERF = ['fp16', 'fp32', 'bf16'] \n# NUM_WARPS_FOR_PERF = [1, 2, 4, 8] # Can be added as a launch hint\n\n@pytest.mark.parametrize(\"block_m_const, block_n_const\", LOAD_REDUCE_BLOCK_SHAPES_FOR_PERF)\n@pytest.mark.parametrize(\"dtype_str\", LOAD_REDUCE_DTYPES_FOR_PERF)\n# @pytest.mark.parametrize(\"num_warps_launch\", NUM_WARPS_FOR_PERF) # Optional\ndef test_performance(block_m_const, block_n_const, dtype_str, request): # Added num_warps_launch if parametrized\n    \n    # num_warps_launch = 4 # Or from parametrize\n    \n    # Capability checks for dtypes\n    if dtype_str == 'bf16':\n        cap = torch.cuda.get_device_capability()\n        if cap[0] < 8:\n            pytest.skip(\"bf16 requires Ampere+ (arch 80+)\")\n    \n    set_seed()\n    \n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    # Input x has shape (block_m_const, block_n_const) as the kernel processes this single block\n    x = torch.randn((block_m_const, block_n_const), device='cuda', dtype=current_dtype)\n    # Output y has shape (block_m_const,)\n    y_buffer = torch.empty((block_m_const,), device='cuda', dtype=current_dtype)\n    \n    op_lambda = lambda: load_reduce_triton_wrapper(\n        x, y_buffer, block_m_const, block_n_const\n        # , num_warps_launch # if num_warps is added to wrapper\n    )\n\n    bench_config = do_bench_config(warm_up=50, repetition=200) # Reduction can be fast\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M_dim\": block_m_const, \"N_dim\": block_n_const, # Use M_dim, N_dim for calculators\n        \"dtype_str\": dtype_str,\n        # \"num_warps\": num_warps_launch # if parametrized\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_load_reduce_gbps,\n                                            tflops_calculator=calculate_load_reduce_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "naive_softmax.py",
        "target_kernel_name": "softmax_kernel_naive",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `softmax_kernel_naive` kernel. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `softmax_kernel_naive`,  naive softmax operation.\n\n**Your objective is to implement the body of  the kernel `softmax_kernel_naive`.**\n\nYou must ensure that:\n1.  All arguments received by `softmax_kernel_naive` are kept intact and not modified.\n2. Provide your final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `softmax_kernel_naive` and relevant helper utilities are provided in the context below. You only need to complete the code for `softmax_kernel_naive` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n}\n######################################## Imports ########################################\n\n\n @triton.jit\ndef softmax_kernel_naive(in_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Computes the softmax function over the last dimension of a 2D input tensor.\n\n    Each program instance is responsible for processing a single row of the input tensor.\n\n    Parameters\n    ----------\n    in_ptr\n        Pointer to the 2D input tensor.\n    output_ptr\n        Pointer to the 2D output tensor where the result is stored. The dimensions\n        of this tensor should match the input tensor.\n    row_stride\n        The number of elements to skip in memory to move from the start of one\n        row to the start of the next. This is used to correctly index into the\n        input and output tensors.\n    n_cols\n        The size of the last dimension of the tensor (i.e., the number of columns\n        in each row).\n    BLOCK_SIZE : tl.constexpr\n        A compile-time constant that defines the size of the data block that each\n        instance processes in a single operation. This is used to tile the\n        computation over the columns of a row.\n    \"\"\"    # Each program instance processes a single row of the input tensor.\n    # 1. Get the row index\n    row_idx = tl.program_id(axis=0)\n\n    # 2. Compute offsets for the current row.\n    # The naive kernel assumes that the number of columns is a power of 2.\n    # and that `BLOCK_SIZE` is equal to `n_cols`.\n    row_start_ptr = in_ptr + row_idx * row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n\n    # 3. Load the row into a 1D block.\n    # `mask` is used to handle rows where `n_cols` is not a power of 2.\n    mask = col_offsets < n_cols\n    # load the input data; use `other=-float('inf')` to ensure correct `max` calculation\n    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n\n    # 4. Compute softmax.\n    #    a. Subtract the maximum value for numerical stability.\n    row_minus_max = row - tl.max(row, axis=0)\n    #    b. Compute the numerator.\n    numerator = tl.exp(row_minus_max)\n    #    c. Compute the denominator.\n    denominator = tl.sum(numerator, axis=0)\n    #    d. Normalize.\n    softmax_output = numerator / denominator\n\n    # 5. Write the result to the output tensor.\n    output_row_start_ptr = output_ptr + row_idx * row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=mask)\n\n",
        "label": "# Copyright(C) [2025] Advanced Micro Devices, Inc. All rights reserved.\n#Imports \nimport argparse\nimport torch\nimport sys\nimport pytest\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef softmax_kernel_naive(in_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n\n    in_max = -float('inf')\n    for offset in range(0, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(0, BLOCK_SIZE)\n        col_mask = col_range + offset < n_cols\n        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n        in_max = tl.maximum(in_max, tl.max(in_data, axis=-1))\n    \n    in_exp_sum = 0.0\n    for offset in range(0, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(0, BLOCK_SIZE)\n        col_mask = col_range + offset < n_cols\n        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n        in_exp_sum = in_exp_sum + tl.sum(tl.exp(in_data - in_max), axis=-1)\n    \n    for offset in range(0, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(0, BLOCK_SIZE)\n        col_mask = col_range + offset < n_cols\n        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n        in_exp = tl.exp(in_data - in_max)\n        tl.store(output_ptr + pid * row_stride + col_range + offset, in_exp / in_exp_sum, mask=col_mask)\n\n\n\n##################################################################################################################################################  \n\n\nimport numpy as np\nimport random\nimport torch \nimport os\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \nCONFIG = {\n  \"llama3\": {\n    \"8B\": {\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"vocab_size\": 128256\n    },\n    \"70B\": {\n      \"num_attention_heads\": 64,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 8192,\n      \"intermediate_size\": 28672,\n      \"vocab_size\": 128256\n    },\n    \"405B\": {\n      \"num_attention_heads\": 128,\n      \"num_key_value_heads\": 8,\n      \"hidden_size\": 16384,\n      \"intermediate_size\": 53248,\n      \"vocab_size\": 128256\n    }\n  },\n  \"mistral\": {\n    \"7B\": {\n      \"hidden_size\": 4096,\n      \"intermediate_size\": 14336,\n      \"num_attention_heads\": 32,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    },\n    \"22B\": {\n      \"hidden_size\": 6144,\n      \"intermediate_size\": 16384,\n      \"num_attention_heads\": 48,\n      \"num_key_value_heads\": 8,\n      \"vocab_size\": 32000\n    }\n\n  }\n}\n\n  \ndef get_model_configs(config_path='model_configs.json', model_families=[\"llama3\"], model=\"all\"):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_path (str): User-provided path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        dict: A dictionary of available models and their configurations for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    # Extract models and their configurations for the specified families  \n    filtered_configs = {}  \n  \n    for family in model_families:  \n        if family in configs:  \n            # Check if model filtering is required  \n            if model == \"all\":  \n                # Include all models in the family  \n                for model_size, model_configs in configs[family].items():  \n                    filtered_configs[f\"{family}-{model_size}\"] = model_configs  \n            else:  \n                # Parse the model string (e.g., llama3_8B or llama3-8B)  \n                delimiter = \"_\" if \"_\" in model else \"-\"  \n                model_parts = model.split(delimiter)  \n  \n                # Check if the family and size match  \n                if len(model_parts) == 2 and model_parts[0] == family:  \n                    model_size = model_parts[1]  \n                    if model_size in configs[family]:  \n                        filtered_configs[f\"{family}-{model_size}\"] = configs[family][model_size]  \n  \n    if not filtered_configs:  \n        print(f\"Warning: No models selected for families: {model_families} with filter: '{model}'\")  \n  \n    return filtered_configs  \n  \n  \ndef get_available_models(config_file='model_configs.json', model_families=[\"llama3\"]):  \n    \"\"\"  \n    Load model names from the configuration file.  \n  \n    Args:  \n        config_file (str): Path to the configuration JSON file.  \n        model_families (list): List of model family names to retrieve.  \n  \n    Returns:  \n        list: A list of available models for the specified families.  \n    \"\"\"  \n    configs = CONFIG.copy()\n  \n    models = [f\"{family}-{model}\" for family in model_families if family in configs for model in configs[family]]  \n  \n    return models  \n\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n  \n\ndef softmax(x):\n    n_rows, n_cols = x.shape\n\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_SIZE = 256\n    y = torch.empty_like(x)\n\n    num_programs = n_rows\n\n    grid = lambda meta: (num_programs, )\n    softmax_kernel_naive[grid](\n        x,\n        y,\n        x.stride(0),\n        n_cols,\n        BLOCK_SIZE,\n    )\n\n    return y\n\n\ndef run_softmax(M, N):\n    print(f\"Running Softmax on shape ({M},{N})\")\n    set_seed()\n    x = torch.randn(M, N, device='cuda')\n    y_triton = softmax(x)\n\n    return y_triton\n\n\n#pytest\n@pytest.mark.parametrize('M, N', [(1823, 781), (1, 1), (128, 1), (1, 128), (8192, 8192), (4096, 8192), (359, 1),\n                                  (1, 359), (1, 131072), (1, 89999)])\ndef test_softmax(M, N, request):\n    set_seed()\n    x = torch.randn(M, N, device='cuda')\n    y_triton = softmax(x)\n    y_torch = torch.softmax(x, axis=1)\n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = y_triton.clone().detach().cpu()\n    ###################################################################\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n\n\n#Benchmark\narg_to_torch_dtype = {'fp16': torch.float16, 'bf16': torch.bfloat16, 'fp32': torch.float32}\n\n# --- Define TFLOPS and GB/s calculators for Softmax Forward ---\ndef calculate_softmax_fwd_gbps(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    dtype_str = params.get('dtype_str', 'fp16')\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    \n    # Read x (M,N), Write y (M,N)\n    # Intermediate m (M) and row_sum (M) are usually kept in registers/SRAM per row,\n    # not necessarily global memory traffic unless spilled, which is hard to model simply.\n    # For online softmax, data is read twice effectively (once for max/sum, once for normalization).\n    bytes_read_x_pass1 = M * N * element_size\n    bytes_read_x_pass2 = M * N * element_size # Or just once if fully fused and data stays in cache\n    bytes_write_y = M * N * element_size\n\n    # A common simplification for bandwidth: 2*M*N (one read, one write)\n    # More accurate for online: read_pass1 + read_pass2 + write\n    # Let's use 2 reads, 1 write for online softmax\n    total_bytes = bytes_read_x_pass1 + bytes_read_x_pass2 + bytes_write_y\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\ndef calculate_softmax_fwd_tflops(params: dict, ms: float) -> float:\n    M, N = params['M'], params['N']\n    # FLOPs for Softmax forward (per row):\n    # 1. Find max: N-1 comparisons (approx N ops)\n    # 2. Subtract max: N subtractions (N ops)\n    # 3. Exp: N exponentials (N * ~5-10 ops, say N*5)\n    # 4. Sum exps: N-1 additions (approx N ops)\n    # 5. Divide by sum: N divisions (N ops)\n    # Total per row approx: N + N + 5N + N + N = 9N ops\n    flops_per_row = 9 * N \n    total_flops = M * flops_per_row\n    tflops = total_flops / (ms / 1000) / 1e12\n    return tflops\n\n# --- Name for the benchmark output JSON file ---\nOP_NAME_FOR_BENCHMARK = \"softmax_triton_perf\"\n\n# --- Pytest test_softmax function MODIFIED for performance benchmarking ---\n# Original parametrization is kept.\nSOFTMAX_SHAPES_FOR_PERF = [\n    (2048, 2048), (4096, 4096), (8192, 8192), # Square\n    (1, 32000), (1, 131072),                 # Typical vocab sizes (batch 1)\n    (1024, 8192), (512, 32000),              # Batch > 1\n    # (1,4), (1823, 781) # Smaller/odd shapes\n]\nSOFTMAX_DTYPES_FOR_PERF = ['fp16', 'bf16', 'fp32']\n\n\n@pytest.mark.parametrize('M, N', SOFTMAX_SHAPES_FOR_PERF)\n@pytest.mark.parametrize('dtype_str', SOFTMAX_DTYPES_FOR_PERF)\ndef test_performance(M, N, dtype_str, request): # Renamed from test_softmax\n    set_seed()\n    \n    if dtype_str == 'fp32': current_dtype = torch.float32\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16\n    else: current_dtype = torch.float16\n\n    x = torch.randn(M, N, device='cuda', dtype=current_dtype)\n\n    # --- Create op_lambda for benchmarking ---\n    op_lambda = lambda: softmax(x)\n\n    # --- Benchmarking ---\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    # Determine BLOCK_SIZE as it's done in the softmax_triton_wrapper for logging\n    # This is for logging consistency, the actual kernel uses autotuned block size.\n    # The BLOCK_SIZE passed to kernel is a key for autotuning.\n    MAX_FUSED_SIZE_log = 65536 // x.element_size()\n    BLOCK_SIZE_log = min(MAX_FUSED_SIZE_log, triton.next_power_of_2(N if N > 0 else 1))\n    if BLOCK_SIZE_log == 0: BLOCK_SIZE_log = 1\n\n\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"dtype_str\": dtype_str,\n        \"LOGGED_BLOCK_SIZE_heuristic\": BLOCK_SIZE_log # Log the heuristic block size\n    }\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                              gbps_calculator=calculate_softmax_fwd_gbps,\n                              tflops_calculator=calculate_softmax_fwd_tflops)\n\n\ndef model_benchmark_configs(args):\n    config_file = args.model_configs\n    configs = get_model_configs(config_path=config_file, model_families=[\"llama3\"], model=args.model)\n\n    x_vals_list = []\n    batch_size = args.b if args.b else 1\n\n    for model_name, config in configs.items():\n        seq_len = args.sq if args.sq else 4096\n        x_vals_list.append((model_name, batch_size * seq_len, config[\"vocab_size\"]))\n\n    return x_vals_list\n\n\ndef run_benchmark(args):\n    config = []\n    if (args.M_benchmark):\n        val = args.M_start\n        x_vals_list = []\n        while val <= args.M_end:\n            x_vals_list.append(val)\n            val *= args.M_step\n        mn_args = {'N': args.N_start}\n        plot_name = str(\"softmax-performance_\" + args.dtype + \"_N\" + str(args.N_start) + \"_M\" + str(args.M_start) +\n                        \"-\" + str(args.M_end) + \"-\" + str(args.M_step))\n        x_names = ['M']\n    else:\n        x_vals_list = [i for i in range(args.N_start, args.N_end, args.N_step)]\n        mn_args = {'M': args.M_start}\n        plot_name = str(\"softmax-performance_\" + args.dtype + \"_M\" + str(args.M_start) + \"_N\" + str(args.N_start) +\n                        \"-\" + str(args.N_end) + \"-\" + str(args.N_step))\n        x_names = ['N']\n\n    if args.model:\n        assert not args.M_benchmark, \\\n            \"Trying to provide both -model benchmark and M_benchmark is not supported!\"\n        x_names = ['model', 'M', 'N']\n        mn_args = {}\n        plot_name = str(\"softmax-performance_\" + args.dtype)\n        x_vals_list = model_benchmark_configs(args)\n\n    dtype = arg_to_torch_dtype[args.dtype]\n\n    print(plot_name)\n    config.append(\n        triton.testing.Benchmark(\n            x_names=x_names,\n            x_vals=x_vals_list,\n            line_arg='provider',\n            line_vals=['triton', 'torch'],\n            line_names=[\n                \"Triton\",\n                \"Torch\",\n            ],\n            styles=[('blue', '-'), ('green', '-')],\n            ylabel=\"GB/s\",\n            plot_name=plot_name,\n            args=mn_args,\n        ))\n\n    @triton.testing.perf_report(config)\n    def benchmark(M, N, provider, model=None):\n        x = torch.randn(M, N, device='cuda', dtype=dtype)\n        stream = torch.cuda.Stream()\n        torch.cuda.set_stream(stream)\n        if provider == 'torch':\n            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n        if provider == 'triton':\n            ms = triton.testing.do_bench(lambda: softmax(x))\n        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n        return gbps(ms)\n\n    benchmark.run(save_path=\".\", show_plots=True, print_data=True)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n######################################## HELPERS for Eval ######################################## \n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        prog=\"Benchmark Softmax\",\n        allow_abbrev=False,\n    )\n    parser.add_argument('-model_configs', type=str, default=\"model_configs.json\", help=\"Model config json file.\")\n\n    available_models = get_available_models(model_families=[\"llama3\"])  # Dynamically load model names\n    model_help = (\n        \"Model name to benchmark. Select from: [\" + \", \".join(available_models) +\n        \"]. Use 'all' to benchmark all models. Not providing runs the default benchmark script with custom configs.\")\n    parser.add_argument('-model', type=str, default=None, help=model_help)\n    parser.add_argument('-b', type=int, default=0, help=\"Batch size used together with model.\")\n    parser.add_argument('-sq', type=int, default=0, help=\"Sequence length used together with model.\")\n    parser.add_argument('-M', \"--M_start\", default=\"1\", type=int)\n    parser.add_argument('-Ms', \"--M_step\", default=\"2\", type=int)\n    parser.add_argument('-Me', \"--M_end\", default=\"512\", type=int)\n    parser.add_argument('-Mb', \"--M_benchmark\", default=False, type=bool)\n\n    parser.add_argument('-N', \"--N_start\", default=\"1024\", type=int)\n    parser.add_argument('-Ns', \"--N_step\", default=\"2048\", type=int)\n    parser.add_argument('-Ne', \"--N_end\", default=\"65536\", type=int)\n\n    parser.add_argument('-d', \"--dtype\", default=\"fp16\")\n    parser.add_argument('-nb', \"--no_benchmark\", default=False, type=bool)\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    if args.no_benchmark:\n        run_softmax(args.M_start, args.N_start)\n    else:\n        run_benchmark(args)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"
    },
    {
        "file": "test_add_kernel.py",
        "target_kernel_name": "add_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `add_kernel` kernels. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThese kernels, `add_kernel`,  performs element-wise addition of two 1D tensors.\n\n**Your objective is to implement the body of both the kernels `add_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_kernel and mxfp_to_bf16_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `add_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `add_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ########################################\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n}\n######################################## Imports ########################################\n\n\n@triton.jit\ndef add_kernel(\n    x_ptr,\n    y_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n    Performs element-wise addition of two 1D tensors, `x` and `y`,\n    and stores the result in `output`. Boundary checks are handled using a mask to ensure that\n    operations are only performed on valid elements within the tensor's\n    `n_elements` bounds.\n\n    Parameters\n    ----------\n    x_ptr\n        Pointer to the first input tensor (x).\n        The elements from this tensor will be added.\n    y_ptr\n        Pointer to the second input tensor (y).\n        The elements from this tensor will be added.\n    output_ptr\n        Pointer to the output tensor where the result of x + y is stored.\n    n_elements\n        The total number of elements in the input and output tensors.\n        This is used to ensure that memory accesses are within bounds.\n    BLOCK_SIZE : tl.constexpr\n        The size of the block that each program instance will process.\n        This is a compile-time constant and dictates how many elements\n        are loaded, processed, and stored together by a single program\n        instance in the Triton kernel. It should typically be a power of two.\n    \"\"\"\n    # Your code here\n\n",
        "label": "######################################## Imports ######################################## \nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n}\n######################################## Imports ######################################## \n\n\n\n@triton.jit\ndef add_kernel(\n    x_ptr,\n    y_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(n_elements, ), strides=(1, ), offsets=(pid * BLOCK_SIZE, ),\n                                    block_shape=(BLOCK_SIZE, ), order=(0, ))\n    x = tl.load(x_block_ptr, boundary_check=(0, ), padding_option='zero')\n\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nfrom numpy.random import RandomState\nimport pytest\nfrom torch.testing import assert_close\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nimport triton\nimport triton.language as tl\n\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n}\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \n# Helper function to define GB/s for add_kernel\ndef calculate_add_gbps(params: Dict, ms: float) -> float:\n    # params will contain 'SIZE', 'dtype_str'\n    size = params['SIZE']\n    dtype = dtype_mapping[params['dtype_str']]\n    # For add: read x, read y, write output\n    # If x, y, output are torch.Tensor objects passed to this calculator:\n    # total_bytes = (x.numel() * x.element_size() +\n    #                y.numel() * y.element_size() +\n    #                output.numel() * output.element_size())\n    # If only params are available:\n    bytes_per_element = torch.tensor([], dtype=dtype).element_size()\n    total_bytes = 3 * size * bytes_per_element # 2 reads, 1 write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n# Helper function to define TFLOPS for add_kernel\ndef calculate_add_tflops(params: Dict, ms: float) -> float:\n    size = params['SIZE']\n    # For add: N operations (N additions)\n    flops = size\n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n\n@pytest.mark.parametrize('SIZE,BLOCK_SIZE,dtype_str',\n                         [(98432, 1024, dtype_str) for dtype_str in ['float16', 'float32']])\ndef test_add(SIZE, BLOCK_SIZE, dtype_str, request):\n    set_seed()\n\n    dtype = dtype_mapping[dtype_str]\n    output = torch.empty(SIZE, device='cuda', dtype=dtype)\n    x = torch.randn(SIZE, device='cuda', dtype=dtype)\n    y = torch.randn(SIZE, device='cuda', dtype=dtype)\n\n    def grid(meta):\n        return (triton.cdiv(SIZE, meta['BLOCK_SIZE']), )\n\n    add_kernel[grid](x, y, output, SIZE, BLOCK_SIZE=BLOCK_SIZE)\n\n    output_torch = x + y\n    torch.set_printoptions(profile='full')\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = output.clone().detach().cpu()\n    ################################################################### \n\n    assert_close(output, output_torch, rtol=1e-2, atol=1e-3, check_dtype=False)\n\n\nOP_NAME_FOR_BENCHMARK = \"add_kernel_perf\"\n\n@pytest.mark.parametrize('SIZE,BLOCK_SIZE_ARG,dtype_str', # BLOCK_SIZE_ARG is the pytest param name\n                         [(98432, 1024, dtype_str) for dtype_str in ['float16', 'float32']] +\n                         [(1048576, 2048, dtype_str) for dtype_str in ['float16', 'float32']]\n                        )\ndef test_performance(SIZE, BLOCK_SIZE_ARG, dtype_str, request): # Function accepts BLOCK_SIZE_ARG\n    set_seed()\n    dtype = dtype_mapping[dtype_str]\n    x = torch.randn(SIZE, device='cuda', dtype=dtype)\n    y = torch.randn(SIZE, device='cuda', dtype=dtype)\n    output = torch.empty(SIZE, device='cuda', dtype=dtype)\n\n    # Kernel launch grid\n    # The 'meta' dict passed to the grid lambda by Triton contains the constexpr arguments\n    # that were passed to the kernel launch.\n    # When we call `add_kernel[grid](..., BLOCK_SIZE=BLOCK_SIZE_ARG)`,\n    # the `meta` dict will have a key 'BLOCK_SIZE' (the name of the constexpr in the kernel signature)\n    # and its value will be the runtime `BLOCK_SIZE_ARG`.\n    grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),) # ***** CORRECTED HERE *****\n\n    kernel_args = [x, y, output, SIZE]\n    \n    # The op_lambda passes BLOCK_SIZE_ARG (runtime value) as the kernel's `BLOCK_SIZE` (constexpr name)\n    op_lambda = lambda: add_kernel[grid](*kernel_args, BLOCK_SIZE=BLOCK_SIZE_ARG)\n\n    bench_config = do_bench_config(warm_up=25, repetition=100) # Smaller for faster debug\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n\n    # The dictionary passed to calculators should use consistent keys\n    current_params_for_calculators = {\"SIZE\": SIZE, \"BLOCK_SIZE_RUNTIME\": BLOCK_SIZE_ARG, \"dtype_str\": dtype_str}\n    # Note: I used \"BLOCK_SIZE_RUNTIME\" here to be explicit that it's the value from parametrize,\n    # not necessarily the same as the constexpr name if they differed.\n    # If your calculators expect 'BLOCK_SIZE', then use that:\n    # current_params_for_calculators = {\"SIZE\": SIZE, \"BLOCK_SIZE\": BLOCK_SIZE_ARG, \"dtype_str\": dtype_str}\n\n\n    benchmarker.run_benchmark(current_params_dict=current_params_for_calculators,\n                              gbps_calculator=calculate_add_gbps,\n                              tflops_calculator=calculate_add_tflops)\n    \n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_gemm_no_scf.py",
        "target_kernel_name": "matmul_no_scf_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `matmul_no_scf_kernel` kernels. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `matmul_no_scf_kernel`,  performs single block of matrix multiplication (C = A @ B) without Structured Control Flow (SCF)\n**Your objective is to implement the body of both the kernels `matmul_no_scf_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `matmul_no_scf_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `matmul_no_scf_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `matmul_no_scf_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \nimport itertools\nimport os\nimport re\n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ######################################## \n\n\n\n\n@triton.jit\ndef matmul_no_scf_kernel(\n    a_ptr,  # tl.pointer_type(dtype)\n    b_ptr,  # tl.pointer_type(dtype)\n    c_ptr,  # tl.pointer_type(dtype)\n    M: int,\n    N: int,\n    K: int,\n    stride_am: int,\n    stride_ak: int,\n    stride_bk: int,\n    stride_bn: int,\n    stride_cm: int,\n    stride_cn: int,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    FLOAT16_OUTPUT: tl.constexpr,\n    USE_TMA_EPILOGUE: tl.constexpr\n):\n    \"\"\"\n    Computes a single block of matrix multiplication (C = A @ B) without explicit\n    iteration over the K dimension (i.e., no `tl.for_loop` for K accumulation,\n    implying K must be equal to BLOCK_K). This kernel is  named \"no_scf\"\n    because performing the full matmul accumulation over K would typically\n    introduce Structured Control Flow (SCF) in the generated MLIR/LLVM IR,\n    which this kernel avoids by processing only one K-block.\n\n    It loads one block of matrix A and one block of matrix B, performs a dot\n    product, and stores the resulting block to matrix C. The kernel should support\n    optional casting of the output to float16 and an optional TMA-based epilogue\n    for storing the result.\n\n    Parameters:\n    -----------\n    a_ptr : tl.pointer_type\n        Pointer to the input matrix A.\n    b_ptr : tl.pointer_type\n        Pointer to the input matrix B.\n    c_ptr : tl.pointer_type\n        Pointer to the output matrix C.\n    M : int\n        Number of rows in matrix A and C. Expected to be equal to BLOCK_M.\n    N : int\n        Number of columns in matrix B and C. Expected to be equal to BLOCK_N.\n    K : int\n        Number of columns in matrix A and rows in matrix B (the common dimension).\n        Expected to be equal to BLOCK_K.\n    stride_am : int\n        Stride of matrix A along the M dimension (row stride).\n    stride_ak : int\n        Stride of matrix A along the K dimension (column stride).\n    stride_bk : int\n        Stride of matrix B along the K dimension (row stride).\n    stride_bn : int\n        Stride of matrix B along the N dimension (column stride).\n    stride_cm : int\n        Stride of matrix C along the M dimension (row stride).\n    stride_cn : int\n        Stride of matrix C along the N dimension (column stride).\n    BLOCK_M : tl.constexpr\n        Compile-time constant defining the height of the blocks to be processed from\n        matrices A and C.\n    BLOCK_N : tl.constexpr\n        Compile-time constant defining the width of the blocks to be processed from\n        matrices B and C.\n    BLOCK_K : tl.constexpr\n        Compile-time constant defining the width of the block from matrix A and\n        the height of the block from matrix B (common dimension for dot product).\n    FLOAT16_OUTPUT : tl.constexpr\n        Compile-time boolean constant. If True, the output matrix C will be cast\n        to float16 before storing. Otherwise, it will be stored in the compute\n        precision (e.g., float32).\n    USE_TMA_EPILOGUE : tl.constexpr\n        Compile-time boolean constant. If True, the epilogue (storing the result C)\n        will use Tensor Memory Access (TMA) operations via `tl.make_block_ptr`\n        and `tl.store`. If False, it will use a more traditional epilogue by\n        calculating destination pointers manually with `tl.arange` and `tl.store`.\n    \"\"\"\n    # Your code here\n\n",
        "label": "# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n######################################## Imports ######################################## \nimport itertools\nimport os\nimport re\n\nimport pytest\nimport torch\nfrom torch.testing import assert_close\n\nimport triton\nimport triton.language as tl\n\n######################################## Imports ######################################## \n\n\n\n@triton.jit\ndef matmul_no_scf_kernel(a_ptr, b_ptr, c_ptr,  #\n                         M, N, K,  #\n                         stride_am, stride_ak,  #\n                         stride_bk, stride_bn,  #\n                         stride_cm, stride_cn,  #\n                         BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,  #\n                         FLOAT16_OUTPUT: tl.constexpr, USE_TMA_EPILOGUE: tl.constexpr  #\n                         ):\n    a_block_ptr = tl.make_block_ptr(\n        base=a_ptr,\n        shape=(M, K),\n        strides=(stride_am, stride_ak),\n        offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_K),\n        order=(1, 0),\n    )\n    b_block_ptr = tl.make_block_ptr(\n        base=b_ptr,\n        shape=(K, N),\n        strides=(stride_bk, stride_bn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_K, BLOCK_N),\n        order=(0, 1),\n    )\n    a = tl.load(a_block_ptr)\n    b = tl.load(b_block_ptr)\n\n    c = tl.dot(a, b)\n\n    if FLOAT16_OUTPUT:\n        c = c.to(tl.float16)\n\n    if USE_TMA_EPILOGUE:\n        c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn), offsets=(0, 0),\n                                        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n        tl.store(c_block_ptr, c)\n    else:\n        offs_m = tl.arange(0, BLOCK_M)\n        offs_n = tl.arange(0, BLOCK_N)\n        c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        tl.store(c_ptrs, c)\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nimport itertools\nimport re\n\nfrom torch.testing import assert_close\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\nimport triton\nimport triton.language as tl\n\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\ndef is_hip():\n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n\n\n######################################## HELPERS for Eval ######################################## \n\n\n\n\n@pytest.mark.parametrize(\n    'M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_TYPE,USE_TMA_EPILOGUE',\n    itertools.chain(*[[\n        # numCTAs = 1, no TMA multicast:\n        [64, 16, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [64, 32, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [64, 64, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [64, 64, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [64, 64, 32, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [64, 64, 64, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [128, 128, 16, 1, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [128, 128, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        # static mask, cluster 4x1\n        [256, 64, 16, 4, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [256, 64, 16, 4, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        # dynamic mask, cluster 2x2\n        [128, 128, 16, 4, 4, False, True, \"float16\", USE_TMA_EPILOGUE],\n        [128, 128, 16, 4, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        # small M, N\n        [16, 16, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [16, 32, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [32, 16, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n        [32, 32, 16, 1, 4, False, True, \"float32\", USE_TMA_EPILOGUE],\n    ] for USE_TMA_EPILOGUE in [True, False]]))\n@pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\ndef test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, request):\n    set_seed()\n    \n    if is_hip() and NUM_CTAS > 1:\n        pytest.skip(\"NUM_CTAS > 1 is not supported in HIP backend\")\n\n    if (TRANS_A):\n        a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n    else:\n        a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n    if (TRANS_B):\n        b = torch.randn((N, K), device='cuda', dtype=torch.float16).T\n    else:\n        b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n\n    if OUTPUT_TYPE == \"float16\":\n        c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    else:\n        c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n\n    matmul_no_scf_kernel[(1, 1)](\n        a_ptr=a, b_ptr=b, c_ptr=c,  #\n        M=M, N=N, K=K,  #\n        stride_am=a.stride(0), stride_ak=a.stride(1),  #\n        stride_bk=b.stride(0), stride_bn=b.stride(1),  #\n        stride_cm=c.stride(0), stride_cn=c.stride(1),  #\n        BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,  #\n        num_warps=NUM_WARPS,  #\n        num_ctas=NUM_CTAS,  #\n        FLOAT16_OUTPUT=(OUTPUT_TYPE == \"float16\"),  #\n        USE_TMA_EPILOGUE=USE_TMA_EPILOGUE)\n    a_f32 = a.to(torch.float32)\n    b_f32 = b.to(torch.float32)\n    golden = torch.matmul(a_f32, b_f32)\n    torch.set_printoptions(profile=\"full\")\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = c.clone().detach().cpu()\n    ###################################################################\n\n\n    assert_close(c, golden, rtol=1e-2, atol=1e-3, check_dtype=False)\n\n\ndef gemm_no_scf_triton_wrapper(a_tensor, b_tensor, c_buffer, \n                               M_dim, N_dim, K_dim, \n                               num_warps_launch, float16_out_flag, use_tma_flag):\n    grid = (1,) \n    matmul_no_scf_kernel[grid](\n        a_ptr=a_tensor, b_ptr=b_tensor, c_ptr=c_buffer,\n        M=M_dim, N=N_dim, K=K_dim,\n        stride_am=a_tensor.stride(0), stride_ak=a_tensor.stride(1),\n        stride_bk=b_tensor.stride(0), stride_bn=b_tensor.stride(1),\n        stride_cm=c_buffer.stride(0), stride_cn=c_buffer.stride(1),\n        BLOCK_M=M_dim, BLOCK_N=N_dim, BLOCK_K=K_dim, \n        FLOAT16_OUTPUT=float16_out_flag,\n        USE_TMA_EPILOGUE=use_tma_flag,\n        num_warps=num_warps_launch\n    )\n    return c_buffer\n\ndef calculate_gemm_no_scf_tflops(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    flops = 2 * M * N * K \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\n\ndef calculate_gemm_no_scf_gbps(params: dict, ms: float) -> float:\n    M, N, K = params['M'], params['N'], params['K']\n    \n    input_dtype_str = params.get('input_dtype_str', 'fp16') \n    output_dtype_str = params['OUTPUT_TYPE'] \n\n    # Convert dtype strings to torch.dtype objects\n    current_input_dtype = torch.float16 \n    if input_dtype_str == 'fp32': current_input_dtype = torch.float32\n    elif input_dtype_str == 'bf16': current_input_dtype = torch.bfloat16\n\n    current_output_dtype = torch.float16 \n    if output_dtype_str == 'fp32': current_output_dtype = torch.float32\n    elif output_dtype_str == 'bf16': current_output_dtype = torch.bfloat16\n    \n    # Get element sizes by creating dummy tensors\n    in_element_size = torch.tensor([], dtype=current_input_dtype).element_size()\n    out_element_size = torch.tensor([], dtype=current_output_dtype).element_size()\n    \n    bytes_a = M * K * in_element_size\n    bytes_b = K * N * in_element_size \n    bytes_c_write = M * N * out_element_size\n    total_bytes = bytes_a + bytes_b + bytes_c_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"gemm_no_scf_triton_perf\"\n\n# --- NEW Performance Test Function using original test_gemm_no_scf's parametrization ---\n@pytest.mark.parametrize(\n    'M,N,K,NUM_CTAS,NUM_WARPS,TRANS_A,TRANS_B,OUTPUT_TYPE,USE_TMA_EPILOGUE',\n    # Using a smaller, more targeted subset for performance to avoid excessive shared memory issues\n    # and long runtimes. The original parametrize is very extensive.\n    # Focus on K values that are likely to fit for a single-block kernel.\n    itertools.chain(*[[\n        # K=16\n        [64, 64, 16, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE], # A(M,K), B(K,N)\n        [64, 64, 16, 1, 4, False, False, \"float32\", USE_TMA_EPILOGUE],\n        [128, 128, 16, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE],\n        # K=32\n        [64, 64, 32, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE],\n        [64, 64, 32, 1, 4, False, False, \"float32\", USE_TMA_EPILOGUE],\n        [128, 64, 32, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE], # M=128, N=64, K=32\n        # K=64\n        [32, 32, 64, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE], # M=32, N=32, K=64\n        [64, 64, 64, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE],\n        [64, 64, 64, 1, 4, False, False, \"float32\", USE_TMA_EPILOGUE],\n        # K=128 - starts to get large for single block shared mem\n        [32, 32, 128, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE],\n        # [64, 64, 128, 1, 4, False, False, \"float16\", USE_TMA_EPILOGUE], # (32*128+128*32)*2 = 16384. Ok.\n                                                                        # (64*128+128*64)*2 = 32768. Ok.\n    ] for USE_TMA_EPILOGUE in [True, False]]))\n# @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\ndef test_performance(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, request):\n    cap = torch.cuda.get_device_capability()\n    if cap[0] < 9:\n        pytest.skip(\"Requires compute capability >= 9 (as per original test)\")\n    if is_hip() and NUM_CTAS > 1: \n        pytest.skip(\"NUM_CTAS > 1 is not supported in HIP for this test (as per original)\")\n\n    # Shared memory check (for fp16 inputs)\n    input_element_size = 2 # Assuming fp16 inputs\n    smem_elements_needed = (M * K) + (K * N)\n    if smem_elements_needed * input_element_size > 65536: # 64KB limit\n        pytest.skip(f\"Skipping M{M}N{N}K{K} due to estimated shared memory for inputs: \"\n                    f\"{smem_elements_needed * input_element_size} > 65536\")\n\n    set_seed()\n    \n    input_torch_dtype = torch.float16 # Original test uses fp16 for a and b\n\n    # Input setup: Ensure a is (M,K) and b is (K,N) before passing to wrapper\n    a_shape_before_trans = (K, M) if TRANS_A else (M, K)\n    b_shape_before_trans = (N, K) if TRANS_B else (K, N)\n    \n    a_host = torch.randn(a_shape_before_trans, device='cuda', dtype=input_torch_dtype)\n    if TRANS_A: a_host = a_host.T \n    \n    b_host = torch.randn(b_shape_before_trans, device='cuda', dtype=input_torch_dtype)\n    if TRANS_B: b_host = b_host.T \n\n    c_out_torch_dtype = torch.float16 if OUTPUT_TYPE == \"float16\" else torch.float32\n    c_buffer = torch.empty((M, N), device=a_host.device, dtype=c_out_torch_dtype)\n\n    op_lambda = lambda: gemm_no_scf_triton_wrapper(\n        a_host, b_host, c_buffer, M, N, K, \n        NUM_WARPS, (OUTPUT_TYPE == \"float16\"), USE_TMA_EPILOGUE\n    )\n\n    bench_config = do_bench_config(warm_up=25, repetition=100)\n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"M\": M, \"N\": N, \"K\": K, \"NUM_CTAS\": NUM_CTAS, \"NUM_WARPS\": NUM_WARPS,\n        \"TRANS_A\": TRANS_A, \"TRANS_B\": TRANS_B, \n        \"OUTPUT_TYPE\": OUTPUT_TYPE, \"USE_TMA_EPILOGUE\": USE_TMA_EPILOGUE,\n        \"input_dtype_str\": \"fp16\" # Hardcoded based on original test's a,b creation\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_gemm_no_scf_gbps,\n                                            tflops_calculator=calculate_gemm_no_scf_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n######################################## HELPERS for Eval ########################################"
    },
    {
        "file": "test_flashattention_fwd.py",
        "target_kernel_name": "flash_fwd_kernel",
        "instruction": "\nYou are an expert in triton programming language. You will be given the function definition for the `flash_fwd_kernel` kernels. Your task is to complete the kernel code. Only complete the kernel code in the function definition, DONT remove any python imports or helper utils in the instruction/code provided, DONT change/interfere with the provided function definition and parameter list.\n\nThis kernel, `flash_fwd_kernel`,  performs forward pass of the FlashAttention algorithm\n**Your objective is to implement the body of both the kernels `flash_fwd_kernel`.**\n\nYou must ensure that:\n1.  All arguments received by `flash_fwd_kernel` are kept intact and not modified.\n2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `flash_fwd_kernel` and relevant helper utilities are provided in the context below. You only need to complete the code for `flash_fwd_kernel` whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n######################################## Imports ######################################## \n\n# import numpy as np\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n\n@triton.jit\ndef flash_fwd_kernel(\n    Q, K, V, sm_scale,  # Input tensors and softmax scale\n    L, M,  # Intermediate tensors for online softmax\n    Out,  # Output tensor\n    stride_qz, stride_qh, stride_qm, stride_qk,  # Strides for Q\n    stride_kz, stride_kh, stride_kn, stride_kk,  # Strides for K\n    stride_vz, stride_vh, stride_vk, stride_vn,  # Strides for V\n    stride_oz, stride_oh, stride_om, stride_on,  # Strides for Out\n    Z, H, N_CTX, D0,  # Tensor dimensions\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr  # Block sizes\n):\n    \"\"\"\n    Computes the forward pass of FlashAttention.\n\n    Args:\n        Q (Tensor): Query tensor with shape (Z, H, N_CTX, D_HEAD). This kernel expects a pointer to the beginning of this tensor.\n        K (Tensor): Key tensor with shape (Z, H, N_CTX, D_HEAD). This kernel expects a pointer to the beginning of this tensor.\n        V (Tensor): Value tensor with shape (Z, H, N_CTX, D_HEAD). This kernel expects a pointer to the beginning of this tensor.\n        sm_scale (float): Scaling factor applied to the QK^T product before softmax. Typically 1/sqrt(D_HEAD).\n        L (Tensor): Output tensor of shape (Z*H, N_CTX) used to store the row-wise sum of `exp(scores - max_scores)` for the online softmax calculation.\n                    It acts as the normalizer `l_i` in the FlashAttention algorithm.\n        M (Tensor): Output tensor of shape (Z*H, N_CTX) used to store the row-wise maximum of QK^T scores (`m_i` in FlashAttention) for numerically stable online softmax.\n        Out (Tensor): Output tensor of shape (Z, H, N_CTX, D_HEAD) where the attention output is stored.\n        stride_qz (int): Stride for the Z (batch) dimension of the Q tensor, in terms of number of elements.\n        stride_qh (int): Stride for the H (head) dimension of the Q tensor, in terms of number of elements.\n        stride_qm (int): Stride for the M (query sequence length, N_CTX) dimension of the Q tensor, in terms of number of elements.\n        stride_qk (int): Stride for the K (head dimension, D_HEAD) dimension of the Q tensor, in terms of number of elements.\n        stride_kz (int): Stride for the Z (batch) dimension of the K tensor.\n        stride_kh (int): Stride for the H (head) dimension of the K tensor.\n        stride_kn (int): Stride for the N (key sequence length, N_CTX) dimension of the K tensor.\n        stride_kk (int): Stride for the K (head dimension, D_HEAD) dimension of the K tensor.\n        stride_vz (int): Stride for the Z (batch) dimension of the V tensor.\n        stride_vh (int): Stride for the H (head) dimension of the V tensor.\n        stride_vk (int): Stride for the K (key/value sequence length, N_CTX) dimension of the V tensor. (Note: `_vk` here refers to the sequence dim for V).\n        stride_vn (int): Stride for the N (head dimension, D_HEAD) dimension of the V tensor. (Note: `_vn` here refers to the head dim for V).\n        stride_oz (int): Stride for the Z (batch) dimension of the Out tensor.\n        stride_oh (int): Stride for the H (head) dimension of the Out tensor.\n        stride_om (int): Stride for the M (query sequence length, N_CTX) dimension of the Out tensor.\n        stride_on (int): Stride for the N (head dimension, D_HEAD) dimension of the Out tensor.\n        Z (int): Batch size.\n        H (int): Number of attention heads.\n        N_CTX (int): Sequence length (context length). Assumed to be the same for Q, K, and V for simplicity in this kernel's structure, particularly for causal masking and L, M storage.\n        D0 (int): This parameter represents the sequence length dimension (N_CTX) for a single head's data matrix. It is used in `tl.make_block_ptr` for the `shape` argument's first dimension when viewing a head's Q, K, or V data. It should be equal to N_CTX.\n        BLOCK_M (tl.constexpr): The size of the block along the query sequence length dimension (M). Queries are processed in blocks of this size.\n        BLOCK_DMODEL (tl.constexpr): The head dimension size (D_HEAD). The kernel processes the full head dimension.\n        BLOCK_N (tl.constexpr): The size of the block along the key/value sequence length dimension (N). Keys and values are loaded and processed in blocks of this size.\n    \"\"\"\n    # Your code here\n\n",
        "label": "# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\nFused Attention\n===============\nThis is a Triton implementation of the Flash Attention algorithm\n(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n\"\"\"\n######################################## Imports ######################################## \n\n# import numpy as np\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n######################################## Imports ######################################## \n\n\n@triton.jit\ndef flash_fwd_kernel(Q, K, V, sm_scale,  #\n                L, M,  #\n                Out,  #\n                stride_qz, stride_qh, stride_qm, stride_qk,  #\n                stride_kz, stride_kh, stride_kn, stride_kk,  #\n                stride_vz, stride_vh, stride_vk, stride_vn,  #\n                stride_oz, stride_oh, stride_om, stride_on,  #\n                Z, H, N_CTX, D0,  #\n                BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    # TODO: may replace with TMA store without range offset\n    # initialize offsets for store\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    # initialize pointer to m and l\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    stride_qh_2d = stride_qh // stride_qm // stride_qk\n\n    q_tile_ptr = tl.make_block_ptr(\n        base=Q,\n        shape=(D0, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(off_hz * stride_qh_2d + start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    k_tile_ptr = tl.make_block_ptr(\n        base=K,\n        shape=(D0, BLOCK_DMODEL),\n        strides=(stride_kn, stride_kk),\n        offsets=(off_hz * stride_qh_2d, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    v_tile_ptr = tl.make_block_ptr(\n        base=V,\n        shape=(D0, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(off_hz * stride_qh_2d, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    out_tile_ptr = tl.make_block_ptr(\n        base=Out,\n        shape=(D0, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(off_hz * stride_qh_2d + start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    # load q: it will stay in SRAM throughout\n    q = tl.load(q_tile_ptr)\n\n    # loop over k, v and update accumulators\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        # -- compute qk ----\n        k = tl.load(k_tile_ptr, boundary_check=(0, 1))\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n        # compute new m\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n        # correct old l\n        l_prev *= tl.exp(m_prev - m_curr)\n        # attention weights\n        p = tl.exp(qk - m_curr[:, None])\n        l_curr = tl.sum(p, 1) + l_prev\n        # rescale operands of matmuls\n        l_rcp = 1. / l_curr\n        p *= l_rcp[:, None]\n        acc *= (l_prev * l_rcp)[:, None]\n        # update acc\n        p = p.to(tl.float16)\n        v = tl.load(v_tile_ptr, boundary_check=(0, 1))\n        acc += tl.dot(p, v)\n        # update m_i and l_i\n        l_prev = l_curr\n        m_prev = m_curr\n        # update pointers\n        k_tile_ptr = tl.advance(k_tile_ptr, [BLOCK_N, 0])\n        v_tile_ptr = tl.advance(v_tile_ptr, [BLOCK_N, 0])\n    # rematerialize offsets to save registers\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    # write back l and m\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, l_prev)\n    tl.store(m_ptrs, m_prev)\n\n    acc = acc.to(tl.float16)\n    tl.store(out_tile_ptr, acc, boundary_check=(0, 1))\n\n##################################################################################################################################################  \n\nimport numpy as np\nimport random\nimport torch \nimport os\nimport pytest\nfrom numpy.random import RandomState\nfrom tb_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nresult_gold = {}\n\n######################################## HELPERS for Eval ######################################## \ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\n\nempty = torch.empty(128, device=\"cuda\")\n\n\nclass _attention(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, sm_scale):\n        BLOCK = 128\n        # shape constraints\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n        assert Lq == Lk and Lk == Lv\n        assert Lk in {16, 32, 64, 128}\n        o = torch.empty_like(q)\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        num_warps = 4 if Lk <= 64 else 8\n        D0 = q.shape[0] * q.shape[1] * q.shape[2]\n        flash_fwd_kernel[grid](\n            q, k, v, sm_scale,  #\n            L, m,  #\n            o,  #\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #\n            q.shape[0], q.shape[1], q.shape[2], D0,  #\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,  #\n            num_warps=num_warps, num_stages=2)\n\n        ctx.save_for_backward(q, k, v, o, L, m)\n        ctx.grid = grid\n        ctx.sm_scale = sm_scale\n        ctx.BLOCK_DMODEL = Lk\n        return o\n\n\nattention = _attention.apply\n\n\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [\n    (4, 48, 128, 64),\n    (4, 48, 256, 64),\n    (4, 48, 512, 64),\n    (4, 48, 1024, 64),\n    (4, 48, 2048, 64),\n    (4, 48, 4096, 64),\n    #  (4, 48, 8192, 64), out of memory\n])\n@pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"requires arch 9+\")\ndef test_op(Z, H, N_CTX, D_HEAD, request, dtype=torch.float16):\n    \n    set_seed()\n\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n    sm_scale = 0.2\n    dout = torch.randn_like(q)\n    # reference implementation\n    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n    for z in range(Z):\n        for h in range(H):\n            p[:, :, M == 0] = float(\"-inf\")\n    p = torch.softmax(p.float(), dim=-1).half()\n    # p = torch.exp(p)\n    ref_out = torch.matmul(p, v)\n\n    # triton implementation\n    tri_out = attention(q, k, v, sm_scale)\n\n\n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = tri_out.clone().detach().cpu()\n    ################################################################### \n\n    # compare\n    torch.testing.assert_close(ref_out, tri_out, atol=1e-2, rtol=0)\n\ndef calculate_flash_attention_fwd_tflops(params: dict, ms: float) -> float:\n    Z, H, N_CTX, D_HEAD = params['Z'], params['H'], params['N_CTX'], params['D_HEAD']\n    flops = 2 * Z * H * N_CTX * N_CTX * D_HEAD \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef calculate_flash_attention_fwd_gbps(params: dict, ms: float) -> float:\n    Z, H, N_CTX, D_HEAD = params['Z'], params['H'], params['N_CTX'], params['D_HEAD']\n    dtype_str = params.get('dtype_str', 'fp16') \n    current_dtype = torch.float16\n    if dtype_str == 'fp32': current_dtype = torch.float32\n    # bf16 was removed from perf test, but keep for completeness if added back\n    elif dtype_str == 'bf16': current_dtype = torch.bfloat16 \n    element_size = torch.tensor([], dtype=current_dtype).element_size()\n    bytes_q, bytes_k, bytes_v, bytes_o_write = [Z * H * N_CTX * D_HEAD * element_size] * 4\n    bytes_L_write, bytes_M_write = [Z * H * N_CTX * 4] * 2\n    total_bytes = bytes_q + bytes_k + bytes_v + bytes_o_write + bytes_L_write + bytes_M_write\n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\nOP_NAME_FOR_BENCHMARK = \"flash_attention_fwd_triton_perf\"\n\n# --- NEW Performance Test Function using original test_op's parametrization ---\n# It mirrors test_op's parametrize for Z, H, N_CTX, D_HEAD.\n# It adds its own parametrize for dtype_str, EXCLUDING bf16 for now.\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [\n    (4, 48, 128, 64), (4, 48, 256, 64), (4, 48, 512, 64),\n    (4, 48, 1024, 64), (4, 48, 2048, 64), (4, 48, 4096, 64),\n    # (2, 12, 1024, 64), (1, 8, 2048, 32) # Example additional shapes if desired\n])\n@pytest.mark.parametrize('dtype_str', ['fp16']) # MODIFIED: Only fp16 for now to avoid bf16 JIT errors\n# @pytest.mark.parametrize('dtype_str', ['fp16', 'fp32']) # Can add fp32 if it works\ndef test_performance(Z, H, N_CTX, D_HEAD, dtype_str, request): \n    \n    cap = torch.cuda.get_device_capability()\n    if cap[0] < 9: \n         pytest.skip(\"Original test requires arch 90+ for this flash attention kernel version.\")\n    # No bf16 specific skip needed as bf16 is removed from parametrize for this function\n\n    # Shared memory OOM skip for D_HEAD=128 (if such cases were added)\n    # This logic relies on knowing the hardcoded BLOCK and num_stages in _attention.forward\n    hardcoded_block_in_wrapper = 128 \n    hardcoded_num_stages_in_wrapper = 2\n    if D_HEAD == 128 and \\\n       hardcoded_block_in_wrapper == 128 and \\\n       hardcoded_num_stages_in_wrapper == 2:\n        pytest.skip(f\"Skipping D_HEAD={D_HEAD} with BLOCK=128, num_stages=2 due to high shared memory demand.\")\n    \n    set_seed()\n    \n    if dtype_str == 'fp32': current_dtype = torch.float32\n    # elif dtype_str == 'bf16': current_dtype = torch.bfloat16 # bf16 removed\n    else: current_dtype = torch.float16 \n\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=current_dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_(False)\n    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=current_dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_(False)\n    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=current_dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_(False)\n    sm_scale = 0.2 \n\n    op_lambda = lambda: attention(q, k, v, sm_scale)\n\n    bench_config = do_bench_config(warm_up=10, repetition=50) \n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    current_params_for_logs_and_calc = {\n        \"Z\": Z, \"H\": H, \"N_CTX\": N_CTX, \"D_HEAD\": D_HEAD,\n        \"dtype_str\": dtype_str, \"sm_scale\": sm_scale\n    }\n    \n    perf_result = benchmarker.run_benchmark(current_params_dict=current_params_for_logs_and_calc,\n                                            gbps_calculator=calculate_flash_attention_fwd_gbps,\n                                            tflops_calculator=calculate_flash_attention_fwd_tflops)\n\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n######################################## HELPERS for Eval ########################################\n\ntry:\n    from flash_attn.flash_attn_interface import flash_attn_func\n    HAS_FLASH = True\nexcept BaseException:\n    HAS_FLASH = False\n\nBATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n# vary seq length for fixed head and batch=4\nconfigs = [\n    triton.testing.Benchmark(\n        x_names=['N_CTX'],\n        x_vals=[2**i for i in range(10, 14)],\n        line_arg='provider',\n        line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n        line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n        styles=[('red', '-'), ('blue', '-')],\n        ylabel='ms',\n        plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n        args={\n            'H': N_HEADS,\n            'BATCH': BATCH,\n            'D_HEAD': D_HEAD,\n            'dtype': torch.float16,\n            'mode': mode,\n        },\n    ) for mode in ['fwd', 'bwd']\n]\n\n\n@triton.testing.perf_report(configs)\ndef bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n    assert mode in ['fwd', 'bwd']\n    if provider == \"triton\":\n        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n        sm_scale = 1.3\n        fn = lambda: attention(q, k, v, sm_scale)\n        if mode == 'bwd':\n            o = fn()\n            do = torch.randn_like(o)\n            fn = lambda: o.backward(do, retain_graph=True)\n        ms = triton.testing.do_bench(fn)\n        return ms\n    if provider == \"flash\":\n        lengths = torch.full((BATCH, ), fill_value=N_CTX, device=device)\n        cu_seqlens = torch.zeros((BATCH + 1, ), device=device, dtype=torch.int32)\n        cu_seqlens[1:] = lengths.cumsum(0)\n        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n        if mode == 'bwd':\n            o = fn()\n            do = torch.randn_like(o)\n            fn = lambda: o.backward(do, retain_graph=True)\n        ms = triton.testing.do_bench(fn)\n        return ms\n\n\n"
    }
]